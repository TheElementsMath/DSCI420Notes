# Matrix Decompositions





Matrices provide a compact way to represent linear mappings and data, where rows often correspond to observations and columns correspond to features. This chapter focuses on three central questions about matrices:

1. How to summarize a matrix with key numerical characteristics.   
2. How to decompose a matrix into simpler, interpretable components.  
3. How to use these decompositions for approximations and analysis.



<p align="center">
<img src="Figure4.1MML.png" alt=" A mind map of the concepts introduced in this chapter, along with where they are used in other parts of the book." width="400">
</p>











<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->





##  Determinant and Trace




The **determinant** of a square matrix \( \mathbf{A} \in \mathbb{R}^{n \times n} \), denoted as \(\det(\mathbf{A})\) or \(|\mathbf{A}|\), is a scalar that characterizes several key properties of \( \mathbf{A} \).

For small matrices:

\[
\det \begin{pmatrix} a_{11} \end{pmatrix} = a_{11}, \quad
\det \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} = a_{11}a_{22} - a_{12}a_{21}.
\]


<div class="example">
Let  
\[
 \mathbf{A} = 
\begin{bmatrix}
3 & -1 \\
5 & 2
\end{bmatrix}.
\]

The determinant of $\mathbf{A}$ is
\[
\det(\mathbf{A})
= (3)(2) - (-1)(5)
= 6 + 5
= 11.
\]
</div>

For larger matrices, we can compute determinants recursively using the **Laplace expansion**:

\[
\det(\mathbf{A}) = \sum_{k=1}^n (-1)^{k+j} a_{kj} \det(\mathbf{A}_{k,j}),
\]
where \( \mathbf{A}_{k,j} \) is the submatrix obtained by removing row \( k \) and column \( j \).



<div class="example">
Let  
\[
 \mathbf{A} =
\begin{bmatrix}
2 & -1 & 3 \\
1 & 4 & 0 \\
-2 & 5 & 1
\end{bmatrix}.
\]

We compute \(\det( \mathbf{A})\) using Laplace expansion along the first row.
\[
\det( \mathbf{A})
= 2
\begin{vmatrix}
4 & 0 \\
5 & 1
\end{vmatrix}
- (-1)
\begin{vmatrix}
1 & 0 \\
-2 & 1
\end{vmatrix}
+ 3
\begin{vmatrix}
1 & 4 \\
-2 & 5
\end{vmatrix}.
\]
Next, we compute each minor
\begin{align*}
\begin{vmatrix}
4 & 0 \\
5 & 1
\end{vmatrix}
&= (4)(1) - (0)(5) = 4\\

\begin{vmatrix}
1 & 0 \\
-2 & 1
\end{vmatrix}
&= (1)(1) - (0)(-2) = 1\\

\begin{vmatrix}
1 & 4 \\
-2 & 5
\end{vmatrix}
&= (1)(5) - (4)(-2) = 5 + 8 = 13
\end{align*}

Using these determinants in the Laplace formula gives us
\[
\det( \mathbf{A}) = 2(4) - (-1)(1) + 3(13)
= 8 + 1 + 39
= 48.
\]
</div>

 A matrix \( \mathbf{A} \) is **invertible** if and only if \( \det(\mathbf{A}) \neq 0 \).  


<div class="example">
Verify that a matrix is invertible if and only if its determinant is non-zero.

</div>


For triangular matrices, the determinant equals the product of the diagonal elements. 


<div class="example">
Let  
\[
 \mathbf{A} =
\begin{bmatrix}
3 & 2 & -1 \\
0 & 5 & 4 \\
0 & 0 & 7
\end{bmatrix}.
\]

Since \( \mathbf{A}\) is upper triangular, we already know that  
\[
\det( \mathbf{A}) = 3 \cdot 5 \cdot 7 = 105.
\]
But we will verify this using Laplace expansion.

Expand along column 1:
\begin{align*}
\det(\mathbf{A}) &=
3
\begin{vmatrix}
5 & 4 \\
0 & 7
\end{vmatrix}
- 0
\begin{vmatrix}
2 & -1 \\
0 & 7
\end{vmatrix}
+ 0
\begin{vmatrix}
2 & -1 \\
5 & 4
\end{vmatrix}\\
&= 3
\begin{vmatrix}
5 & 4 \\
0 & 7
\end{vmatrix}\\
&= 3\left[(5)(7) - (4)(0)\right]\\
&= 3(35)\\
&= 105.
\end{align*}
This matches the product of the diagonal entries, as expected for triangular matrices.
</div>

The determinant changes sign when two rows (or columns) are swapped, and scales when a row is multiplied by a scalar.



<div class="example">

Let
\[
 \mathbf{A}=\begin{bmatrix}1 & 2\\[4pt] 3 & 4\end{bmatrix}.
\]
Compute \(\det( \mathbf{A})\):
\[
\det( \mathbf{A})=1\cdot 4 - 2\cdot 3 = 4 - 6 = -2.
\]

Swap row 1 and row 2 to get
\[
 \mathbf{B}=\begin{bmatrix}3 & 4\\[4pt] 1 & 2\end{bmatrix}.
\]
Compute \(\det( \mathbf{B})\):
\[
\det( \mathbf{B})=3\cdot 2 - 4\cdot 1 = 6 - 4 = 2.
\]

Observation: \(\det( \mathbf{B})=2 = -(-2)= -\det( \mathbf{A})\).  
Swapping two rows changed the sign of the determinant.
</div>


<div class="example">
Let
\[
 \mathbf{C}=\begin{bmatrix}2 & 1\\[4pt] 0 & 3\end{bmatrix}.
\]
Compute \(\det( \mathbf{C})\):
\[
\det( \mathbf{C})=2\cdot 3 - 1\cdot 0 = 6 - 0 = 6.
\]

Multiply the first row of \( \mathbf{C}\) by \(2\) to get
\[
 \mathbf{D}=\begin{bmatrix}4 & 2\\[4pt] 0 & 3\end{bmatrix}.
\]
Compute \(\det( \mathbf{D})\):
\[
\det( \mathbf{D})=4\cdot 3 - 2\cdot 0 = 12 - 0 = 12.
\]

Observation: \(\det( \mathbf{D})=12 = 2\cdot 6 = 2\det( \mathbf{C})\).  
Scaling a single row by a factor \(2\) scales the determinant by \(2\).
</div>


### Geometric Interpretation

The determinant measures the **signed volume** of the parallelepiped spanned by the columns of \( \mathbf{A} \):

- In \( \mathbb{R}^2 \): \( |\det(\mathbf{A})| \) gives the **area** of a parallelogram.  
- In \( \mathbb{R}^3 \): \( |\det(\mathbf{A})| \) gives the **volume** of a parallelepiped.

If the determinant is zero, the columns are **linearly dependent** and the volume collapses to zero.



### Properties of the Determinant


<div class="theorem">
For square matrices $\mathbf{A}$ and $\mathbf{B}$ and $\lambda \in \mathbb{R}$, the following properties hold:
\[
\begin{aligned}
\det(\mathbf{A}\mathbf{B}) &= \det(\mathbf{A})\det(\mathbf{B}), \\
\det(\mathbf{A}^\top) &= \det(\mathbf{A}), \\
\det(\mathbf{A}^{-1}) &= \frac{1}{\det(\mathbf{A})}, \\
\det(\lambda \mathbf{A}) &= \lambda^n \det(\mathbf{A}).
\end{aligned}
\]
</div>



<div class="example">
For $2 \times 2$ matrices,
\[
 \mathbf{A} = \begin{bmatrix} a & b \\ c & d \end{bmatrix}, 
\quad
\mathbf{B} = \begin{bmatrix} e & f \\ g & h \end{bmatrix},
\]
we have
\[
 \mathbf{A}\mathbf{B} =
\begin{bmatrix}
ae + bg & af + bh \\
ce + dg & cf + dh
\end{bmatrix}.
\]
This has determinant
\begin{align*}
\det( \mathbf{A}\mathbf{B}) &=
\begin{vmatrix}
ae + bg & af + bh \\
ce + dg & cf + dh
\end{vmatrix} \\
& = (ae + bg)(cf + dh) - (af + bh)(ce + dg)\\
& = aecf + aedh + bgcf + bgdh 
  - \left( afce + afdg + bhce + bhdg \right)\\
& = ac(ef - fe) + ad(eh - fg) + bc(gf - he) + bd(gh - hg)\\
& = ad(eh - fg) - bc(eh - fg)\\
& = (ad - bc)(eh - fg) \\
&= \det( \mathbf{A})\det(\mathbf{B}).
\end{align*}
</div>

<div class="theorem">
 A matrix is **invertible** if and only if it is **full rank**, i.e. \( \text{rank}(\mathbf{A}) = n \).
</div>



---

### Trace



<div class="definition">
The **trace** of a square matrix \( \mathbf{A} \in \mathbb{R}^{n \times n} \) is the sum of its diagonal elements:
\[
\text{tr}(\mathbf{A}) = \sum_{i=1}^n a_{ii}.
\]
</div>



<div class="theorem">
For square matrices $\mathbf{A}$ and $\mathbf{B}$ and $\alpha \in \mathbb{R}$, the following properties hold:
\[
\begin{aligned}
\text{tr}(\mathbf{A} + \mathbf{B}) &= \text{tr}(\mathbf{A}) + \text{tr}(\mathbf{B}), \\
\text{tr}(\alpha \mathbf{A}) &= \alpha \, \text{tr}(\mathbf{A}), \\
\text{tr}(\mathbf{A}\mathbf{B}) &= \text{tr}(\mathbf{B}\mathbf{A}), \\
\text{tr}(\mathbf{I}_n) &= n.
\end{aligned}
\]
</div>


The trace is **invariant under cyclic permutations**, meaning \(\text{tr}(\mathbf{A}\mathbf{K}\mathbf{L}) = \text{tr}(\mathbf{K}\mathbf{L}\mathbf{A})\).  It is also **independent of basis**, so the trace of a linear map \( \Phi \) is the same in all matrix representations.



<div class="example">
Let  
\[
\mathbf{A} = \begin{bmatrix}
3 & -1 & 4 \\
0 & 2 & 5 \\
7 & 1 & -6
\end{bmatrix}.
\]
The trace is
\[
\text{tr}(\mathbf{A}) = 3 + 2 + (-6) = -1.
\]

</div>

---

### Characteristic Polynomial



<div class="definition">
The **characteristic polynomial** of a square matrix \( \mathbf{A} \) is defined as:
\[
p_ \mathbf{A}(\lambda) = \det(\mathbf{A} - \lambda \mathbf{I}) = c_0 + c_1 \lambda + \cdots + c_{n-1} \lambda^{n-1} + (-1)^n \lambda^n.
\]
</div>

The characteristic polynomial for $\mathbf{A}$ encodes key properties of \( \mathbf{A} \):
\[
c_0 = \det(\mathbf{A}), \quad
c_{n-1} = (-1)^{n-1} \text{tr}(\mathbf{A}).
\]
The roots of this polynomial are the **eigenvalues** of \( \mathbf{A} \), which will be explored in the next section.

---


<div class="example">
Let  
\[
\mathbf{A} = \begin{bmatrix}
3 & -1 & 4 \\
0 & 2 & 5 \\
7 & 1 & -6
\end{bmatrix}.
\]
The characteristic polynomial is
\begin{align*}
p(\lambda) &= \det(A - \lambda I) \\
&= \det\left(
\begin{bmatrix}
3 - \lambda & -1 & 4 \\
0 & 2 - \lambda & 5 \\
7 & 1 & -6 - \lambda
\end{bmatrix} \right) \\
&= (3 - \lambda)
\begin{vmatrix}
2 - \lambda & 5 \\
1 & -6 - \lambda
\end{vmatrix}
+ 7
\begin{vmatrix}
-1 & 4 \\
2 - \lambda & 5
\end{vmatrix}\\
&= (3 - \lambda)(\lambda^2 + 4\lambda - 17)
+ 7(4\lambda - 13) \\
&= -\lambda^3 - \lambda^2 + 57\lambda - 142.
\end{align*}

</div>

### Exercises {.unnumbered .unlisted}





<div class="exercise">
 Find $\det\left(\begin{bmatrix} 2 & 3 \\ 4 & 5 \end{bmatrix} \right)$ using the formula.

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find $\det\left(\begin{bmatrix} 2 & 3 &4\\ 5 & 6 &7\\ 8 & 9 & 1 \end{bmatrix} \right)$ using the formula.
<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find $\det\left(\begin{bmatrix} 2 & 3 &4\\ 5 & 6 &7\\ 8 & 9 & 1 \end{bmatrix} \right)$ using the Laplace method.
<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Prove that if $A$ is a square matrix with a row or column of 0's, then $\det(A) = 0$. 
<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find $\det\left(\begin{bmatrix} 0& 2 & 3 &4\\ 5 & 6 &7&0 \\1& 8 & 9 & 1\\0&2&3&0 \end{bmatrix} \right)$ using the Laplace method. 
<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Prove that if $A$ is a square matrix with 2 identical rows or columns, then $\det(A) = 0$. 
<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find $\det\left(\begin{bmatrix} 2 & 0 &0\\ 5 & 6 &0\\ 8 & 9 & 1 \end{bmatrix} \right)$.
<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
 Find $\text{tr}\left(\begin{bmatrix} 2 & 0 &0\\ 5 & 6 &0\\ 8 & 9 & 1 \end{bmatrix} \right)$. 
<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find the characteristic polynomial for $\begin{bmatrix} 2 & 0 &0\\ 5 & 6 &0\\ 8 & 9 & 1 \end{bmatrix}$.
<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Prove the following properties of the trace:

*  $\text{tr}(A+B) = \text{tr}(A) + \text{tr}(B)$
*  $\text{tr}(\alpha A) = \alpha \text{tr}(A)$
*  $\text{tr}(I_n) = n$
<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">

<div style="text-align: right;">
[Solution]( )
</div> 
</div>








<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->






## Eigenvalues and Eigenvectors

Eigenvalues and eigenvectors provide a way to characterize a matrix \(\mathbf{A} \in \mathbb{R}^{n \times n}\) and its associated linear mapping.



<div class="definition">
**Eigenvalue & Eigenvector**:  
   A scalar \(\lambda \in \mathbb{R}\) is an eigenvalue of \(\mathbf{A}\) and a nonzero vector \(\mathbf{x} \in \mathbb{R}^n\) is a corresponding eigenvector if  
  \[
  \mathbf{A}\mathbf{x} = \lambda \mathbf{x}.
  \]  
This is called the **eigenvalue equation**.
</div>




<div class="definition">
The span of the set of eigenvectors associated with \(\lambda\) spans a subspace \(E_\lambda \subset \mathbb{R}^n\) known as the **eigenspace**.
</div>



<div class="definition">
The set of all eigenvalues of \(\mathbf{A}\) is called the **eigenspectrum**.
</div>




<div class="definition">
Number of times \(\lambda\) appears as a root of the characteristic polynomial \(p_ \mathbf{A}(\lambda) = \det(\mathbf{A} - \lambda I)\) is called the **algebraic multiplicity** of the eigenvalue.
</div>




<div class="definition">
The dimension of the eigenspace associated with \(\lambda\) is called the **geometric multiplicity**.
</div>




<div class="example">
Find the eigenspace(s) of the matrix
\[
\mathbf{A} = \begin{pmatrix}
2 & 1\\
0 & 2
\end{pmatrix}.
\]

**Step 1: Find the eigenvalues**

Compute the characteristic polynomial:
\[
\det(\mathbf{A} - \lambda \mathbf{I})
= \det \begin{pmatrix}
2-\lambda & 1\\
0 & 2-\lambda
\end{pmatrix}
= (2-\lambda)^2.
\]

So the only eigenvalue is  
\[
\lambda = 2
\quad \text{(with algebraic multiplicity 2).}
\]

**Step 2: Find the eigenspace for \(\lambda = 2\)**

Form the matrix:
\[
\mathbf{A} - 2\mathbf{I} =
\begin{pmatrix}
2-2 & 1\\
0 & 2-2
\end{pmatrix}
=
\begin{pmatrix}
0 & 1\\
0 & 0
\end{pmatrix}.
\]

Solve:
\[
(\mathbf{A} - 2\mathbf{I})\mathbf{x} = 0.
\]

This gives:
\[
x_2 = 0.
\]

So \(x_1\) is free, and the solution vectors have the form:
\[
\mathbf{x} = \begin{pmatrix}x_1\\ 0\end{pmatrix}
= x_1 \begin{pmatrix}1\\ 0\end{pmatrix}.
\]

**Step 3: Write the eigenspace**

The eigenspace is:
\[
E_2 = \text{span}\left\{\begin{pmatrix}1\\ 0\end{pmatrix}\right\}.
\]

So the eigenspace is **one-dimensional**, even though the eigenvalue has algebraic multiplicity 2.
</div>




### Key Properties

Eigenvalues and eigenvectors have several key properties:

- \(\mathbf{A}\) and \(\mathbf{A}^\top\) have the same eigenvalues, not necessarily the same eigenvectors.  
- Similar matrices have identical eigenvalues. Matrices \( \mathbf{A} \) and \( \mathbf{D} \) are similar if \(\mathbf{A} = \mathbf{P} \mathbf{D} \mathbf{P}^{-1} \) for some matrix \( \mathbf{P} \). 
- Symmetric, positive definite matrices have real, positive eigenvalues.  
-  A matrix with \(n\) distinct eigenvalues has linearly independent eigenvectors forming a basis of \(\mathbb{R}^n\).  
- **Defective matrices**: Have fewer than \(n\) linearly independent eigenvectors.  


<div class="example">
Consider the matrix 

\[
\mathbf{A} = 
\begin{bmatrix} 
2 & 1 \\ 
0 & 3 
\end{bmatrix}
\]

and the invertible matrix 

\[
\mathbf{P} = 
\begin{bmatrix} 
1 & 1 \\ 
0 & 1 
\end{bmatrix}.
\]

We can compute a matrix \(\mathbf{B}\) that is **similar** to \(\mathbf{A}\) using the formula:

\[
\mathbf{B} = \mathbf{P}^{-1} \mathbf{A} \mathbf{P}
\]

First, find \(\mathbf{P}^{-1}\):

\[
\mathbf{P}^{-1} = 
\begin{bmatrix} 
1 & -1 \\ 
0 & 1 
\end{bmatrix}.
\]

Then,

\[
\mathbf{B} = 
\begin{bmatrix} 1 & -1 \\ 0 & 1 \end{bmatrix}
\begin{bmatrix} 2 & 1 \\ 0 & 3 \end{bmatrix}
\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}
=
\begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}.
\]

Thus, \(\mathbf{A}\) and \(\mathbf{B}\) are **similar matrices**, since there exists an invertible matrix \(\mathbf{P}\) such that \(\mathbf{B} = \mathbf{P}^{-1} \mathbf{A} \mathbf{P}\).  One can verify that matrices $\mathbf{A}$ and $\mathbf{B}$ have the same eigenvalues ($\lambda = 2$ and 3).
</div>

<div class="theorem">
**Spectral Theorem:** If \(\mathbf{A}\) is symmetric, there exists an orthonormal basis of $\mathbb{R}^n$ consisting of eigenvectors of $\mathbf{A}$ (and all eigenvalues are real).  Furthermore, $\mathbf{A}$ can be decomposed as  
  \[
  \mathbf{A} = \mathbf{P}\mathbf{D}\mathbf{P}^\top
  \]  
  where \(\mathbf{P}\) contains eigenvectors and \(\mathbf{D}\) is diagonal with eigenvalues.
</div>




<div class="example">

We illustrate the decomposition of
\[
\mathbf{A} =
\begin{pmatrix}
2 & 1\\
1 & 2
\end{pmatrix}.
\]

First, we need to find the eigenvalues/ eigenvectors for $\mathbf{A}$.

\[
\det(\mathbf{A} - \lambda \mathbf{I}) =
\det\begin{pmatrix}
2-\lambda & 1\\
1 & 2-\lambda
\end{pmatrix}
= (2-\lambda)^2 - 1.
\]

So the eigenvalues are:
\[
\lambda_1 = 3, \qquad \lambda_2 = 1.
\]

For \(\lambda_1 = 3\),
\[
(\mathbf{A} - 3\mathbf{I})\mathbf{v}_1
=
\begin{pmatrix}
-1 & 1\\
1 & -1
\end{pmatrix}
\begin{pmatrix}x\\y\end{pmatrix}
= 0
\Rightarrow x = y.
\]
Choose:
\[
\mathbf{v}_1 = \frac{1}{\sqrt{2}}
\begin{pmatrix}1\\1\end{pmatrix}.
\]
For \(\lambda_2 = 1\),
\[
(\mathbf{A}-\mathbf{I})\mathbf{v}_2
=
\begin{pmatrix}
1 & 1\\
1 & 1
\end{pmatrix}
\begin{pmatrix}x\\y\end{pmatrix}
= 0
\Rightarrow x = -y.
\]
Choose:
\[
\mathbf{v}_2 = \frac{1}{\sqrt{2}}
\begin{pmatrix}1\\-1\end{pmatrix}.
\]

To form $\mathbf{P}$ and $\mathbf{D}$, take the normalized eigenvectors to form the columns of $\mathbf{P}$ and the eigenvalues to form the main diagonal of $\mathbf{D}$,
\[
\mathbf{P}
=
\begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\[4pt]
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}
\end{pmatrix},
\qquad
\mathbf{D}
=
\begin{pmatrix}
3 & 0\\
0 & 1
\end{pmatrix}.
\]

To check,
\[
\mathbf{PDP}^\top
=
\begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}
\end{pmatrix}
\begin{pmatrix}
3 & 0\\
0 & 1
\end{pmatrix}
\begin{pmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}
\end{pmatrix}
=
\begin{pmatrix}
2 & 1\\
1 & 2
\end{pmatrix}
= \mathbf{A}.
\]

</div>


### Relations to Determinant and Trace

Eigenvalues and eigenvectors are related to the determinant and trace of a matrix.  For example, \[\det(\mathbf{A}) = \prod_{i=1}^n \lambda_i.\]  Furthermore, \[\text{tr}(\mathbf{A}) = \sum_{i=1}^n \lambda_i.\]  Geometrically, eigenvectors are directions stretched by \(\lambda_i\); determinant gives volume scaling, trace gives scaling of perimeter.



<div class="example">
Let $\mathbf{A} = \begin{pmatrix}4 & 2 \\ 1 & 3\end{pmatrix}$.  Then  

-  Eigenvalues: $\lambda_1 = 2, \;\;\; \lambda_2 = 5$  
-  Eigenspaces: $E_2 = \text{span}\{[1, -1]^\top\}, \;\;\;  E_5 = \text{span}\{[2,1]^\top\}$
-  Trace is $\text{sum of diagonals of the matrix} = 4 + 3  = 7 = 5 + 2 = \text{sum of the eigenvalues}$
-  Determinant is $\text{by the formula} = 4(3) - 2(1) = 10 = 5 \times 2 = \text{product of the eigenvalues}$.
</div>

**Google's PageRank  Algorithm** uses the eigenvector of the maximal eigenvalue (\(\lambda = 1\)) of the web connectivity matrix to rank web pages. 



---



### Exercises {.unnumbered .unlisted}






<div class="exercise">
Find the characteristic equation, eigenvalues, eigenspace, determinant, and trace corresponding to \[\mathbf{A} = \begin{bmatrix} 1 & 4\\3 & 2 \end{bmatrix}.\]

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find the characteristic equation, eigenvalues, eigenspace, determinant, and trace corresponding to \[\mathbf{A} = \begin{bmatrix} 2 & 2\\1 & 3 \end{bmatrix}.\]

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find the characteristic equation, eigenvalues, eigenspace, determinant, and trace corresponding to \[\mathbf{A} = \begin{bmatrix}1&0&0\\0&1&2\\0&0&0\end{bmatrix}.\]

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find the characteristic equation, eigenvalues, eigenspace, determinant, and trace corresponding to \[\mathbf{A} = \begin{bmatrix}2&0&4\\0&3&0\\0&1&2\end{bmatrix}.\]

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Given a matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$, we can always obtain a symmetric positive semidefinite matrix $\mathbf{S} \in \mathbb{R}^{n \times n}$ by defining $\mathbf{S} = \mathbf{A}^T\mathbf{A}$. Prove this statement for $2 \times 2$, $3 \times 2$ and $2 \times 3$ matrices.  

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Explain why geometrically, an eigenvector is a vector whose direction is unchanged by multiplying by matrix $\mathbf{A}$.

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">

<div style="text-align: right;">
[Solution]( )
</div> 
</div>










<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->







## Cholesky Decomposition

The **Cholesky decomposition** is a square-root-like factorization for symmetric, positive definite matrices. It generalizes the concept of a square root from numbers to matrices.




<div class="theorem">
**Cholesky Decomposition:**   A symmetric, positive definite matrix \(\mathbf{A} \in \mathbb{R}^{n \times n}\) can be factorized as:
\[
\mathbf{A} = L L^\top,
\]
where \(L\) is a **lower-triangular matrix** with positive diagonal entries. The matrix \(L\) is called the **Cholesky factor** of \(\mathbf{A}\) and is unique.
</div>



<div class="example">
For
\[
\mathbf{A} = \begin{bmatrix}
a_{11} & a_{21} & a_{31} \\
a_{21} & a_{22} & a_{32} \\
a_{31} & a_{32} & a_{33}
\end{bmatrix},
\quad
L = \begin{bmatrix}
l_{11} & 0 & 0 \\
l_{21} & l_{22} & 0 \\
l_{31} & l_{32} & l_{33}
\end{bmatrix},
\]
the components of \(L\) are computed as:
\[
\begin{aligned}
l_{11} &= \sqrt{a_{11}}, & l_{22} &= \sqrt{a_{22} - l_{21}^2}, & l_{33} &= \sqrt{a_{33} - (l_{31}^2 + l_{32}^2)}, \\
l_{21} &= \frac{a_{21}}{l_{11}}, & l_{31} &= \frac{a_{31}}{l_{11}}, & l_{32} &= \frac{a_{32} - l_{31}l_{21}}{l_{22}}.
\end{aligned}
\]
</div>



<div class="example">
Let
\[
\mathbf{A} =
\begin{bmatrix}
4 & 2 & 2 \\
2 & 5 & 1 \\
2 & 1 & 3
\end{bmatrix}
\]
We want to find a lower triangular matrix \(\mathbf{L}\) such that
\[
\mathbf{A} = \mathbf{L}\mathbf{L}^T.
\]

Assume
\[
\mathbf{L} =
\begin{bmatrix}
l_{11} & 0 & 0 \\
l_{21} & l_{22} & 0 \\
l_{31} & l_{32} & l_{33}
\end{bmatrix}.
\]
By multiplying \(\mathbf{L}\mathbf{L}^T\) and comparing entries with \(\mathbf{A}\), we obtain:
\begin{align*}
l_{11}^2 = 4 &\Rightarrow l_{11} = 2 & 2l_{21} = 2 &\Rightarrow l_{21} = 1\\
2l_{31} = 2 &\Rightarrow l_{31} = 1 &1^2 + l_{22}^2 = 5 &\Rightarrow l_{22} = 2 \\
1(1) + 2l_{32} = 1 &\Rightarrow l_{32} = 0 & 1^2 + 0^2 + l_{33}^2 = 3 &\Rightarrow l_{33} = \sqrt{2}
\end{align*}

Therefore,
\[
\mathbf{L} =
\begin{bmatrix}
2 & 0 & 0 \\
1 & 2 & 0 \\
1 & 0 & \sqrt{2}
\end{bmatrix}
\]

and

\[
\mathbf{L}^T =
\begin{bmatrix}
2 & 1 & 1 \\
0 & 2 & 0 \\
0 & 0 & \sqrt{2}
\end{bmatrix},
\]
and
\[
\mathbf{A}
=
\mathbf{L}\mathbf{L}^T.
\]
</div>


Cholesky decompositions provide an efficient computation of determinants: \(\det(\mathbf{A}) = \prod_i l_{ii}^2\).  They are also important to provide numerical stability in machine learning algorithms





<div class="example">
Find the determinant of 
\[
\mathbf{A} =
\begin{bmatrix}
4 & 2 & 2 \\
2 & 5 & 1 \\
2 & 1 & 3
\end{bmatrix}.
\]

Since \[
\mathbf{A}
=
\mathbf{L}\mathbf{L}^T,
\]
we have
\[
\det(\mathbf{A})
=
\det(\mathbf{L}\mathbf{L}^T)
=
\det(\mathbf{L}) \det(\mathbf{L}^T)
=(2 \times 2 \times \sqrt{2}) \times (2 \times 2 \times \sqrt{2})
= 32.
\]
</div>



---



### Exercises {.unnumbered .unlisted}





<div class="exercise">
 Let $\mathbf{A} = \begin{bmatrix}9 & 6 \\ 6 & a \end{bmatrix}$, $a > 4$.  Verify that this matrix satisfies the criteria for the Cholesky decomposition and find it.  Hint, you are going to have to derive the equations yourself. 
    
<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Compute the Cholesky decomposition for \[\mathbf{A} = \begin{bmatrix} 1&0&0\\0&1&0\\0&0&1 \end{bmatrix}.\] 

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Compute the Cholesky decomposition for \[\mathbf{A} = \begin{bmatrix} 5&0&0\\0&10&0\\0&0&0.17 \end{bmatrix}.\] 

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Compute the Cholesky decomposition for \[\mathbf{A} = \begin{bmatrix} 2&-1&0\\-1&2&-1\\0&-1&2 \end{bmatrix}.\] 

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Compute the Cholesky decomposition for \[\mathbf{A} = \begin{bmatrix} 25&15&-5\\15&18&0\\-5&0&11 \end{bmatrix}.\] 

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Show that if $\mathbf{A}$ is a symmetric, positive definite matrix, then \[det(\mathbf{A}) = det(\mathbf{L})^2= \left(\prod_{i=1}^n l_{ii}\right)^2,\] where $\mathbf{L}$ is the Cholesky factor of $\mathbf{A}$.  Verify each step. 

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">

<div style="text-align: right;">
[Solution]( )
</div> 
</div>










<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->







## Eigendecomposition and Diagonalization



<div class="definition">
 A **diagonal matrix** has zeros on all off-diagonal elements:
\[
\mathbf{D} = \begin{bmatrix}
c_1 & 0 & \cdots & 0 \\
0 & c_2 & \cdots & 0 \\
\vdots & & \ddots & \vdots \\
0 & 0 & \cdots & c_n
\end{bmatrix}.
\]
</div>




<div class="lemma">
Let $\mathbf{D}$ be a diagonal matrix.  Then $\mathbf{D}$ has the following properties:

- \(\det(\mathbf{D}) = \prod_i c_i\)  
- \(\mathbf{D}^k = \text{diag}(c_1^k, \dots, c_n^k)\)  
- \(\mathbf{D}^{-1} = \text{diag}(1/c_1, \dots, 1/c_n)\) (if all \(c_i \neq 0\))  
</div>





<div class="example">
Consider the diagonal matrix
\[
\mathbf{D} =
\begin{bmatrix}
2 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & 3 & 0 \\
0 & 0 & 0 & 4
\end{bmatrix}.
\]

The determinant is the product of the diagonal entries:
\[
\det(\mathbf{D}) = 2 \cdot (-1) \cdot 3 \cdot 4 = -24.
\]

The inverse is obtained by taking the reciprocal of each diagonal entry:
\[
\mathbf{D}^{-1} =
\begin{bmatrix}
\frac{1}{2} & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & \frac{1}{3} & 0 \\
0 & 0 & 0 & \frac{1}{4}
\end{bmatrix}.
\]


Powers of a diagonal matrix are computed by raising each diagonal entry to the given power:
\[
\mathbf{D}^5 =
\begin{bmatrix}
2^5 & 0 & 0 & 0 \\
0 & (-1)^5 & 0 & 0 \\
0 & 0 & 3^5 & 0 \\
0 & 0 & 0 & 4^5
\end{bmatrix}
=
\begin{bmatrix}
32 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & 243 & 0 \\
0 & 0 & 0 & 1024
\end{bmatrix}.
\]

</div>



### Diagonalizable Matrices



<div class="definition">
 A matrix \(\mathbf{A} \in \mathbb{R}^{n \times n}\) is **diagonalizable** if there exists an invertible matrix \(\mathbf{P}\) such that:
\[
\mathbf{D} = \mathbf{P}^{-1} \mathbf{A} \mathbf{P},
\]
where \(\mathbf{D}\) is diagonal.
</div>

 A matrix $\mathbf{A}$ is diagonalizable **if and only if** \(\mathbf{A}\) has \(n\) linearly independent eigenvectors
\[
\mathbf{A}\mathbf{P} = \mathbf{P}\mathbf{D} \quad \Leftrightarrow \quad \mathbf{A} p_i = \lambda_i p_i, \ i=1,\dots,n.
\]
Here, the columns of \(\mathbf{P}\) are eigenvectors of \(\mathbf{A}\), and the diagonal entries of \(\mathbf{D}\) are the eigenvalues of \(\mathbf{A}\).

### Eigendecomposition Theorems


<div class="theorem">
Let $\mathbf{A}$ be a matrix.  Then there exist a diagonal matrix $\mathbf{D}$ and matrix $\mathbf{P}$ consisting of eigenvectors of $\mathbf{A}$ such that
\[
\mathbf{A} = \mathbf{P} \mathbf{D} \mathbf{P}^{-1},
\]
if and only if eigenvectors of \(\mathbf{A}\) form a basis of \(\mathbb{R}^n\). Only **non-defective matrices** (ones with $n$ linear independent eigenvectors) are diagonalizable.
</div>



<div class="theorem">
 A symmetric matrix \(S \in \mathbb{R}^{n \times n}\) is always diagonalizable. By the spectral theorem, the eigenvectors can form an orthonormal basis (ONB), giving:
\[
\mathbf{D} = \mathbf{P}^\top \mathbf{S} \mathbf{P},
\]
with \(\mathbf{P}\) orthogonal.
</div>

Geometrically, eigendecomposition represents a basis change to the eigenbasis.  \(\mathbf{D}\) scales vectors along eigenvectors by eigenvalues \(\lambda_i\) while \(\mathbf{P}\) maps scaled vectors back to the standard basis.



<div class="example">
Consider the matrix
\[
\mathbf{A} =
\begin{bmatrix}
4 & 1 \\
1 & 2
\end{bmatrix}.
\]

The characteristic polynomial is
\[
\det(\mathbf{A} - \lambda \mathbf{I})
=
\begin{vmatrix}
4 - \lambda & 1 \\
1 & 2 - \lambda
\end{vmatrix}
=
(4 - \lambda)(2 - \lambda) - 1 = \lambda^2 - 6\lambda + 7 = 0.
\]
Solving,
\[
\lambda_{1,2} = 3 \pm \sqrt{2}.
\]

The eigenvector for \( \lambda_1 = 3 + \sqrt{2} \) is found when we solve \((\mathbf{A} - \lambda_1 \mathbf{I})\mathbf{v} = \mathbf{0}\):
\[
\begin{bmatrix}
1 - \sqrt{2} & 1 \\
1 & -1 - \sqrt{2}
\end{bmatrix}
\mathbf{v} = \mathbf{0}.
\]
A corresponding eigenvector is
\[
\mathbf{v}_1 =
\begin{bmatrix}
1 \\
\sqrt{2} - 1
\end{bmatrix}.
\]

The eigenvector for \( \lambda_2 = 3 - \sqrt{2} \) is found when we solve \((\mathbf{A} - \lambda_2 \mathbf{I})\mathbf{v} = \mathbf{0}\):
\[
\begin{bmatrix}
1 + \sqrt{2} & 1 \\
1 & -1 + \sqrt{2}
\end{bmatrix}
\mathbf{v} = \mathbf{0}.
\]
A corresponding eigenvector is
\[
\mathbf{v}_2 =
\begin{bmatrix}
1 \\
- (1 + \sqrt{2})
\end{bmatrix}.
\]


Let
\[
\mathbf{P} =
\begin{bmatrix}
1 & 1 \\
\sqrt{2} - 1 & -(1 + \sqrt{2})
\end{bmatrix},
\quad
\mathbf{D} =
\begin{bmatrix}
3 + \sqrt{2} & 0 \\
0 & 3 - \sqrt{2}
\end{bmatrix}.
\]
Then the eigendecompositio* of \(\mathbf{A}\) is
\[
\mathbf{A} = \mathbf{P}\mathbf{D}\mathbf{P}^{-1}.
\]
</div>



<div class="note">
Eigendecomposition requires square matrices. For general matrices, the **Singular Value Decomposition (SVD)** is used.
</div>






---



### Exercises {.unnumbered .unlisted}





<div class="exercise">
Discuss 2 reasons why we might want to complete an eigendecomposition.

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find an eigendecomposition for \[\mathbf{A} = \begin{bmatrix}0&1\\ 3&2 \end{bmatrix}.\]  Use this to compute $\mathbf{A}^4$.

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find an eigendecomposition for \[\mathbf{A} = \begin{bmatrix}0&2\\ 2&3 \end{bmatrix}.\] Use this to compute $\mathbf{A}^5$. 

<div style="text-align: right;">
[Solution]( )
</div> 
</div>



<div class="exercise">
Find an eigendecomposition for \[\mathbf{A} = \begin{bmatrix}2&3\\ 2&1 \end{bmatrix}.\] Use this to compute $\mathbf{A}^3$. 

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Show that, if $\mathbf{A}$ has an eigendecomposition, that $det(\mathbf{A}) = \prod_{i=1}^n \lambda_{i}$, where the product is over the eigenvalues. 
    
<div style="text-align: right;">
[Solution]( )
</div> 
</div>








<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->






## Singular Value Decomposition (SVD)

The Singular Value Decomposition (SVD) is a fundamental matrix decomposition method in linear algebra, applicable to all matrices (square or rectangular). It expresses a matrix \(\mathbf{A} \in \mathbb{R}^{m \times n}\) as:

\[
\mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top
\]

where:  
- \(\mathbf{U} \in \mathbb{R}^{m \times m}\) is an orthogonal matrix of **left-singular vectors** \(u_i\),  
- \(\mathbf{V} \in \mathbb{R}^{n \times n}\) is an orthogonal matrix of **right-singular vectors** \(v_j\),  
- \(\mathbf{\Sigma} \in \mathbb{R}^{m \times n}\) is diagonal with **non-negative singular values** \(\sigma_i\) (ordered \(\sigma_1 \ge \sigma_2 \ge \dots \ge 0\)).

### Geometric Intuition

The SVD can be interpreted as three sequential linear transformations:  

1. **Basis change** in the domain via \(\mathbf{V}^\top\)  
2. **Scaling** by singular values via \(\mathbf{\Sigma}\) and possibly dimension change  
3. **Basis change** in the codomain via \(\mathbf{U}\)  

Unlike eigendecomposition, the domain and codomain in SVD can have different dimensions, and \(\mathbf{U}\) and \(\mathbf{V}\) are orthonormal but generally not inverses of each other.

<div class="example">
Consider the matrix
\[
\mathbf{A} =
\begin{bmatrix}
3 & 1 \\
0 & 2
\end{bmatrix}.
\]

It has a Singular Value Decomposition given by
\[
\mathbf{A}
=
\underbrace{\mathbf{U}}_{\text{basis change}} \;\;
\underbrace{\boldsymbol{\Sigma}}_{\text{scaling}} \;\;
\underbrace{\mathbf{V}^\top}_{\text{basis change}}
=
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
3 & 0 \\
0 & 2
\end{bmatrix}
\frac{1}{\sqrt{2}}
\begin{bmatrix}
1 & 1 \\
1 & -1
\end{bmatrix}.
\]

**Geometric Interpretation:**

1. **\( \mathbf{V}^\top \)** Rotates the input vector into a new orthonormal basis.

2. **\( \boldsymbol{\Sigma} \)** Scales the coordinates by factors of **3** and **2** along perpendicular axes.

3. **\( \mathbf{U} \)** Rotates the result into the output space.
</div>



### Construction of the SVD

The following are the steps required to complete a SVD:

1. Compute \(\mathbf{A}^\top \mathbf{A}\) (symmetric, positive semidefinite)  
2. Diagonalize \(\mathbf{A}^\top \mathbf{A} = \mathbf{P} D \mathbf{P}^\top\) to obtain right-singular vectors \(\mathbf{V} = \mathbf{P}\)  
3. Singular values \(\sigma_i = \sqrt{\lambda_i}\), where \(\lambda_i\) are eigenvalues of \(\mathbf{A}^\top \mathbf{A}\)  
4. Compute left-singular vectors \(u_i = \frac{1}{\sigma_i} \mathbf{A} v_i\)  
5.  Assemble \(\mathbf{U}, \mathbf{\Sigma}, \mathbf{V}\) to form \(\mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top\)  

<div class="example">

Consider the matrix
\[
\mathbf{A} =
\begin{bmatrix}
3 & 1 \\
0 & 2
\end{bmatrix}.
\]
We will compute its Singular Value Decomposition
\[
\mathbf{A} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top.
\]

**Step 1: Compute \( \mathbf{A}^\top \mathbf{A} \)**

\[
\mathbf{A}^\top \mathbf{A}
=
\begin{bmatrix}
3 & 0 \\
1 & 2
\end{bmatrix}
\begin{bmatrix}
3 & 1 \\
0 & 2
\end{bmatrix}
=
\begin{bmatrix}
9 & 3 \\
3 & 5
\end{bmatrix}.
\]

**Step 2: Singular Values**

The eigenvalues of \( \mathbf{A}^\top \mathbf{A} \) satisfy
\[
\det(\mathbf{A}^\top \mathbf{A} - \lambda \mathbf{I})
=
\begin{vmatrix}
9 - \lambda & 3 \\
3 & 5 - \lambda
\end{vmatrix}
=
\lambda^2 - 14\lambda + 36 = 0.
\]
So,
\[
\lambda_1 = 9, \quad \lambda_2 = 4.
\]
The singular values are
\[
\sigma_1 = 3, \quad \sigma_2 = 2.
\]
Thus,
\[
\boldsymbol{\Sigma} =
\begin{bmatrix}
3 & 0 \\
0 & 2
\end{bmatrix}.
\]

**Step 3: Right Singular Vectors (\(\mathbf{V}\))**

Eigenvectors of \( \mathbf{A}^\top \mathbf{A} \):

- For \( \lambda = 9 \): eigenvector \( \begin{bmatrix} 1 \\ 1 \end{bmatrix} \)
- For \( \lambda = 4 \): eigenvector \( \begin{bmatrix} 1 \\ -1 \end{bmatrix} \)

After normalization,
\[
\mathbf{V} =
\frac{1}{\sqrt{2}}
\begin{bmatrix}
1 & 1 \\
1 & -1
\end{bmatrix}.
\]
This represents the **first basis change** in the domain.

**Step 4: Left Singular Vectors (\(\mathbf{U}\))**

Using \( \mathbf{U} = \mathbf{A}\mathbf{V}\boldsymbol{\Sigma}^{-1} \),
we obtain
\[
\mathbf{U} =
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}.
\]
(This matrix happens to be the identity here, meaning no rotation in the codomain.)

**Step 5: Final SVD**

\[
\mathbf{A}
=
\underbrace{\mathbf{U}}_{\text{basis change}} \;\;
\underbrace{\boldsymbol{\Sigma}}_{\text{scaling}} \;\;
\underbrace{\mathbf{V}^\top}_{\text{basis change}}
=
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
3 & 0 \\
0 & 2
\end{bmatrix}
\frac{1}{\sqrt{2}}
\begin{bmatrix}
1 & 1 \\
1 & -1
\end{bmatrix}.
\]

</div>


<div class="note">
The SVD always exists for any matrix. For symmetric positive definite matrices, it coincides with the eigendecomposition.
</div>

<div class="example">
Consider the \( 3 \times 2 \) matrix
\[
\mathbf{A} =
\begin{bmatrix}
1 & 0 \\
0 & 2 \\
0 & 0
\end{bmatrix}.
\]
We compute the Singular Value Decomposition of $\mathbf{A}$.

**Step 1: Compute \( \mathbf{A}^\top \mathbf{A} \)**

\[
\mathbf{A}^\top \mathbf{A}
=
\begin{bmatrix}
1 & 0 & 0 \\
0 & 2 & 0
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
0 & 2 \\
0 & 0
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 \\
0 & 4
\end{bmatrix}.
\]

**Step 2: Singular Values**

The eigenvalues of \( \mathbf{A}^\top \mathbf{A} \) are
\[
\lambda_1 = 4, \quad \lambda_2 = 1.
\]
Thus the singular values are
\[
\sigma_1 = 2, \quad \sigma_2 = 1 \;\;\; \Longrightarrow \;\;\;
\boldsymbol{\Sigma} =
\begin{bmatrix}
2 & 0 \\
0 & 1 \\
0 & 0
\end{bmatrix}.
\]

**Step 3: Right Singular Vectors (\(\mathbf{V}\))**

Since \( \mathbf{A}^\top \mathbf{A} \) is diagonal,
\[
\mathbf{V} =
\begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix}
\quad \text{(any orthonormal ordering is valid)}.
\]
This matrix represents a **basis change in the domain** \( \mathbb{R}^2 \).

**Step 4: Left Singular Vectors (\(\mathbf{U}\))**

Compute
\[
\mathbf{U} = \mathbf{A}\mathbf{V}\boldsymbol{\Sigma}^{-1}.
\]
The resulting orthonormal basis is
\[
\mathbf{U} =
\begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1
\end{bmatrix}.
\]
This is a basis for \( \mathbb{R}^3 \), extending the image of \( \mathbf{A} \) to a full orthonormal basis.

**Step 5: Final SVD**

\[
\mathbf{A}
=
\underbrace{\mathbf{U}}_{\text{basis change in } \mathbb{R}^3}
\underbrace{\boldsymbol{\Sigma}}_{\text{scaling}}
\underbrace{\mathbf{V}^\top}_{\text{basis change in } \mathbb{R}^2}.
\]
Explicitly,
\[
\mathbf{A}
=
\begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
2 & 0 \\
0 & 1 \\
0 & 0
\end{bmatrix}
\begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix}.
\]

</div>

### Comparison: Eigenvalue Decomposition vs SVD

| Feature | Eigendecomposition | SVD |
|---------|------------------|-----|
| Matrix type | Square only |  Any \(m \times n\) |
| Basis vectors | Not necessarily orthonormal | Orthonormal (U, V) |
| Diagonal entries | Eigenvalues (can be negative/complex) | Non-negative singular values |
| Basis change | Same vector space | Domain and codomain can differ |
| Relation to eigenvectors | Only eigenvectors of square matrices | Left-singular vectors = eigenvectors of \(\mathbf{A}\mathbf{A}^\top\), Right-singular = eigenvectors of \(\mathbf{A}^\top \mathbf{A}\) |



---



### Exercises {.unnumbered .unlisted}



<div class="exercise">
Find a SVD of \[\mathbf{A} = \begin{bmatrix}3&0\\4&5 \end{bmatrix}.\]

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find a SVD of \[\mathbf{A} = \begin{bmatrix}3&2&2\\2&3&-2 \end{bmatrix}.\]

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find a SVD of \[\mathbf{A} = \begin{bmatrix}4&0\\3&-5 \end{bmatrix}.\]
    
<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find a SVD of \[\mathbf{A} = \begin{bmatrix}1&0&1\\-1&1&0 \end{bmatrix}.\]

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find a SVD of \[\mathbf{A} = \begin{bmatrix}1&-1\\0&1\\1&0 \end{bmatrix}.\]

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find a SVD of \[\mathbf{A} = \begin{bmatrix}1&1&1\\ -1&0&-2\\ 1&2&0 \end{bmatrix}.\]

<div style="text-align: right;">
[Solution]( )
</div> 
</div>








<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->








##  Matrix  Approximation via SVD

The SVD of a matrix \(\mathbf{A} \in \mathbb{R}^{m \times n}\):
\[
\mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top
\]
allows us to represent \(\mathbf{A}\) as a sum of **rank-1 matrices**:
\[
\mathbf{A}_i := u_i v_i^\top, \quad 
\mathbf{A} = \sum_{i=1}^r \sigma_i \mathbf{A}_i,
\]
where \(r = \text{rank}(\mathbf{A})\) and \(\sigma_i\) are singular values.  




<div class="definition">
 A **rank-k approximation** of \(\mathbf{A}\) (with \(k < r\)) is:
\[
\mathbf{A}^{(k)} = \sum_{i=1}^k \sigma_i u_i v_i^\top.
\]
</div>

 A rank-$k$ approximation reduces storage and computation costs compared to the original.  For example, a \(1432 \times 1910\) image approximated with rank-5 requires 16,715 numbers instead of 2,735,120 (~0.6%).  
 
 
<div class="example">
 
Write $\mathbf{A}$ as the sum of rank 1 matrices where
\[
\mathbf{A} =
\begin{bmatrix}
3 & 0 \\
0 & 1
\end{bmatrix}.
\]



**Step 1: Singular Value Decomposition**

Since \( \mathbf{A} \) is diagonal with positive entries, its SVD is especially simple:
\[
\mathbf{A} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top,
\]
where
\[
\mathbf{U} =
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix},
\quad
\boldsymbol{\Sigma} =
\begin{bmatrix}
3 & 0 \\
0 & 1
\end{bmatrix},
\quad
\mathbf{V} =
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}.
\]

The singular values are
\[
\sigma_1 = 3, \quad \sigma_2 = 1.
\]

**Step 2: Rank-1 Decomposition**

The SVD gives the expansion
\[
\mathbf{A}
=
\sum_{i=1}^2 \sigma_i \mathbf{u}_i \mathbf{v}_i^\top,
\]
where \( \mathbf{u}_i \) and \( \mathbf{v}_i \) are the columns of \( \mathbf{U} \) and \( \mathbf{V} \).


**Step 3: Individual Rank-1 Matrices**

- First term:
\[
\sigma_1 \mathbf{u}_1 \mathbf{v}_1^\top
=
3
\begin{bmatrix}
1 \\ 0
\end{bmatrix}
\begin{bmatrix}
1 & 0
\end{bmatrix}
=
\begin{bmatrix}
3 & 0 \\
0 & 0
\end{bmatrix}.
\]

- Second term:
\[
\sigma_2 \mathbf{u}_2 \mathbf{v}_2^\top
=
1
\begin{bmatrix}
0 \\ 1
\end{bmatrix}
\begin{bmatrix}
0 & 1
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix}.
\]

Each matrix has **rank 1**.


**Step 4: Sum of Rank-1 Matrices**

Adding the two rank-1 matrices:
\[
\mathbf{A}
=
\begin{bmatrix}
3 & 0 \\
0 & 0
\end{bmatrix}
+
\begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix}
=
\begin{bmatrix}
3 & 0 \\
0 & 1
\end{bmatrix}.
\]


Each term \( \sigma_i \mathbf{u}_i \mathbf{v}_i^\top \) is a rank-1 matrix.  The SVD expresses \( \mathbf{A} \) as a sum of rank-1 outer products.  Truncating this sum gives the **best low-rank approximation** of \( \mathbf{A} \) (in the least-squares sense).

</div>




### Error Measurement

The **spectral norm** of a matrix \(\mathbf{A}\) is:
\[
\|\mathbf{A}\|_2 := \max_{x \neq 0} \frac{\|\mathbf{A}x\|_2}{\|x\|_2}.
\]
The spectral norm of \(\mathbf{A}\) is its largest singular value \(\sigma_1\).  



<div class="theorem">
**Eckart-Young Theorem:** For any rank-\(k\) approximation \(\mathbf{A}^{(k)}\):
\[
\mathbf{A}^{(k)} = \arg \min_{\text{rank}(\mathbf{B}) = k} \|\mathbf{A} - \mathbf{B}\|_2
\]
\[
\|\mathbf{A} - \mathbf{A}^{(k)}\|_2 = \sigma_{k+1}
\]
</div>

The SVD provides the best low-rank approximation in the spectral norm sense.  This is used for **lossy compression**, dimensionality reduction, noise filtering, and regularization.

 
 
<div class="example">
Consider the matrix 
\[
\mathbf{A} = \begin{bmatrix} 3 & 0 \\ 0 & 1 \end{bmatrix}.
\]



Since \(\mathbf{A}\) is diagonal, its singular values are simply the diagonal entries in descending order:
\[
\sigma_1 = 3, \quad \sigma_2 = 1.
\]
We can write the SVD as 
\[
\mathbf{A} = U \Sigma V^\top
\]
with 
\[
U = V = I_2, \quad \Sigma = \begin{bmatrix} 3 & 0 \\ 0 & 1 \end{bmatrix}.
\]


The rank-1 approximation of \(\mathbf{A}\) is obtained by keeping only the largest singular value \(\sigma_1 = 3\):
\[
\mathbf{A}_1 = \sigma_1 u_1 v_1^\top = 3 \begin{bmatrix} 1 \\ 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \end{bmatrix} = \begin{bmatrix} 3 & 0 \\ 0 & 0 \end{bmatrix}.
\]


The **Eckart-Young Theorem** states that this rank-1 approximation \(\mathbf{A}_1\) is optimal in the sense that it minimizes the spectral norm of the difference:
\[
\|\mathbf{A} - \mathbf{A}_1\|_2 = \sigma_2 = 1.
\]

Indeed:
\[
\mathbf{A} - \mathbf{A}_1 = \begin{bmatrix} 3 & 0 \\ 0 & 1 \end{bmatrix} - \begin{bmatrix} 3 & 0 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}.
\]
The largest singular value of this difference is 1, which matches \(\sigma_2\).


Therefore, \(\mathbf{A}_1\) captures the most significant component of \(\mathbf{A}\).  The error in the approximation, measured by the 2-norm, is exactly the next singular value \(\sigma_2 = 1\).

</div>



---


### Exercises {.unnumbered .unlisted}





<div class="exercise">
Find a SVD of \[\mathbf{A} = \begin{bmatrix}3&0\\4&5 \end{bmatrix}.\]  Find the rank 1 approximation for $\mathbf{A}$.

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find a SVD of \[\mathbf{A} = \begin{bmatrix}3&2&2\\2&3&-2 \end{bmatrix}.\]  Find the rank 1 approximation for $\mathbf{A}$.

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find a SVD of \[\mathbf{A} = \begin{bmatrix}4&0\\3&-5 \end{bmatrix}.\]  Find the rank 1 approximation for $\mathbf{A}$.
    
<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find a SVD of \[\mathbf{A} = \begin{bmatrix}1&0&1\\-1&1&0 \end{bmatrix}.\]  Find the rank 1 approximation for $\mathbf{A}$.

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find a SVD of \[\mathbf{A} = \begin{bmatrix}1&-1\\0&1\\1&0 \end{bmatrix}.\]  Find the rank 1 approximation for $\mathbf{A}$.

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Find a SVD of \[\mathbf{A} = \begin{bmatrix}1&1&1\\ -1&0&-2\\ 1&2&0 \end{bmatrix}.\]  Find the rank 1 approximation for $\mathbf{A}$.

<div style="text-align: right;">
[Solution]( )
</div> 
</div>







<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->
<!-- NEW SECTION --><!-- NEW SECTION --><!-- NEW SECTION -->






##  Matrix Phylogeny (Overview)

Matrices can be classified based on properties and decompositions:

| Matrix type | Property |
|------------|----------|
| Square, invertible | Determinant \(\neq 0\) |
| Non-defective | Diagonalizable, has \(n\) independent eigenvectors |
| Normal | \(\mathbf{A}^\top \mathbf{A} = \mathbf{A}\mathbf{A}^\top\) |
| Orthogonal | \(\mathbf{A}^\top \mathbf{A} = \mathbf{A}\mathbf{A}^\top = I\), subset of invertible matrices |
| Symmetric | \(S = S^\top\), real eigenvalues |
| Positive definite | \(x^\top P \mathbf{x} > 0\) for all \(x \neq 0\), unique Cholesky decomposition |
| Diagonal | Closed under multiplication/addition, special case: identity matrix \(I\) |

SVD exists for all real matrices, square or rectangular.  Eigenvalue decomposition exists only for non-defective square matrices.  The phylogenetic relationships between matrix types help organize matrix operations and decompositions.



