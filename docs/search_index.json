[["index.html", "DSCI 420: Mathematics for Machine Learning A Bookdown Companion to the Original Text Welcome", " DSCI 420: Mathematics for Machine Learning A Bookdown Companion to the Original Text D420 Faculty Team 2026-01-02 Welcome This online book expands on the concepts introduced in Mathematics for Machine Learning by Marc Deisenroth, A. Faisal, and C. Ong. Youll find: Step-by-step theory walkthroughs Worked examples and visualizations Embedded video explanations Subsection-level problem sets The aim here is not to replace the textbook; but rather, we wish to provide additional supports for a more novice reader. The ideas contained in the textbook, in our opinion, are accessible to a very wide audience - specifically one that lacks a strong mathematical foundation. By providing additional examples and exercises and by breaking down the notation, we hope to expand the audience to a wider audience. To read this book, the audience should begin with a very basic understanding of calculus (perhaps at a first year level). "],["table-of-symbols.html", "Table of Symbols", " Table of Symbols Important Symbols and Where to Find Them: Symbol Typical Meaning Reference a, b, c, , ,  Scalars (lowercase) x, y, z Vectors (bold lowercase) Section 1.1, 2.0 A, B, C Matrices (bold uppercase) Section 2.1, 2.2 \\(x^\\top, A^\\top\\) Transpose of a vector or matrix Section 2.2 \\(A^{-1}\\) Inverse of a matrix Section 2.2 \\(\\langle x, y \\rangle\\) Inner product of \\(x\\) and \\(y\\) Section 4.2 \\(x^\\top y\\) Dot product of \\(x\\) and \\(y\\) Section 2.0, 4.2 \\(B = (b_1, b_2, b_3)\\) Ordered tuple \\(B = [b_1, b_2, b_3]\\) Matrix of column vectors stacked horizontally Section 2.6 \\(B = \\{b_1, b_2, b_3\\}\\) Set of vectors (unordered) Section 2.6 \\(\\mathbb{Z}, \\mathbb{N}\\) Integers and natural numbers \\(\\mathbb{R}, \\mathbb{C}\\) Real and complex numbers \\(\\mathbb{R}^n\\) \\(n\\)-dimensional vector space of reals \\(\\forall x\\) Universal quantifier (for all \\(x\\)) \\(\\exists x\\) Existential quantifier (there exists \\(x\\)) \\(a := b\\) \\(a\\) is defined as \\(b\\) \\(a =: b\\) \\(b\\) is defined as \\(a\\) \\(a \\propto b\\) \\(a\\) is proportional to \\(b\\) (\\(a = \\text{constant} \\cdot b\\)) \\(g \\circ f\\) Function composition (\\(g\\) after \\(f\\)) \\(\\Leftrightarrow\\) If and only if \\(\\Rightarrow\\) Implies \\(A, C\\) Sets \\(a \\in A\\) \\(a\\) is an element of \\(A\\) \\(\\emptyset\\) Empty set \\(A \\setminus B\\) Elements in \\(A\\) but not in \\(B\\) Section 2.3 \\(D\\) Number of dimensions (\\(d = 1, \\dots, D\\)) \\(N\\) Number of data points (\\(n = 1, \\dots, N\\)) \\(I_m\\) Identity matrix of size \\(m \\times m\\) Section 2.2 \\(0_{m,n}\\) Matrix of zeros of size \\(m \\times n\\) \\(1_{m,n}\\) Matrix of ones of size \\(m \\times n\\) \\(e_i\\) Standard (canonical) basis vector (1 in the \\(i\\)-th position) Section 2.6 dim Dimensionality of a vector space Section 2.6 rk(A) Rank of matrix \\(A\\) Section 2.6 Im() Image of a linear mapping \\(\\) Section 2.7 ker() Kernel (null space) of \\(\\) Section 2.6 span[b] Span (generating set) of \\(b_1\\) Section 2.6 tr(A) Trace of \\(A\\) Section 4.1 det(A) Determinant of \\(A\\) Section 2.2, 4.1 \\(| \\cdot |\\) Absolute value or determinant (depending on context) \\(\\| \\cdot \\|\\) Norm (Euclidean unless stated otherwise) Section 3.1 \\(\\lambda\\) Eigenvalue or Lagrange multiplier \\(E_\\lambda\\) Eigenspace corresponding to eigenvalue \\(\\lambda\\) \\(x \\perp y\\) \\(x\\) and \\(y\\) are orthogonal Section 3.2 \\(V\\) Vector space Section 2.0, 2.4 \\(V^\\perp\\) Orthogonal complement of \\(V\\) Section 3.6 \\(\\sum_{n=1}^N x_n\\) Sum: \\(x_1 + \\dots + x_N\\) \\(\\prod_{n=1}^N x_n\\) Product: \\(x_1 \\cdot \\dots \\cdot x_N\\) \\(\\theta\\) Parameter vector \\(\\frac{\\partial f}{\\partial x}\\) Partial derivative of \\(f\\) with respect to \\(x\\) \\(\\frac{df}{dx}\\) Total derivative of \\(f\\) with respect to \\(x\\) \\(\\nabla\\) Gradient \\(f^* = \\min_x f(x)\\) Minimum value of \\(f\\) \\(x^* \\in \\arg\\min_x f(x)\\) Value \\(x^*\\) that minimizes \\(f\\) \\(\\mathcal{L}\\) Lagrangian \\(\\mathcal{L}\\) Negative log-likelihood \\(\\binom{n}{k}\\) Binomial coefficient, \\(n\\) choose \\(k\\) \\(V_X[x]\\) Variance of \\(x\\) with respect to the random variable \\(X\\) \\(E_X[x]\\) Expectation of \\(x\\) with respect to the random variable \\(X\\) \\(\\mathrm{Cov}_{X,Y}[x, y]\\) Covariance between \\(x\\) and \\(y\\) \\(X \\perp\\!\\!\\!\\perp Y \\mid Z\\) \\(X\\) is conditionally independent of \\(Y\\) given \\(Z\\) \\(X \\sim p\\) Random variable \\(X\\) is distributed according to \\(p\\) \\(\\mathcal{N}(\\mu, \\Sigma)\\) Gaussian distribution with mean \\(\\mu\\) and covariance \\(\\Sigma\\) \\(\\mathrm{Ber}(\\mu)\\) Bernoulli distribution with parameter \\(\\mu\\) \\(\\mathrm{Bin}(N, \\mu)\\) Binomial distribution with parameters \\(N, \\mu\\) \\(\\mathrm{Beta}(\\alpha, \\beta)\\) Beta distribution with parameters \\(\\alpha, \\beta\\) Table of Abbreviations and Acronyms Acronym Meaning e.g. Exempli gratia (Latin: for example) GMM Gaussian mixture model i.e. Id est (Latin: this means) i.i.d. Independent, identically distributed MAP Maximum a posteriori MLE Maximum likelihood estimation/estimator ONB Orthonormal basis PCA Principal component analysis PPCA Probabilistic principal component analysis REF Row-echelon form SPD Symmetric, positive definite SVM Support vector machine "],["introduction-and-motivation.html", "Chapter 1 Introduction and Motivation", " Chapter 1 Introduction and Motivation Machine learning focuses on designing algorithms that automatically extract meaningful information from data. A concise and widely accepted definition of machine learning comes from Tom Mitchell (1997): A computer program is said to learn from experience E with respect to some class of tasks \\(T\\) and performance measure \\(P\\), if its performance at tasks in \\(T\\), as measured by \\(P\\), improves with experience \\(E\\). "],["finding-words-for-intuitions.html", "1.1 Finding Words for Intuitions", " 1.1 Finding Words for Intuitions Definition 1.1 Machine learning is the study and development of algorithms that improve automatically through experience and data, without being explicitly programmed for each task. Machine learning is a field that combines data, models, and learning methods to identify patterns and make predictions or decisions  ideally generalizing well to new, unseen situations. Data is the foundation  machine learning aims to discover useful patterns from data without relying heavily on domain expertise. Definition 1.2 Data are pieces of information collected to describe, measure, or analyze phenomena. In practice, data is represented numerically, often as vectors, \\(\\mathbf{x} = \\begin{bmatrix}x_1\\\\ x_2\\\\ \\vdots\\\\ x_N \\end{bmatrix}\\). Models describe how data is generated or how inputs map to outputs. Definition 1.3 A model is a learned representation that maps inputs to outputs based on patterns found in data. A model learns when its performance improves after processing data. Good models generalize to new, unseen data. Definition 1.4 Learning is the process of using data to automatically improve a models ability to perform a task. The goal is not just to fit the training data, but to perform well on new examples. Formally, you can think of an algorithm as a mapping from inputs to outputs, where each step is precise, unambiguous, and executable by a computer. Definition 1.5 An Algorithm: is a finite sequence of well-defined instructions or steps designed to solve a specific problem or perform a computation. In the context of machine learning, an algorithm provides a systematic procedure for processing data  either to make predictions (as in a predictive algorithm) or to adjust model parameters (as in a training algorithm). In this way, machine learning involves two overlapping meanings of algorithm: A predictor that makes predictions based on data. A training procedure that updates the predictors parameters to improve future performance. Understanding the mathematical foundations behind data, models, and learning helps us build, interpret, and improve machine learning systems  and recognize their assumptions and limitations. "],["two-ways-to-read-this-book.html", "1.2 Two Ways to Read This Book", " 1.2 Two Ways to Read This Book There are two main strategies for learning the mathematics that underpins machine learning. The bottom-up approach builds understanding from fundamental mathematical concepts toward more advanced ideas. This method provides a solid conceptual foundation, ensuring that each new idea rests on well-understood principles. However, for many learners, this approach can feel slow or disconnected from practical motivation, since the relevance of abstract concepts may not be immediately clear. In contrast, the top-down approach begins with real-world problems and drills down to the mathematics required to solve them. This goal-driven strategy keeps motivation high and helps learners understand why each concept matters. The drawback, however, is that the underlying mathematical ideas can remain fragilereaders may learn to use tools effectively without fully grasping their theoretical basis. Mathematics for Machine Learning is designed to support both approaches  foundational (Part I) and applied (Part II)  so readers can move between mathematics and machine learning freely. This book is designed to assist readers in their understanding of the textbook Mathematics for Machine Learning. It is more of a foundational approach designed to fill in any gaps a reader might have. In particular, we aim to provide more examples in a less theoretical way. Whether you are reading from a top down or bottom up approach, this book will support your learning. 1.2.1 Part I: Mathematical Foundations Part I develops the mathematical tools that support all major ML methods  the four pillars of machine learning: Regression Dimensionality Reduction Density Estimation Classification It covers: Linear Algebra (Ch. 2): Vectors, matrices, and their relationships. Analytic Geometry (Ch. 3): Similarity and distance between vectors. Matrix Decomposition (Ch. 4): Interpreting and simplifying data. Vector Calculus (Ch. 5): Gradients and differentiation. Probability Theory (Ch. 6): Quantifying uncertainty and noise. Optimization (Ch. 7): Finding parameters that maximize performance. 1.2.2 Part II: Machine Learning Applications Part II applies the math from Part I to the four pillars: Ch. 8  Foundations of ML: Data, models, and learning; designing robust experiments. Ch. 9  Regression: Predicting continuous outcomes using linear and Bayesian approaches. Ch. 10  Dimensionality Reduction: Compressing high-dimensional data (e.g., PCA). Ch. 11  Density Estimation: Modeling data distributions (e.g., Gaussian mixtures). Ch. 12  Classification: Assigning discrete labels (e.g., support vector machines). 1.2.3 Learning Path Readers are encouraged to mix bottom-up and top-down learning: Build foundational skills when needed. Explore applications that connect math to real machine learning systems. This modular structure makes the book suitable for both mathematical learners and practitioners aiming to deepen their theoretical understanding. "],["exercises-and-feedback.html", "1.3 Exercises and Feedback", " 1.3 Exercises and Feedback While Mathematics for Machine learning provides some examples and exercises, this book is built to support those who want to practice particular skills or build their knowledge in a particular area. We have added a number of exercises, examples, and videos to hopefully aid your understanding of the material. Exercises Exercise 1.1 Discuss the ideas of data, models and learning. How are they related? Solution Exercise 1.2 In machine learning, how are data typically represented? Solution Exercise 1.3 What is meant by learning in the context of a model? Solution "],["linear-algebra.html", "Chapter 2 Linear Algebra", " Chapter 2 Linear Algebra When formalizing intuitive mathematical ideas, we define a set of objects and rules for manipulating themthis structure is called an algebra. In particular, linear algebra focuses on vectors and the rules that govern how they can be added and scaled. "],["vectors.html", "2.0 Vectors", " 2.0 Vectors Definition 2.1 A vector is a mathematical object that can be added to other vectors and multiplied by scalars, resulting in another vector of the same kind. A vector \\(\\mathbf{v} \\in \\mathbb{R}^n\\) has the form \\[ \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}, \\] where each \\(v_i \\in \\mathbb{R}\\). 2.0.1 Vector Spaces While most people are familiar with geometric vectors (arrows with direction and magnitude), vectors can also take more abstract formsas long as they obey the two key operations: Addition: \\(\\mathbf{a} + \\mathbf{b} = \\mathbf{c}\\) Scalar multiplication: \\(\\lambda \\mathbf{a} = \\mathbf{b}\\) Definition 2.2 A set \\(V\\) is a vector space over \\(\\mathbb{R}\\) if for any \\(\\mathbf{u}, \\mathbf{v} \\in V\\) and any scalar \\(\\lambda \\in \\mathbb{R}\\): \\[ \\mathbf{u} + \\mathbf{v} \\in V \\quad \\text{and} \\quad \\lambda \\mathbf{u} \\in V \\] Definition 2.3 Let \\(\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^n\\) be two vectors. The sum of two vectors is obtained by adding their corresponding components: \\[ \\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\end{bmatrix} + \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ \\vdots \\\\ u_n + v_n \\end{bmatrix} \\] Example 2.1 We can add two vectors componentwise: \\[ \\begin{bmatrix} 1 \\\\ 4 \\\\ 10\\\\ 20 \\end{bmatrix} + \\begin{bmatrix} 2 \\\\ 6 \\\\ 15\\\\ 30 \\end{bmatrix} = \\begin{bmatrix} 1+2 \\\\ 4+6 \\\\ 10+15\\\\ 20+30 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 10 \\\\ 25\\\\ 50 \\end{bmatrix}. \\] Definition 2.4 Let \\(\\mathbf{u} \\in \\mathbb{R}^n\\) be a vector, and let \\(\\lambda \\in \\mathbb{R}\\) be a scalar. The product of a scalar \\(\\lambda\\) and a vector \\(\\mathbf{u}\\) is obtained by multiplying each component of the vector by the scalar: \\[ \\lambda \\mathbf{u} = \\lambda \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\end{bmatrix} = \\begin{bmatrix} \\lambda u_1 \\\\ \\lambda u_2 \\\\ \\vdots \\\\ \\lambda u_n \\end{bmatrix} \\] Example 2.2 Scalar multiplication is applied to each term: \\[ 5\\begin{bmatrix} 1 \\\\ 4 \\\\ 10\\\\ 20 \\end{bmatrix} = \\begin{bmatrix} 5 \\times 1 \\\\ 5 \\times 4 \\\\ 5 \\times 10\\\\ 5 \\times 20 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 20 \\\\ 50\\\\ 100 \\end{bmatrix}. \\] Example 2.3 The set of complex numbers \\(\\mathbb{C}\\) is a vector space. To prove this, we need to show that it satisfies the two properties: For \\(u, v \\in \\mathbb{C}\\), we have \\(u+v \\in \\mathbb{C}\\). For \\(u \\in \\mathbb{C}\\) and \\(\\lambda \\in \\mathbb{R}\\), we have \\(\\lambda u \\in \\mathbb{C}\\). Let \\(u,v \\in \\mathbb{C}\\). Then \\(u = a+bi\\) and \\(v = c + di\\). Vector addition: The usual complex addition is defined as: \\[ u + v = (a + bi) + (c + di) = (a + c) + (b + d)i \\in \\mathbb{C} \\] Scalar multiplication: For real scalars \\(r \\in \\mathbb{R}\\), scalar multiplication is defined as: \\[ r \\cdot (a + bi) = (ra) + (rb)i \\in \\mathbb{C}. \\] Other examples of Vector spaces include: Geometric vectors: Can be drawn in space and manipulated visually. Polynomials: Can be added and scaled to form new polynomials. Audio signals: Represented as sequences of numbers that can be combined or scaled. Tuples of real numbers in \\(\\mathbb{R}^n\\) Treating vectors as elements of \\(\\mathbb{R}^n\\) aligns with how data is represented in computer programsarrays of real numbers. This makes linear algebra essential for computational work and for algorithms in machine learning and data science. 2.0.2 Closure A central idea in mathematics, known as closure, asks what new elements can be formed by combining existing ones through defined operations. In linear algebra, the set of all possible linear combinations of vectors forms a vector space, a foundational concept throughout machine learning. Definition 2.5 The closure property (or simply closure) describes whether a set is closed under an operation  meaning that when the operation is applied to elements of the set, the result is also an element of the same set. Formally, a set \\(S\\) is closed under an operation \\(\\circ\\) if for all \\(a, b \\in S\\): \\[ a \\circ b \\in S \\] Example 2.4 The set of real numbers \\(\\mathbb{R}\\) is closed under addition because for any \\(a, b \\in \\mathbb{R}\\), the sum \\(a + b \\in \\mathbb{R}\\). Example 2.5 The set of integers \\(\\mathbb{Z}\\) is not closed under division, since \\(1 \\div 2 = 0.5 \\notin \\mathbb{Z}\\). Example 2.6 The set of vectors of a set length, \\(n\\), is closed since \\(\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^n\\) implies that \\[\\mathbf{a} + \\mathbf{b} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{bmatrix} = \\begin{bmatrix} a_1+b_1 \\\\ a_2+b_2 \\\\ \\vdots \\\\ a_n+b_n \\end{bmatrix}. \\] However, the set of all vectors is not closed since \\(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 2 \\\\3 \\end{bmatrix}\\) is undefined. 2.0.3 Other Properties of Vectors Vectors obey a set of algebraic rules that make them fundamental in both geometry and linear algebra. Lemma 2.1 Let \\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w}\\) be vectors, and let \\(c, d\\) be scalars. Commutativity of Addition \\[ \\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u} \\] Associativity of Addition \\[ (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) \\] Additive Identity There exists a zero vector \\(\\mathbf{0}\\) such that \\[ \\mathbf{v} + \\mathbf{0} = \\mathbf{v} \\] Additive Inverse For every vector \\(\\mathbf{v}\\), there exists a vector \\(-\\mathbf{v}\\) such that \\[ \\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0} \\] Distributive Properties \\[ c(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v} \\] \\[ (c + d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v} \\] Associativity of Scalar Multiplication \\[ c(d\\mathbf{v}) = (cd)\\mathbf{v} \\] Multiplicative Identity \\[ 1 \\mathbf{v} = \\mathbf{v} \\] Sometimes, we use row vectors rather than column vectors simply to save space or for aesthetic reasons. Example 2.7 To prove the commutative rule for vector addition, write \\(\\mathbf{u}=[u_1,\\dots,u_n]\\), \\(\\mathbf{v}=[v_1,\\dots,v_n]\\). Addition is componentwise: \\[ u + v = [u_1+v_1,\\dots,u_n+v_n]. \\] By commutativity in \\(\\mathbb{R}\\), \\[ u_i + v_i = v_i + u_i \\quad \\forall i, \\] so \\[ \\mathbf{u} + \\mathbf{v} = [v_1+u_1, \\dots, v_n+u_n] = \\mathbf{v} + \\mathbf{u}. \\] The dot product (also called the inner product) is an operation that takes two vectors and returns a single number. It measures how similar or aligned the two vectors are. Definition 2.6 For vectors \\(\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^n\\), the dot product is defined as: \\[ \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i \\] Example 2.8 Compute the dot product of \\(\\begin{bmatrix} 2 \\\\ 5 \\\\ 4 \\end{bmatrix}\\) and \\(\\begin{bmatrix} -3\\\\ 0 \\\\ -2 \\end{bmatrix}\\). \\[\\begin{align*} \\begin{bmatrix} 2 \\\\ 5 \\\\ 4 \\end{bmatrix} \\cdot \\begin{bmatrix} -3\\\\ 0 \\\\ -2 \\end{bmatrix} &amp;= \\sum_{i=1}^{3} a_i b_i\\\\ &amp;= a_1b_1 + a_2 b_2 + a_3 b_3\\\\ &amp;= 2(-3) + 5(0) + 4(-2)\\\\ &amp;= -14 \\end{align*}\\] 2.0.4 Geometric Interpretation of a Vector Geometrically, vectors can be thought of as arrows that have both magnitude (length) and direction. They are often used to represent quantities such as displacement, velocity, or force. Vector addition corresponds to placing one arrows tail at the head (the triangle rule), resulting in a new vector that represents the combined effect of both. Scalar multiplication stretches or shrinks a vector and can reverse its direction if the scalar is negative. These geometric operations follow the same algebraic properties found in vector spacessuch as commutativity, associativity, and distributivity  allowing us to interpret abstract vector operations visually as movements and scalings in space. Example 2.9 Let \\(\\mathbf{u} = [3,-2]\\) and \\(\\mathbf{v} = [-1,4]\\). Then \\(\\mathbf{u} + \\mathbf{v}\\) and \\(\\mathbf{u} - \\mathbf{v}\\) can be computed using vectors: \\[\\mathbf{u} + \\mathbf{v}= [2,2] \\;\\;\\; \\text{ and } \\;\\;\\; \\mathbf{u} - \\mathbf{v} = [4,-6].\\] Exercises Exercise 2.1 Vector addition Solution Exercise 2.2 Scalar Multiplication Solution Exercise 2.3 Both Solution Exercise 2.4 Show that each of the following are vector spaces: \\(\\mathbb{R}^n\\) Polynomials Continuous functions Sequences Solution Exercise 2.5 Let \\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w}\\) be vectors, and let \\(c, d\\) be scalars. Prove each of the following properties: Commutativity of Addition \\[ \\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u} \\] Associativity of Addition \\[ (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) \\] Additive Identity There exists a zero vector \\(\\mathbf{0}\\) such that \\[ \\mathbf{v} + \\mathbf{0} = \\mathbf{v} \\] Additive Inverse For every vector \\(\\mathbf{v}\\), there exists a vector \\(-\\mathbf{v}\\) such that \\[ \\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0} \\] Distributive Properties \\[ c(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v} \\] \\[ (c + d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v} \\] Associativity of Scalar Multiplication \\[ c(d\\mathbf{v}) = (cd)\\mathbf{v} \\] Multiplicative Identity \\[ 1 \\mathbf{v} = \\mathbf{v} \\] Solution Exercise 2.6 Dot product example Solution Exercise 2.7 Solution "],["systems-of-linear-equations.html", "2.1 Systems of Linear Equations", " 2.1 Systems of Linear Equations Systems of linear equations are fundamental in linear algebra, as many problems can be formulated in this way. Linear algebra provides the tools to solve these systems efficiently. Example 2.10 A company produces products \\(N_1, \\ldots, N_n\\) using resources \\(R_1, \\ldots, R_m\\). Each product \\(N_j\\) requires \\(a_{ij}\\) units of resource \\(R_i\\). If \\(b_i\\) units of each resource \\(R_i\\) are available, then the total resources used must satisfy \\[ a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n = b_i, \\quad i = 1, \\ldots, m \\] or, in general matrix form, \\[ A\\mathbf{x} = \\mathbf{b}, \\] where \\(A = [a_{ij}] \\in \\mathbb{R}^{m \\times n}\\), \\(\\mathbf{x} \\in \\mathbb{R}^n\\), and \\(\\mathbf{b} \\in \\mathbb{R}^m\\). Example 2.11 Suppose a factory produces two products: P1 and P2. Let: \\(x_1\\) = units of P1 produced \\(x_2\\) = units of P2 produced Constraints: Labor: Each unit of P1 requires 2 hours, P2 requires 3 hours, and total available labor is 120 hours: \\[ 2x_1 + 3x_2 \\leq 120 \\] Material: Each unit of P1 uses 1 kg of material, P2 uses 2 kg, and total available material is 100 kg: \\[ 1x_1 + 2x_2 \\leq 100 \\] Non-negativity: \\[ x_1 \\geq 0, \\quad x_2 \\geq 0 \\] We can rewrite the inequalities as a matrix inequality: \\[ \\underbrace{ \\begin{pmatrix} 2 &amp; 3 \\\\ 1 &amp; 2 \\end{pmatrix}}_{A} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\leq \\underbrace{ \\begin{pmatrix} 120 \\\\ 100 \\end{pmatrix}}_{b} \\] Here: \\(\\mathbf{A}\\) is the constraint matrix. \\(x = \\begin{pmatrix}x_1 \\\\ x_2\\end{pmatrix}\\) is the decision variable vector. \\(b\\) is the resource vector. 2.1.1 Solutions to Systems of Linear Equations In general, a system of linear equations can have: no solution, exactly one solution, or infinitely many solutions. Example 2.12 The system \\[ \\begin{aligned} x_1 + x_2 + x_3 &amp;= 3 \\\\ x_1 - x_2 + 2x_3 &amp;= 2 \\\\ 2x_1 + 3x_3 &amp;= 1 \\end{aligned} \\] has no solution, since combining the first two equations gives \\(2x_1 + 3x_3 = 5\\), which contradicts the third equation. Systems of equations with no solutions are called inconsistent. Definition 2.7 A system of linear equations is said to be inconsistent if no set of values for the unknown variables satisfies all equations simultaneously. In other words, the equations contradict each other, and there is no common solution. Example 2.13 The system \\[ \\begin{aligned} x_1 + x_2 + x_3 &amp;= 3 \\\\ x_1 - x_2 + 2x_3 &amp;= 2 \\\\ x_2 + x_3 &amp;= 2 \\end{aligned} \\] has a unique solution \\((x_1, x_2, x_3) = (1, 1, 1)\\). Example 2.14 The system \\[ \\begin{aligned} x_1 + x_2 + x_3 &amp;= 3 \\\\ x_1 - x_2 + 2x_3 &amp;= 2 \\\\ 2x_1 + 3x_3 &amp;= 5 \\end{aligned} \\] has infinitely many solutions, since the third equation is a linear combination of the first two. If we set \\(x_3 = a \\in \\mathbb{R}\\), then \\[ (x_1, x_2, x_3) = \\left( \\tfrac{5}{2} - \\tfrac{3}{2}a,\\, \\tfrac{1}{2} + \\tfrac{1}{2}a,\\, a \\right), \\quad a \\in \\mathbb{R}. \\] Hence, the solution set forms a line in \\(\\mathbb{R}^3\\). Definition 2.8 A system of linear equations is called consistent if there exists at least one set of values for the unknown variables that satisfies all equations simultaneously. If there is exactly one solution, the system is uniquely consistent. If there are infinitely many solutions, the system is dependent (still consistent). Example 2.15 The system of equations\\[ \\begin{cases} x + y = 5 \\\\ 2x - y = 1 \\end{cases} \\] is a consistent system with a unique solution: \\(x = 2\\), \\(y = 3\\). Example 2.16 The system of equations \\[ \\begin{cases} x + y = 4 \\\\ 2x + 2y = 10 \\end{cases} \\] is inconsistent since the second equation is equivalent to \\(x + y = 5\\), which contradicts the first equation (\\(x + y = 4\\)). Thus, the system has no solution. 2.1.2 Geometric Interpretation In two dimensions, each linear equation represents a line on the \\(x_1x_2\\)-plane. The solution set is the intersection of these lines, which can be: a point (unique solution), a line (infinitely many solutions), or empty (no solution). For three variables, each equation defines a plane in \\(\\mathbb{R}^3\\). Their intersection can be a plane, line, point, or empty set. Example 2.17 \\[ \\begin{aligned} 4x_1 + 4x_2 &amp;= 5 \\\\ 2x_1 - 4x_2 &amp;= 1 \\end{aligned} \\] has the unique solution \\((x_1, x_2) = (1, \\tfrac{1}{4})\\). Example 2.18 Consider the system: \\[ \\begin{cases} x + y + z = 3 \\\\ 2x + 2y + 2z = 6 \\\\ x - y + z = 1 \\end{cases} \\] The second equation is just \\(2 \\times\\) the first equation, so it doesnt add a new constraint. The first and third equations define a plane intersection. This leaves one free variable, so there are infinitely many solutions. Let \\(z = t\\) (free parameter), then: \\[ \\begin{aligned} x + y + t &amp;= 3 \\implies x = 3 - y - t \\\\ x - y + t &amp;= 1 \\implies (3 - y - t) - y + t = 1 \\implies 2y = 2 \\implies y = 1 \\\\ x &amp;= 3 - 1 - t = 2 - t \\end{aligned} \\] Thus, the general solution is: \\[ (x, y, z) = (2 - t, 1, t), \\quad t \\in \\mathbb{R}. \\] 2.1.3 Matrix Formulation A system of linear equations can be written compactly as: \\[ \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix} \\] This matrix representation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) provides a compact and powerful way to describe and solve systems of linear equations. Example 2.19 Write the system of equations as a matrix: \\[ \\begin{cases} x + y = 5 \\\\ 2x - y = 1 \\end{cases} \\] The matrix version of this system is: \\[ \\begin{bmatrix} 1 &amp; 1 \\\\ 2 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 1 \\end{bmatrix} \\] Example 2.20 Write the system of equations as a matrix: \\[\\begin{cases} x + y = 4 \\\\ 2x + 2y = 10 \\end{cases} \\] The matrix version of this system is: \\[ \\begin{bmatrix} 1 &amp; 1 \\\\ 2 &amp; 2 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 10 \\end{bmatrix} \\] Exercises Exercise 2.8 Write a system of equations with a unique solution. Exercise 2.9 Write a system of equations with infinitely many solutions. Exercise 2.10 Write a system of equations with no solutions. Exercise 2.11 Solve example 2.11 Solution Exercise 2.12 Write 2.15 and 2.16 as a matrix equations Solution Exercise 2.13 Solve a system of equations Solution Exercise 2.14 Solve a system of equations Solution Exercise 2.15 Turn into a matrix form Solution Exercise 2.16 Change from matrix to equations Solution "],["matrices.html", "2.2 Matrices", " 2.2 Matrices Matrices play a central role in linear algebra. They provide a compact way to represent systems of linear equations and also serve as representations of linear functions (or mappings). Definition 2.9 A matrix is a rectangular array of numbers arranged in \\(m\\) rows and \\(n\\) columns. Formally, a real-valued matrix \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) is: \\[ \\mathbf{A} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\dots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\dots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\dots &amp; a_{mn} \\end{bmatrix} \\] The notation \\((\\mathbf{A})_{ij}\\) refers to the \\(ij^{th}\\) element of \\(\\mathbf{A}\\). So, \\((\\mathbf{A})_{ij} = a_{ij}\\). For example, if \\[ \\mathbf{A} = \\begin{bmatrix} 2 &amp; 4 \\\\ 1 &amp; 3 \\end{bmatrix}, \\] Then \\(\\mathbf{A}_{11} = 2, \\mathbf{A}_{12} = 4, \\mathbf{A}_{21} = 1\\) and \\(\\mathbf{A}_{22} = 3\\). Matrices with one row are called row vectors, and those with one column are column vectors. 2.2.1 Matrix Addition Definition 2.10 For two matrices \\(\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\), their sum and difference are defined element-wise: \\[ (\\mathbf{A} + \\mathbf{B})_{ij} = a_{ij} + b_{ij} \\;\\;\\;\\;\\;\\;\\;\\;\\; (\\mathbf{A} - \\mathbf{B})_{ij} = a_{ij} - b_{ij} \\] The result of matrix addition is another \\(m \\times n\\) matrix. Example 2.21 Let: \\[ \\mathbf{A} = \\begin{bmatrix} 2 &amp; 4 \\\\ 1 &amp; 3 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 5 &amp; 0 \\\\ -2 &amp; 1 \\end{bmatrix}. \\] Then \\[ \\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} 2 + 5 &amp; 4 + 0 \\\\ 1 + (-2) &amp; 3 + 1 \\end{bmatrix} = \\begin{bmatrix} 7 &amp; 4 \\\\ -1 &amp; 4 \\end{bmatrix} \\] \\[ \\mathbf{A} - \\mathbf{B} = \\begin{bmatrix} 2 - 5 &amp; 4 - 0 \\\\ 1 - (-2) &amp; 3 - 1 \\end{bmatrix} = \\begin{bmatrix} -3 &amp; 4 \\\\ 3 &amp; 2 \\end{bmatrix}. \\] Example 2.22 In the previous example, we can see that \\[(\\mathbf{A}+\\mathbf{B})_{21} = -1 \\quad \\text{and} \\quad (\\mathbf{A}-\\mathbf{B})_{22} = 2.\\] 2.2.2 Matrix Multiplication Definition 2.11 For matrices \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{n \\times k}\\), their product \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B} \\in \\mathbb{R}^{m \\times k}\\) is defined as: \\[ c_{ij} = \\sum_{l=1}^{n} a_{il} b_{lj} \\] That is, each element of \\(\\mathbf{C}\\) is obtained by taking the dot product of the corresponding row of \\(\\mathbf{A}\\) and column of \\(\\mathbf{B}\\). Matrix multiplication is only defined when the inner dimensions match (the number of columns of \\(\\mathbf{A}\\) equals the number of rows of \\(\\mathbf{B}\\)). Matrix multiplication is not commutative, meaning \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\) in general. Example 2.23 Let \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 2 &amp; 0 \\\\ 1 &amp; 2 \\end{bmatrix}. \\] Then \\[ \\mathbf{A} \\mathbf{B} = \\begin{bmatrix} 1(2) + 2(1) &amp; 1(0) + 2(2) \\\\ 3(2) + 4(1) &amp; 3(0) + 4(2) \\end{bmatrix} = \\begin{bmatrix} 4 &amp; 4 \\\\ 10 &amp; 8 \\end{bmatrix}. \\] Example 2.24 Let \\[ \\mathbf{C} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix}, \\quad \\mathbf{D} = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\\\ 5 &amp; 6 \\end{bmatrix}. \\] Then \\[ \\mathbf{C} \\mathbf{D} = \\begin{bmatrix} 1(1) + 2(3) + 3(5) &amp; 1(2) + 2(4) + 3(6) \\\\ 4(1) + 5(3) + 6(5) &amp; 4(2) + 5(4) + 6(6) \\end{bmatrix} = \\begin{bmatrix} 22 &amp; 28 \\\\ 49 &amp; 64 \\end{bmatrix}. \\] 2.2.3 Identity Matrix Definition 2.12 The identity matrix \\(\\mathbf{I}_n \\in \\mathbb{R}^{n \\times n}\\) is a square matrix with 1s on the diagonal and 0s elsewhere: \\[ \\mathbf{I}_n = \\begin{bmatrix} 1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 \\end{bmatrix} \\] The identity matrix satisfies: \\[ \\mathbf{I}_m \\mathbf{A} = \\mathbf{A} \\mathbf{I}_n = \\mathbf{A} \\] for any matrix \\(\\mathbf{A}\\) (with the appropriate dimensions). Example 2.25 Let \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix} .\\] Then \\[\\mathbf{A}\\mathbf{I} = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}= \\begin{bmatrix} 1(1) + 2(0) &amp; 1(0) + 2(1) \\\\ 3(1) + 4(0) &amp; 3(0) + 4(1) \\end{bmatrix}= \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix}.\\] 2.2.4 Matrix Properties There are several important properties of matrices. Lemma 2.2 For any matrices \\(\\mathbf{A}, \\mathbf{B}\\), and \\(\\mathbf{C}\\) with appropriate dimensions for addition/ multiplication, the following properties hold. Associativity: \\[ (\\mathbf{A}\\mathbf{B})\\mathbf{C} = \\mathbf{A}(\\mathbf{B}\\mathbf{C}) \\] Distributivity: \\[ (\\mathbf{A} + \\mathbf{B})\\mathbf{C} = \\mathbf{A}\\mathbf{C} + \\mathbf{B}\\mathbf{C}, \\quad \\mathbf{A}(\\mathbf{C} + \\mathbf{D}) = \\mathbf{A}\\mathbf{C} + \\mathbf{A}\\mathbf{D} \\] Identity Property: \\[ \\mathbf{I}_m \\mathbf{A} = \\mathbf{A} \\mathbf{I}_n = \\mathbf{A} \\] Matrix multiplication is not element-wise. When multiplication is performed element by element, it is called the Hadamard product. 2.2.5 Matrix Inverse Definition 2.13 A square matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) is invertible (or nonsingular) if there exists a matrix \\(\\mathbf{B} \\in \\mathbb{R}^{n \\times n}\\) such that \\[ \\mathbf{A}\\mathbf{B} = \\mathbf{I}_n = \\mathbf{B}\\mathbf{A} .\\] In this case, \\(\\mathbf{B}\\) is called the inverse of \\(\\mathbf{A}\\) and is denoted \\(\\mathbf{A}^{-1}\\). If no such matrix exists, \\(\\mathbf{A}\\) is singular or noninvertible. Example 2.26 Let \\[ \\mathbf{A} = \\begin{bmatrix} 2 &amp; 1 \\\\ 7 &amp; 4 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 4 &amp; -1 \\\\ -7 &amp; 2 \\end{bmatrix}. \\] We will show that \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are inverses of each other by verifying that: \\[ \\mathbf{A} \\mathbf{B} = \\mathbf{B} \\mathbf{A} = \\mathbf{I} \\] Notice that \\[ \\mathbf{A} \\mathbf{B} = \\begin{bmatrix} 2 &amp; 1 \\\\ 7 &amp; 4 \\end{bmatrix} \\begin{bmatrix} 4 &amp; -1 \\\\ -7 &amp; 2 \\end{bmatrix} = \\begin{bmatrix} (2)(4) + (1)(-7) &amp; (2)(-1) + (1)(2) \\\\ (7)(4) + (4)(-7) &amp; (7)(-1) + (4)(2) \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}. \\] A similar calculation shows that \\(\\mathbf{B}\\mathbf{A} = \\mathbf{I}\\). Theorem 2.1 The inverse of a matrix \\(\\mathbf{A}\\), when it exists, is unique. For a 2Ã—2 matrix \\[ \\mathbf{A} = \\begin{bmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{bmatrix}, \\] the inverse is \\[ \\mathbf{A}^{-1} = \\frac{1}{a_{11}a_{22} - a_{12}a_{21}} \\begin{bmatrix} a_{22} &amp; -a_{12} \\\\ -a_{21} &amp; a_{11} \\end{bmatrix}, \\] provided \\(a_{11}a_{22} - a_{12}a_{21} \\neq 0\\). The term \\(a_{11}a_{22} - a_{12}a_{21}\\) is the determinant of \\(\\mathbf{A}\\). Example 2.27 Let \\[ \\mathbf{A} = \\begin{bmatrix} 4 &amp; 3 \\\\ 2 &amp; 1 \\end{bmatrix}. \\] The determinant is: \\[ \\det(\\mathbf{A}) = (4)(1) - (3)(2) = 4 - 6 = -2. \\] Applying the formula gives us the inverse of \\(\\mathbf{A}\\): \\[ \\mathbf{A}^{-1} = \\frac{1}{-2} \\begin{bmatrix} 1 &amp; -3 \\\\ -2 &amp; 4 \\end{bmatrix} = \\begin{bmatrix} -\\tfrac{1}{2} &amp; \\tfrac{3}{2} \\\\ 1 &amp; -2 \\end{bmatrix}. \\] Lemma 2.3 For a square non-singular matrix \\(\\mathbf{A}\\), the following are true: \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I} = \\mathbf{A}^{-1}\\mathbf{A}\\), \\((\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\\), \\((\\mathbf{A} + \\mathbf{B})^{-1} \\neq \\mathbf{A}^{-1} + \\mathbf{B}^{-1}\\) Example 2.28 Let \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 2 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\] Then, using the definition of the 2x2 inverse, \\[ \\mathbf{A}^{-1} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; \\tfrac{1}{2} \\end{bmatrix}, \\quad \\mathbf{B}^{-1} = \\begin{bmatrix} \\tfrac{1}{2} &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\] So, their sum is \\[ \\mathbf{A}^{-1} + \\mathbf{B}^{-1} = \\begin{bmatrix} 1 + \\tfrac{1}{2} &amp; 0 \\\\ 0 &amp; \\tfrac{1}{2} + 1 \\end{bmatrix} = \\begin{bmatrix} \\tfrac{3}{2} &amp; 0 \\\\ 0 &amp; \\tfrac{3}{2} \\end{bmatrix} \\] On the other hand, \\[ \\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} 1 + 2 &amp; 0 \\\\ 0 &amp; 2 + 1 \\end{bmatrix} = \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 3 \\end{bmatrix}. \\] Using the definition of the inverse, we get: \\[ (\\mathbf{A} + \\mathbf{B})^{-1} = \\begin{bmatrix} \\tfrac{1}{3} &amp; 0 \\\\ 0 &amp; \\tfrac{1}{3} \\end{bmatrix} \\] Comparing, we see \\[ (\\mathbf{A} + \\mathbf{B})^{-1} = \\begin{bmatrix} \\tfrac{1}{3} &amp; 0 \\\\ 0 &amp; \\tfrac{1}{3} \\end{bmatrix} \\quad \\text{vs.} \\quad \\mathbf{A}^{-1} + \\mathbf{B}^{-1} = \\begin{bmatrix} \\tfrac{3}{2} &amp; 0 \\\\ 0 &amp; \\tfrac{3}{2} \\end{bmatrix}. \\] Clearly, \\[ (\\mathbf{A} + \\mathbf{B})^{-1} \\neq \\mathbf{A}^{-1} + \\mathbf{B}^{-1}. \\] 2.2.6 Matrix Transpose Definition 2.14 The transpose of \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) is \\(\\mathbf{A}^\\top \\in \\mathbb{R}^{n \\times m}\\), obtained by interchanging rows and columns. Example 2.29 Let \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 4 &amp; 7 \\\\ 2 &amp; 5 &amp; 8 \\end{bmatrix}. \\] Matrix \\(\\mathbf{A}\\) is a \\(2 \\times 3\\) matrix (2 rows, 3 columns). The transpose of \\(\\mathbf{A}\\), denoted \\(\\mathbf{A}^\\top\\), is formed by turning rows into columns: \\[ \\mathbf{A}^\\top = \\begin{bmatrix} 1 &amp; 2 \\\\ 4 &amp; 5 \\\\ 7 &amp; 8 \\end{bmatrix}. \\] Now \\(\\mathbf{A}^\\top\\) is a \\(3 \\times 2\\) matrix (3 rows, 2 columns). Lemma 2.4 For \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{n \\times p}\\), the following properties hold: \\((\\mathbf{A}^\\top)^\\top = \\mathbf{A}\\), \\((\\mathbf{A}\\mathbf{B})^\\top = \\mathbf{B}^\\top \\mathbf{A}^\\top\\), \\((\\mathbf{A} + \\mathbf{B})^\\top = \\mathbf{A}^\\top + \\mathbf{B}^\\top\\). Example 2.30 Prove that for any two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) of the same size, \\[ (\\mathbf{A} + \\mathbf{B})^\\top = \\mathbf{A}^\\top + \\mathbf{B}^\\top. \\] Proof: Let \\(\\mathbf{A} = [a_{ij}]\\) and \\(\\mathbf{B} = [b_{ij}]\\) be \\(m \\times n\\) matrices. Then the sum \\(\\mathbf{A} + \\mathbf{B}\\) is defined elementwise as: \\[ (\\mathbf{A} + \\mathbf{B})_{ij} = a_{ij} + b_{ij}. \\] The transpose of a matrix swaps its rows and columns. So, the \\((i, j)\\)-th entry of \\((\\mathbf{A} + \\mathbf{B})^\\top\\) is: \\[ (\\mathbf{A} + \\mathbf{B})^\\top_{ij} = (\\mathbf{A} + \\mathbf{B})_{ji} = a_{ji} + b_{ji}. \\] Now, consider \\(\\mathbf{A}^\\top + \\mathbf{B}^\\top\\). The \\((i, j)\\)-th entry of this matrix is: \\[ (\\mathbf{A}^\\top + \\mathbf{B}^\\top)_{ij} = \\mathbf{A}^\\top_{ij} + \\mathbf{B}^\\top_{ij} = a_{ji} + b_{ji}. \\] Since the corresponding entries are equal for all \\(i, j\\), \\[ (\\mathbf{A} + \\mathbf{B})^\\top = \\mathbf{A}^\\top + \\mathbf{B}^\\top. \\] 2.2.7 Symmetric Matrices Definition 2.15 matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) is symmetric if \\(\\mathbf{A} = \\mathbf{A}^\\top\\). Only square matrices can be symmetric. If \\(\\mathbf{A}\\) is invertible, then \\(\\mathbf{A}^\\top\\) is also invertible and \\[ (\\mathbf{A}^{-1})^\\top = (\\mathbf{A}^\\top)^{-1}. \\] The sum of symmetric matrices is symmetric, but their product generally is not. Example 2.31 Matrix \\(\\mathbf{A} \\in \\mathbb{R}^{4 \\times 4}\\) is symmetric \\[ \\mathbf{A} = \\begin{pmatrix} 2 &amp; 1 &amp; 0 &amp; -1 \\\\ 1 &amp; 3 &amp; 4 &amp; 2 \\\\ 0 &amp; 4 &amp; 5 &amp; 3 \\\\ -1 &amp; 2 &amp; 3 &amp; 6 \\end{pmatrix}. \\] Notice that since \\(\\mathbf{A}\\) is symmetric, we have \\((\\mathbf{A})_{ij} = (\\mathbf{A})_{ji}\\). 2.2.8 Scalar Multiplication Matrices scale the same way that vectors do. Definition 2.16 For \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\), and \\(\\lambda \\in \\mathbb{R}\\), scalar multiplication is defined componentwise as: \\[ \\lambda (\\mathbf{A})_{ij} = \\lambda a_{ij} \\] Example 2.32 Let \\[ \\mathbf{A} = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix}, \\quad \\text{and let the scalar } k = 3. \\] Then the scalar multiplication \\(k \\cdot \\mathbf{A}\\) is: \\[ 3 \\cdot \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix} = \\begin{pmatrix} 3 &amp; 6 \\\\ 9 &amp; 12 \\end{pmatrix}. \\] Lemma 2.5 For \\(\\mathbf{A}, \\mathbf{\\mathbf{B}} \\in \\mathbb{R}^{m \\times n}\\) and scalars \\(\\lambda, \\psi \\in \\mathbb{R}\\): \\[ (\\lambda + \\psi)\\mathbf{A} = \\lambda \\mathbf{A} + \\psi \\mathbf{A}, \\quad \\lambda(\\mathbf{A} + \\mathbf{\\mathbf{B}}) = \\lambda \\mathbf{A} + \\lambda \\mathbf{\\mathbf{B}}. \\] 2.2.9 Compact Form of Linear Systems A system of linear equations such as \\[ \\begin{aligned} 2x_1 + 3x_2 + 5x_3 &amp;= 1 \\\\ 4x_1 - 2x_2 - 7x_3 &amp;= 8 \\\\ 9x_1 + 5x_2 - 3x_3 &amp;= 2 \\end{aligned} \\] can be written in matrix form as \\[ \\mathbf{A}\\mathbf{x} = \\mathbf{b}, \\quad \\text{where} \\quad \\mathbf{A} = \\begin{bmatrix} 2 &amp; 3 &amp; 5 \\\\ 4 &amp; -2 &amp; -7 \\\\ 9 &amp; 5 &amp; -3 \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 8 \\\\ 2 \\end{bmatrix}. \\] This expresses the system as a linear combination of the columns of \\(\\mathbf{A}\\). Example 2.33 Consider the system of equations: \\[ \\begin{cases} 2x + 3y = 5 \\\\ 4x - y = 1 \\end{cases}. \\] We can write this in matrix form as: \\[ \\begin{pmatrix} 2 &amp; 3 \\\\ 4 &amp; -1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 1 \\end{pmatrix}. \\] Example 2.34 Consider the matrix equation: \\[ \\begin{pmatrix} 1 &amp; 2 &amp; -1 \\\\ 3 &amp; 0 &amp; 4 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 7 \\\\ 5 \\end{pmatrix}. \\] This corresponds to the system of equations: \\[ \\begin{cases} x + 2y - z = 7 \\\\ 3x + 0 \\cdot y + 4z = 5 \\end{cases}. \\] Exercises Exercise 2.17 Compute \\(\\begin{bmatrix} 2 &amp; -3\\\\ 1 &amp; 0 \\\\ -1 &amp; 3\\end{bmatrix} + \\begin{bmatrix} 9 &amp; -5 \\\\ 0 &amp; 13 \\\\ -1 &amp; 3\\end{bmatrix}\\). Solution Exercise 2.18 Compute \\(\\begin{bmatrix} 2 &amp; -3\\\\ 1 &amp; 0 \\\\ -1 &amp; 3\\end{bmatrix} - \\begin{bmatrix} 9 &amp; -5 \\\\ 0 &amp; 13 \\\\ -1 &amp; 3\\end{bmatrix}\\). Solution Exercise 2.19 Compute \\(\\begin{bmatrix} 3 &amp; 6\\\\2 &amp; 4\\end{bmatrix} \\begin{bmatrix} 1 &amp; 3\\\\0 &amp; 2\\end{bmatrix}\\). Solution Exercise 2.20 Compute \\(\\begin{bmatrix} 1&amp;2&amp;3\\\\4&amp;3&amp;2\\end{bmatrix} \\begin{bmatrix} 2&amp;3\\\\3&amp;4\\\\1&amp;2\\end{bmatrix}\\) Solution Exercise 2.21 The textbook talks about \\(\\mathbf{A}\\mathbf{B} = \\mathbf{C}\\), where \\(c_{ij} = \\sum_{l=1}^n a_{il}b_{lj}\\). Use this definition to find \\(c_{12}\\) using this definition if \\(\\mathbf{A} = \\begin{bmatrix} 1&amp;2&amp;3\\\\4&amp;3&amp;2\\end{bmatrix}\\) and \\(\\mathbf{B} = \\begin{bmatrix} 2&amp;3\\\\3&amp;4\\\\1&amp;2\\end{bmatrix}\\) Solution Exercise 2.22 Is \\(\\begin{bmatrix} 1&amp;-2\\\\2&amp;5\\end{bmatrix}\\) the inverse of \\(\\begin{bmatrix} 5&amp;-2\\\\2&amp;-1\\end{bmatrix}\\)? Solution Exercise 2.23 Prove that the inverse of a matrix \\(\\mathbf{A}\\) is unique (if it exists). Solution Exercise 2.24 Show that if \\(\\mathbf{A}^{-1}\\) exists, then \\(\\det(\\mathbf{A}) \\neq 0\\). Solution Exercise 2.25 Find \\(\\mathbf{A}^{-1}\\) and \\(\\mathbf{A}^T\\) for \\(\\begin{bmatrix} 2&amp;-1\\\\-4&amp;3\\end{bmatrix}\\). Solution Exercise 2.26 Find \\(\\mathbf{A}^{-1}\\) and \\(\\mathbf{A}^T\\) for \\(\\begin{bmatrix} 3&amp;4/3\\\\-3&amp;-1\\end{bmatrix}\\). Solution Exercise 2.27 Show that, for a 2 x 2 matrix, that \\((\\mathbf{A}^T)^{-1} = (\\mathbf{A}^{-1})^T\\). Solution Exercise 2.28 Show that the sum of symmetric matrices is symmetric. Solution Exercise 2.29 Find an example where the product of symmetric matrices is not symmetric. Solution Exercise 2.30 Prove that if \\(\\mathbf{A}\\mathbf{B} = \\mathbf{I}\\), then \\(\\mathbf{B}\\mathbf{A} = \\mathbf{I}\\). Solution Exercise 2.31 Prove Lemma 2.3 Solution Exercise 2.32 Prove Lemma 2.4 Solution Exercise 2.33 Prove Lemma 2.5 Solution "],["solving-systems-of-equations.html", "2.3 Solving Systems of Equations", " 2.3 Solving Systems of Equations We can represent a system of linear equations as \\[ \\begin{aligned} a_{11}x_1 + \\dots + a_{1n}x_n &amp;= b_1 \\\\ \\vdots \\quad\\quad \\quad\\quad \\quad\\quad \\vdots \\quad &amp; \\quad \\vdots \\\\ a_{m1}x_1 + \\dots + a_{mn}x_n &amp;= b_m \\end{aligned} \\] where \\(a_{ij}, b_i \\in \\mathbb{R}\\) are known constants and \\(x_j\\) are unknowns. This can be written compactly in matrix form as \\[ \\mathbf{A}\\mathbf{x} = \\mathbf{b}. \\] Matrices allow for a compact representation and straightforward manipulation of linear systems. Next, we focus on finding solutions to these systems and introduce the idea of a matrix inverse. 2.3.1 Particular and General Solutions Consider a system with fewer equations than unknowns. Such systems often have infinitely many solutions. To solve such a system of equations, follow these steps: Find a particular solution to \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\). Find all homogeneous solutions to \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\). Combine the results: \\[ \\mathbf{x} = \\mathbf{x}_p + \\mathbf{x}_h. \\] Neither the particular solution nor the general solution is unique. Example 2.35 \\[ \\begin{bmatrix} 1 &amp; 0 &amp; 8 &amp; -4 \\\\ 0 &amp; 1 &amp; 2 &amp; 12 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{bmatrix} = \\begin{bmatrix} 42 \\\\ 8 \\end{bmatrix}. \\] From inspection, we can find one particular solution: \\[ \\mathbf{x}_p = \\begin{bmatrix} 42 \\\\ 8 \\\\ 0 \\\\ 0 \\end{bmatrix}. \\] To find all possible solutions, we add all non-trivial combinations that satisfy \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\). Let \\(c_i\\) represent column \\(i\\). Then \\[c_3 = 8\\begin{bmatrix}1\\\\0 \\end{bmatrix} + 2\\begin{bmatrix}0 \\\\ 1 \\end{bmatrix} = 8c_1 + 2 c_2,\\] or rather \\[ \\begin{aligned} 8c_1 + 2c_2 - c_3 &amp;= 0, \\\\ -4c_1 + 12c_2 - c_4 &amp;= 0, \\end{aligned} \\] which yield two homogeneous solutions: \\[ \\mathbf{x}_1 = \\begin{bmatrix} 8 \\\\ 2 \\\\ -1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix} -4 \\\\ 12 \\\\ 0 \\\\ -1 \\end{bmatrix}. \\] Thus, the general solution is \\[ \\mathbf{x} = \\begin{bmatrix} 42 \\\\ 8 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\lambda_1 \\begin{bmatrix} 8 \\\\ 2 \\\\ -1 \\\\ 0 \\end{bmatrix} + \\lambda_2 \\begin{bmatrix} -4 \\\\ 12 \\\\ 0 \\\\ -1 \\end{bmatrix}, \\quad \\lambda_1, \\lambda_2 \\in \\mathbb{R}. \\] In general, systems of equations are not as simple as the example above. To solve any linear system, we can use Gaussian elimination, an algorithmic procedure that: Applies elementary row transformations to simplify the system, and Transforms it into a form where the three steps above can be applied directly. This provides a systematic way to find all solutions to \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\). 2.3.2 Elementary Transformations Definition 2.17 Elementary transformations are operations that simplify a system of linear equations without changing its solution set. These are: Row exchange: Swap two equations (rows). Row scaling: Multiply an equation (row) by a nonzero scalar \\(\\lambda \\in \\mathbb{R} \\setminus \\{0\\}\\). Row addition: Add a multiple of one equation (row) to another. These operations are key to transforming a system into simpler forms, such as row-echelon form (REF) or reduced row-echelon form (RREF). Example 2.36 Given a system of equations, \\[\\begin{align*} -2x_1 + 4x_2 - 2x_3 - x_4 + 4x_5 &amp;= 3\\\\ 4x_1 - 8x_2 + 3x_3 - 3x_4 + x_5 &amp;= 2\\\\ x_1 - 2x_2 + x_3 - x_4 + x_5 &amp;= 0\\\\ x_1 - 2x_2 - 3x_4 + 4x_5 &amp;= a\\\\ \\end{align*}\\] we write it in augmented matrix form: \\[ [\\mathbf{A} | \\mathbf{b}] = \\begin{bmatrix} -2 &amp; 4 &amp; -2 &amp; -1 &amp; 4 &amp; -3 \\\\ 4 &amp; -8 &amp; 3 &amp; -3 &amp; 1 &amp; 2 \\\\ 1 &amp; -2 &amp; 1 &amp; -1 &amp; 1 &amp; 0 \\\\ 1 &amp; -2 &amp; 0 &amp; -3 &amp; 4 &amp; a \\end{bmatrix} \\] By applying elementary row operations, we obtain a simpler (row-echelon) form: \\[ \\begin{bmatrix} 1 &amp; -2 &amp; 1 &amp; -1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 &amp; 3 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -2 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; a + 1 \\end{bmatrix} \\] The resulting equations are: \\[ \\begin{aligned} x_1 - 2x_2 + x_3 - x_4 + x_5 &amp;= 0 \\\\ x_3 - x_4 + 3x_5 &amp;= -2 \\\\ x_4 - 2x_5 &amp;= 1 \\\\ 0 &amp;= a + 1 \\end{aligned} \\] A particular solution exists only when \\(a = -1\\): \\[ \\mathbf{x}_p = \\begin{bmatrix} 2 \\\\ 0 \\\\ -1 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] The general solution is: \\[ \\mathbf{x} = \\begin{bmatrix} 2 \\\\ 0 \\\\ -1 \\\\ 1 \\\\ 0 \\end{bmatrix} + \\lambda_1 \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\lambda_2 \\begin{bmatrix} 2 \\\\ 0 \\\\ -1 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\lambda_1, \\lambda_2 \\in \\mathbb{R}. \\] Definition 2.18 A matrix is in row-echelon form if: All-zero rows are at the bottom. Each pivot (first nonzero entry from the left) is strictly to the right of the pivot in the row above. This produces a staircase structure. Definition 2.19 Basic variables correspond to pivot columns. Free variables correspond to non-pivot columns. In the previous example: \\(x_1, x_3, x_4\\) are basic; \\(x_2, x_5\\) are free. Definition 2.20 A system is in reduced row-echelon form if: It is in REF. Each pivot is 1. Each pivot is the only nonzero entry in its column. RREF makes it easy to: Identify basic and free variables. Read off particular and general solutions. Gaussian elimination is a process that systematically applies elementary transformations to bring a matrix to RREF. Gaussian elimination allows direct solution of \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) or \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\). Example 2.37 \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 3 &amp; 0 &amp; 0 &amp; 3 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 9 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -4 \\end{bmatrix} \\] Pivot columns: 1, 3, and 4 (basic variables). Non-pivot columns: 2 and 5 (free variables). The general solution to \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) is: \\[ \\mathbf{x} = \\lambda_1 \\begin{bmatrix} 3 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\lambda_2 \\begin{bmatrix} 3 \\\\ 0 \\\\ 9 \\\\ -4 \\\\ -1 \\end{bmatrix}, \\quad \\lambda_1, \\lambda_2 \\in \\mathbb{R}. \\] 2.3.3 The Minus-1 Trick For a homogeneous system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) in RREF: Extend \\(\\mathbf{A}\\) to an \\(n \\times n\\) matrix \\(\\tilde{\\mathbf{A}}\\) by adding rows with a single 1 where pivots are missing. The columns of \\(\\tilde{\\mathbf{A}}\\) containing 1 entries form a basis for the solution space (kernel/null space). Example 2.38 For \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 3 &amp; 0 &amp; 0 &amp; 3 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 9 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -4 \\end{bmatrix}, \\] we use the -1 trick to get \\[ \\tilde{\\mathbf{A}} = \\begin{bmatrix} 1 &amp; 3 &amp; 0 &amp; 0 &amp; 3 \\\\ 0 &amp; -1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 9 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -4 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; -1 \\end{bmatrix}. \\] Columns 2 and 5 are exactly the solutions for the homogeneous equation. The columns with 1 yield the same solution vectors as before. Example 2.39 Consider the homogeneous system: \\[ \\begin{cases} x + 2y - z = 0 \\\\ 2x + 4y - 2z = 0. \\end{cases} \\] We can write this as an augmented matrix: \\[ \\left[\\begin{array}{ccc|c} 1 &amp; 2 &amp; -1 &amp; 0 \\\\ 2 &amp; 4 &amp; -2 &amp; 0 \\end{array}\\right]. \\] Row reduction gives us \\[ \\left[\\begin{array}{ccc|c} 1 &amp; 2 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right] \\] We rewrite with the -1 trick: \\[ \\left[\\begin{array}{ccc|c} 1 &amp; 2 &amp; -1 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; -1 &amp; 0\\\\ \\end{array}\\right] \\] Hence the general (homogeneous) solution is: \\[ \\mathbf{x} = s\\begin{bmatrix} 2 \\\\ -1 \\\\ 0 \\end{bmatrix} + t\\begin{bmatrix} -1 \\\\ 0 \\\\ -1 \\end{bmatrix}, \\quad s,t \\in \\mathbb{R}. \\] 2.3.4 Calculating an Inverse Matrix via Gaussian Elimination Lemma 2.6 To find \\(\\mathbf{A}^{-1}\\) for \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\), use Gaussian elimination to transform: \\[ [\\mathbf{A} | \\mathbf{I}_n] \\;\\Rightarrow\\; [\\mathbf{I}_n | \\mathbf{A}^{-1}]. \\] Example 2.40 \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 2 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 \\end{bmatrix}. \\] Append the identity matrix. \\[ [\\mathbf{A}|\\mathbf{I}_4] = \\begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; 0 &amp;1&amp;0&amp;0&amp;0\\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp;0&amp;1&amp;0&amp;0\\\\ 1 &amp; 2 &amp; 0 &amp; 1 &amp;0&amp;0&amp;1&amp;0\\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp;0&amp;0&amp;0&amp;1 \\end{bmatrix}. \\] After elimination: \\[ [\\mathbf{I}_4|\\mathbf{A}^{-1}] = \\begin{bmatrix} 1&amp;0&amp;0&amp;0&amp; -1 &amp; 2 &amp; -2 &amp; 2 \\\\ 0&amp;1&amp;0&amp;0&amp; 1 &amp; -1 &amp; 2 &amp; -2 \\\\ 0&amp;0&amp;1&amp;0&amp; 1 &amp; -1 &amp; 1 &amp; -1 \\\\ 0&amp;0&amp;0&amp;1&amp; -1 &amp; 0 &amp; -1 &amp; 2 \\end{bmatrix}. \\] \\[ \\mathbf{A}^{-1} = \\begin{bmatrix} -1 &amp; 2 &amp; -2 &amp; 2 \\\\ 1 &amp; -1 &amp; 2 &amp; -2 \\\\ 1 &amp; -1 &amp; 1 &amp; -1 \\\\ -1 &amp; 0 &amp; -1 &amp; 2 \\end{bmatrix}. \\] Verification: You should check that \\[ \\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}_4. \\] 2.3.5 Algorithms for Solving a System of Linear Equations When solving a system of linear equations \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\), we typically assume that a solution exists. If not, approximate methods such as linear regression (see Chapter 9) are used. In special cases where \\(\\mathbf{A}\\) is square and invertible, the exact solution can be written as \\[ \\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}. \\] However, this is often not feasible, so we use the MoorePenrose pseudo-inverse: \\[ \\mathbf{x} = (\\mathbf{A}^{\\top}\\mathbf{A})^{-1}\\mathbf{A}^{\\top}\\mathbf{b}, \\] which provides the minimum-norm least-squares solution. Despite being conceptually simple, this approach is computationally expensive and numerically unstable, so it is rarely used in practice. For large-scale systems, we use iterative methods, which gradually improve an approximate solution. These methods include: Stationary iterative methods: Richardson, Jacobi, GaussSeidel, and successive over-relaxation. Krylov subspace methods: Conjugate gradients, generalized minimal residual (GMRES), and biconjugate gradients. These methods iteratively update the estimate of the solution according to \\[ \\mathbf{x}^{(k+1)} = \\mathbf{C}\\mathbf{x}^{(k)} + \\mathbf{d}, \\] reducing the residual error \\(\\|\\mathbf{x}^{(k+1)} - \\mathbf{x}^*\\|\\) at each step until convergence to the true solution \\(\\mathbf{x}^*\\). Exercises Exercise 2.34 Particular/ General solution example. Solution Exercise 2.35 Gaussian Elimination example. Solution Exercise 2.36 -1 trick example. Solution Exercise 2.37 Find the inverse of \\(\\begin{bmatrix}1&amp;2&amp;3\\\\2&amp;3&amp;0\\\\3&amp;0&amp;1\\end{bmatrix}\\) Solution Exercise 2.38 Moore-Penrose pseudo inverse example. Solution Exercise 2.39 Solve the system of equations \\[\\begin{align*}2x + 5y + 2z &amp; = - 38\\\\ 3x - 2y + 4z &amp; = 17\\\\ - 6x + y - 7z &amp; = - 12\\end{align*}\\] Solution Exercise 2.40 Solve the system of equations \\[\\begin{align*}2x - 4y + 5z &amp; = - 33\\\\ 4x - y &amp; = - 5\\\\ - 2x + 2y - 3z &amp; = 19\\end{align*}\\] Solution Exercise 2.41 Solve the system of equations \\[\\begin{align*}x - y &amp; = 6\\\\ - 2x + 2y &amp; = 1\\end{align*}\\] Solution Exercise 2.42 Solve the system of equations \\[\\begin{align*}2x + 5y &amp; = - 1\\\\ - 10x - 25y &amp; = 5\\end{align*}\\] Solution Exercise 2.43 Find the inverse of \\(\\begin{bmatrix}1&amp;0&amp;2&amp;3\\\\2&amp;3&amp;0&amp;1\\\\0&amp;3&amp;0&amp;1\\\\1&amp;0&amp;0&amp;1\\end{bmatrix}\\). Solution Exercise 2.44 Consider a \\(4 \\times 4\\) matrix. How many operations does Gaussian Elimination require? What about for an \\(n \\times n\\) matrix? Solution "],["vector-spaces-1.html", "2.4 Vector Spaces", " 2.4 Vector Spaces A vector space is a structured set of objects called vectors that can be added together and scaled by real numbers (scalars), while remaining within the same set. To understand this concept, we first introduce groups, which form the foundation for vector space structure. 2.4.1 Groups Definition 2.21 A group is a set \\(G\\) with an operation \\(\\otimes\\) that satisfies: Closure: \\(x \\otimes y \\in G\\) for all \\(x, y \\in G\\). Associativity: \\((x \\otimes y) \\otimes z = x \\otimes (y \\otimes z)\\). Neutral element: There exists \\(e \\in G\\) such that \\(x \\otimes e = e \\otimes x = x\\). Inverse element: For every \\(x \\in G\\), there exists \\(y \\in G\\) such that \\(x \\otimes y = e\\). If the operation is also commutative, the group is called Abelian. Example 2.41 \\((\\mathbb{Z}, +)\\) is an Abelian group. \\((\\mathbb{N}_0, +)\\) is not a group (no inverse elements). \\((\\mathbb{R} \\setminus \\{0\\}, \\cdot)\\) is an Abelian group. \\((\\mathbb{R}^{n \\times n}, \\cdot)\\) forms a group only for invertible matricescalled the general linear group, denoted \\(\\mathrm{GL}(n, \\mathbb{R})\\). This group is not Abelian because matrix multiplication is not commutative. Example 2.42 Prove that \\(\\mathrm{GL}(2,\\mathbb{R})\\) forms a group under multiplication. To show that \\(\\mathrm{GL}(2,\\mathbb{R})\\) forms a group, we verify each property: Closure Let \\(A, B \\in \\mathrm{GL}(2,\\mathbb{R})\\). Then both \\(A\\) and \\(B\\) are invertible, meaning there exist matrices \\(A^{-1}\\) and \\(B^{-1}\\) such that: \\[ AA^{-1} = A^{-1}A = I, \\quad BB^{-1} = B^{-1}B = I \\] We must show that \\(AB\\) is also invertible. Consider the product \\(B^{-1}A^{-1}\\). Then: \\[ (AB)(B^{-1}A^{-1}) = A(BB^{-1})A^{-1} = AIA^{-1} = AA^{-1} = I \\] and similarly, \\[ (B^{-1}A^{-1})(AB) = B^{-1}(A^{-1}A)B = B^{-1}IB = BB^{-1} = I \\] Hence, \\(B^{-1}A^{-1}\\) is the inverse of \\(AB\\). Therefore, \\(AB\\) is invertible and \\(AB \\in \\mathrm{GL}(2,\\mathbb{R})\\). Closure holds. Associativity Matrix multiplication is associative for all \\(2 \\times 2\\) real matrices. Thus, for any \\(A, B, C \\in \\mathrm{GL}(2,\\mathbb{R})\\): \\[ (AB)C = A(BC) \\] Associativity holds. Identity Element The \\(2 \\times 2\\) identity matrix \\(I\\) is its own inverse. It is therefore a \\(2 \\times 2\\) invertible matrix and so is in (2,)). Identity exists. Inverse Element By definition, each \\(A \\in \\mathrm{GL}(2,\\mathbb{R})\\) is invertible. So there exists \\(A^{-1}\\) such that: \\[ AA^{-1} = A^{-1}A = I \\] Moreover, since \\(A\\) is the inverse of \\(A^{-1}\\), we know that \\(A^{-1}\\) is also an invertible \\(2 \\times 2\\) matrix and so is in \\(\\mathrm{GL}(2,\\mathbb{R})\\). Inverses exist. Therefore, since all conditions are satisfied, \\(\\mathrm{GL}(2,\\mathbb{R})\\) is a group under matrix multiplication. Definition 2.22 A real-valued vector space \\(V = (V, +, \\cdot)\\) consists of: An inner operation (vector addition) \\(+ : V \\times V \\to V\\). An outer operation (scalar multiplication) \\(\\cdot : \\mathbb{R} \\times V \\to V\\). The operations satisfy: \\((V, +)\\) is an Abelian group. Distributivity: \\(\\lambda \\cdot (\\mathbf{x} + \\mathbf{y}) = \\lambda \\cdot \\mathbf{x} + \\lambda \\cdot \\mathbf{y}\\) \\((\\lambda + \\psi) \\cdot \\mathbf{x} = \\lambda \\cdot \\mathbf{x} + \\psi \\cdot \\mathbf{x}\\) Associativity: \\(\\lambda \\cdot (\\psi \\cdot \\mathbf{x}) = (\\lambda \\psi) \\cdot \\mathbf{x}\\) Neutral element: \\(\\mathbf{1} \\cdot \\mathbf{x} = \\mathbf{x}\\) The zero vector \\(\\mathbf{0}\\) acts as the neutral element of addition. Example 2.43 \\(\\mathbb{R}^n\\): Vectors added and scaled componentwise. \\(\\mathbb{R}^{m \\times n}\\): Matrices added and scaled elementwise. \\(\\mathbb{C}\\): The complex numbers under standard addition and multiplication. In notation, vectors are typically written as column vectors \\[ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}, \\] and their transposes \\(\\mathbf{x}^\\top\\) are row vectors. 2.4.2 Vector Subspaces Definition 2.23 A vector subspace \\(U \\subseteq V\\) is a subset of a vector space that is itself a vector space under the same operations. To be a subspace, \\(U\\) must satisfy: \\(\\mathbf{0} \\in U\\) Closure under scalar multiplication: \\(\\lambda \\mathbf{x} \\in U\\) Closure under addition: \\(\\mathbf{x} + \\mathbf{y} \\in U\\) Example 2.44 The set of all solutions to a homogeneous system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) is a subspace of \\(\\mathbb{R}^n\\). The set of solutions to an inhomogeneous system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) (where \\(\\mathbf{b} \\neq 0\\)) is not a subspace. The intersection of any number of subspaces is also a subspace. Exercises Exercise 2.45 SHow that each of these is a group: \\((\\mathbb{Z}, +)\\) is an Abelian group. \\((\\mathbb{N}_0, +)\\) is not a group (no inverse elements). \\((\\mathbb{R} \\setminus \\{0\\}, \\cdot)\\) is an Abelian group. \\((\\mathbb{R}^{n \\times n}, \\cdot)\\) forms a group only for invertible matricescalled the general linear group, denoted \\(\\mathrm{GL}(n, \\mathbb{R})\\). This group is not Abelian because matrix multiplication is not commutative. Solution Exercise 2.46 SHow that each of these is a vector space: \\(\\mathbb{R}^n\\): Vectors added and scaled componentwise. \\(\\mathbb{R}^{m \\times n}\\): Matrices added and scaled elementwise. \\(\\mathbb{C}\\): The complex numbers under standard addition and multiplication. Solution Exercise 2.47 Show each of these is a vector subspace: The set of all solutions to a homogeneous system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) is a subspace of \\(\\mathbb{R}^n\\). The set of solutions to an inhomogeneous system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) (where \\(\\mathbf{b} \\neq 0\\)) is not a subspace. The intersection of any number of subspaces is also a subspace. Solution Exercise 2.48 Prove that \\(\\mathbb{Z}\\) is a group under addition. Solution Exercise 2.49 Prove that \\(\\mathbb{Z}^+\\) is not a group under addition. Solution Exercise 2.50 Prove that \\(\\mathbb{R} \\setminus \\{0\\}\\) is a group under multiplication. Solution Exercise 2.51 Let \\(G\\) be the set of matrices of the form \\(\\begin{bmatrix}a&amp;b\\\\0&amp;c \\end{bmatrix}\\) where \\(a,b,c \\in \\mathbb{R}\\) and \\(ac \\not = 0\\). Prove that \\(G\\) forms a subgroup of \\(G(2\\mathbb{R})\\). Solution Exercise 2.52 Let \\(g\\) be an element of a group \\(G\\). Show that its inverse is unique. Solution Exercise 2.53 Let \\(G\\) be a group and let \\(H\\) be a non-empty subset of \\(G\\). Prove the following are equivalent by proving \\(1 \\Rightarrow 3 \\Rightarrow 2 \\Rightarrow 1\\). Solution Exercise 2.54 Prove that \\(\\mathbb{R}^n\\) is a vector space under componentwise addition and scalar multiplication. Solution Exercise 2.55 Prove that \\(\\mathbb{R}^{m \\times n}\\) is a vector space under componentwise addition and scalar multiplication. Solution Exercise 2.56 Solution "],["linear-independence.html", "2.5 Linear Independence", " 2.5 Linear Independence In a vector space, vectors can be added to each other and scaled by scalars. The closure property ensures the result is still within the same vector space. Some sets of vectors, when added and scaled, can be used to represent every vector in the space (through linear combinations). These sets form a basis for the space. Before defining a basis, we first define linear combinations and linear independence. Definition 2.24 A vector \\[ \\mathbf{v} = \\lambda_1 x_1 + \\lambda_2 x_2 + \\cdots + \\lambda_k x_k \\] is called a linear combination of the vectors \\(x_1, \\ldots, x_k\\) where \\(\\lambda_1, \\ldots, \\lambda_k \\in \\mathbb{R}\\). The zero vector can always be written as a linear combination by taking all \\(\\lambda_i = 0\\). A non-trivial linear combination occurs when at least one \\(\\lambda_i \\ne 0\\). Definition 2.25 A set of vectors \\(x_1, \\ldots, x_k \\in V\\) is linearly dependent if there exists a non-trivial combination \\[ \\lambda_1 x_1 + \\cdots + \\lambda_k x_k = 0, \\] with at least one \\(\\lambda_i \\ne 0\\). A set of vectors is linearly independent if the only solution is the trivial one \\[ \\lambda_1 = \\cdots = \\lambda_k = 0. \\] Intuitively, linearly independent vectors contain no redundancy. Removing one changes the span of the set. Example 2.45 Directions such as northwest and southwest form linearly independent directions in a 2D plane. A west direction can be expressed as a combination of those two, making the set linearly dependent. Vectors are either linearly dependent or independent  no third case exists. If any vector is zero or if two vectors are identical, the set is dependent. A set is dependent iff one vector is a linear combination of the others. Scalar multiples of a vector cause dependence: if \\(x_i = \\lambda x_j\\), the set is dependent. Example 2.46 Consider the vectors in \\(\\mathbb{R}^3\\): \\[ \\mathbf{v}_1 = \\begin{pmatrix}1 \\\\ 2 \\\\ 3\\end{pmatrix}, \\quad \\mathbf{v}_2 = \\begin{pmatrix}2 \\\\ 4 \\\\ 6\\end{pmatrix}, \\quad \\mathbf{v}_3 = \\begin{pmatrix}0 \\\\ 1 \\\\ -1\\end{pmatrix} \\] We check if there exist scalars \\(c_1, c_2, c_3\\), not all zero, such that \\[ c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + c_3 \\mathbf{v}_3 = \\mathbf{0}. \\] Notice that \\(\\mathbf{v}_2 = 2 \\mathbf{v}_1\\), so we can take \\[ c_1 = 2, \\quad c_2 = -1, \\quad c_3 = 0 \\] Then: \\[ 2\\mathbf{v}_1 - 1\\mathbf{v}_2 + 0\\mathbf{v}_3 = 2\\mathbf{v}_1 - 2\\mathbf{v}_1 + \\mathbf{0} = \\mathbf{0}. \\] Hence, \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}\\) is linearly dependent. 2.5.1 Gaussian Elimination Method Theorem 2.2 A set of vectors \\(V = \\{ \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k \\}\\) in \\(\\mathbb{R}^n\\) is linearly independent if, when the vectors are placed as columns of a matrix \\[ \\mathbf{A} = [\\mathbf{v}_1 \\ \\mathbf{v}_2 \\ \\cdots \\ \\mathbf{v}_k], \\] and elementary row operations are performed to reduce \\(\\mathbf{A}\\) to row echelon form, each column of \\(\\mathbf{A}\\) contains a pivot (a leading 1 in some row). Equivalently, the set \\(V\\) is linearly independent if and only if the reduced matrix has a pivot in every column. To test independence: Write the vectors as columns of a matrix \\(\\mathbf{A}\\). Perform row reduction to row echelon form. Pivot columns correspond to independent vectors. Non-pivot columns can be expressed as combinations of earlier vectors. The vectors are independent iff every column is a pivot column. Example 2.47 Given \\[ x_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ -3 \\\\ 4 \\end{bmatrix}, \\quad x_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 2 \\end{bmatrix}, \\quad x_3 = \\begin{bmatrix} -1 \\\\ -2 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\] we can put them into a matrix \\[\\begin{bmatrix} 1 &amp; 1 &amp; -1\\\\ 2 &amp; 1 &amp; -2 \\\\ -3 &amp; 0 &amp; 1 \\\\ 4 &amp; 2 &amp; 1 \\end{bmatrix}. \\] Row reduction shows all are pivot columns \\[\\begin{bmatrix} 1 &amp; 1 &amp; -1\\\\ 2 &amp; 1 &amp; -2 \\\\ -3 &amp; 0 &amp; 1 \\\\ 4 &amp; 2 &amp; 1 \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1 &amp; 1 &amp; -1\\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}. \\] Since all the columns are pivot columns, the set is linearly independent. Let \\(\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\) be linearly independent, and define \\(x_j = \\mathbf{B} \\mathbf{\\lambda}_j\\) with \\(\\mathbf{B} = [\\mathbf{b}_1 \\cdots \\mathbf{b}_k]\\). Then, vectors \\(\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_m\\}\\) are linearly independent iff the coefficient vectors \\(\\{\\mathbf{\\lambda}_1, \\ldots, \\mathbf{\\lambda}_m\\}\\) are linearly independent. Example 2.48 Let \\[ \\mathbf{b}_1 = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}, \\quad \\mathbf{b}_2 = \\begin{pmatrix}0 \\\\ 1 \\\\ 0\\end{pmatrix} \\] be linearly independent vectors in \\(\\mathbb{R}^3\\), and define \\(\\mathbf{B} = [\\mathbf{b}_1 \\ \\mathbf{b}_2]\\). Let the coefficient vectors be: \\[ \\mathbf{\\lambda}_1 = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}, \\quad \\mathbf{\\lambda}_2 = \\begin{pmatrix}3 \\\\ 4\\end{pmatrix}. \\] Then define \\[ \\mathbf{x}_j = \\mathbf{B} \\mathbf{\\lambda}_j. \\] Next, we compute \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) \\[ \\mathbf{x}_1 = \\mathbf{B}\\mathbf{\\lambda}_1 = \\begin{pmatrix}1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0\\end{pmatrix} \\begin{pmatrix}1 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 2 \\\\ 0\\end{pmatrix}, \\quad \\mathbf{x}_2 = \\mathbf{B}\\mathbf{\\lambda}_2 = \\begin{pmatrix}3 \\\\ 4 \\\\ 0\\end{pmatrix}. \\] The claim is that \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) are linearly independent. To verify this, check if \\(c_1 \\mathbf{x}_1 + c_2 \\mathbf{x}_2 = \\mathbf{0}\\) has only the trivial solution. \\[ c_1 \\begin{pmatrix}1 \\\\ 2 \\\\ 0\\end{pmatrix} + c_2 \\begin{pmatrix}3 \\\\ 4 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}c_1 + 3c_2 \\\\ 2c_1 + 4c_2 \\\\ 0\\end{pmatrix} = \\mathbf{0}. \\] Solve: \\[ c_1 + 3c_2 = 0 \\quad \\text{and} \\quad 2c_1 + 4c_2 = 0. \\] Both equations are consistent and lead to the trivial solution \\(c_1 = 0, c_2 = 0\\). Hence, \\(\\{\\mathbf{x}_1, \\mathbf{x}_2\\}\\) is linearly independent. In any vector space \\(V\\), if there are more vectors than dimensions (\\(m &gt; k\\)), the vectors are linearly dependent. Exercises Exercise 2.57 Determine if the set of vectors is linearly independent: \\[ \\begin{aligned} x_1 &amp;= b_1 - 2b_2 + b_3 - b_4, \\\\ x_2 &amp;= -4b_1 - 2b_2 + 4b_4, \\\\ x_3 &amp;= 2b_1 + 3b_2 - b_3 - 3b_4, \\\\ x_4 &amp;= 17b_1 - 10b_2 + 11b_3 + b_4, \\end{aligned} \\] Solution Exercise 2.58 Prove that if any vector is zero or if two vectors are identical, the set is dependent. Solution Exercise 2.59 Prove that a set of vectors is dependent iff one vector is a linear combination of the others. Solution Exercise 2.60 Prove that scalar multiples of a vector cause dependence: if \\(x_i = \\lambda x_j\\), the set is dependent. Solution Exercise 2.61 Are the vectors \\(\\left\\{\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}, \\begin{bmatrix}3\\\\5\\\\7\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\2\\end{bmatrix}\\right\\}\\) linearly independent? Solution Exercise 2.62 Are the vectors \\(\\left\\{\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}, \\begin{bmatrix}3\\\\2\\\\9\\end{bmatrix}, \\begin{bmatrix}5\\\\2\\\\-1\\end{bmatrix}\\right\\}\\) linearly independent? Solution Exercise 2.63 Are \\(p(x) = 1 + 3x + 2x^2\\), \\(q(x) = 3 + x + 2x^2\\) and \\(r(x) = 2x + x^2\\) linearly independent? Solution "],["basis-and-rank.html", "2.6 Basis and Rank", " 2.6 Basis and Rank 2.6.1 Generating Set and Basis Definition 2.26 Let \\(V = (V, +, \\cdot)\\) be a vector space and \\(A = \\{ \\mathbf{x}_1, \\ldots, \\mathbf{x}_k \\} \\subseteq V\\). If every vector \\(\\mathbf{v} \\in \\mathbf{V}\\) can be written as a linear combination of \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_k\\), then \\(A\\) is called a generating set of \\(\\mathbf{V}\\). The set of all such linear combinations is called the span of \\(A\\), denoted by: \\[ V = \\text{span}[A] = \\text{span}[\\mathbf{x}_1, \\ldots, \\mathbf{x}_k]. \\] A generating set spans the entire vector space, meaning every vector can be expressed as a combination of those in the set. Definition 2.27 A generating set \\(A \\subseteq V\\) is minimal if no smaller subset \\(\\tilde{A} \\subset A\\) spans \\(V\\). Every linearly independent generating set of \\(V\\) is minimal and is called a basis of \\(V\\). Equivalently: A basis is a minimal generating set. A basis is also a maximal linearly independent set (adding any new vector makes it dependent). Theorem 2.3 For a basis \\(B = \\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\}\\), every vector \\(\\mathbf{x} \\in V\\) can be expressed uniquely as: \\[ \\mathbf{x} = \\sum_{i=1}^{k} \\lambda_i \\mathbf{b}_i = \\sum_{i=1}^{k} \\psi_i \\mathbf{b}_i, \\] where \\(\\lambda_i = \\psi_i\\) for all \\(i = 1, \\ldots, k\\). Example 2.49 The standard basis is: \\[ B= \\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\right\\}. \\] Other valid bases include: \\[ B_1 = \\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\right\\}, \\quad B_2 = \\left\\{ \\begin{bmatrix} 0.5 \\\\ 0.8 \\\\ 0.4 \\end{bmatrix}, \\begin{bmatrix} 1.8 \\\\ 0.3 \\\\ 0.3 \\end{bmatrix}, \\begin{bmatrix} -2.2 \\\\ -1.3 \\\\ 3.5 \\end{bmatrix} \\right\\}. \\] Every vector space \\(V\\) has a basis, but there are usually many possible bases. All bases of \\(V\\) contain the same number of vectors, called basis vectors. The dimension of \\(V\\), written \\(\\text{dim}(V)\\), is the number of basis vectors. If \\(U \\subseteq V\\) is a subspace, then: \\[ \\text{dim}(U) \\leq \\text{dim}(V), \\] with equality if and only if \\(U = V\\). To find a basis for a subspace \\(U = \\text{span}[\\mathbf{x}_1, \\ldots, \\mathbf{x}_m] \\subseteq \\mathbb{R}^n\\): Form a matrix \\(\\mathbf{A}\\) with the spanning vectors as columns. Compute the row echelon form of \\(\\mathbf{A}\\). The columns corresponding to pivot positions form a basis for \\(U\\). Example 2.50 Let \\(U \\subseteq \\mathbb{R}^5\\) be spanned by the vectors: \\[ \\mathbf{x}_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\\\ -1 \\\\ -1 \\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix} 2 \\\\ -1 \\\\ 1 \\\\ 2 \\\\ -2 \\end{bmatrix}, \\quad \\mathbf{x}_3 = \\begin{bmatrix} 3 \\\\ -4 \\\\ 3 \\\\ 5 \\\\ -3 \\end{bmatrix}, \\quad \\mathbf{x}_4 = \\begin{bmatrix} -1 \\\\ 8 \\\\ -5 \\\\ -6 \\\\ 1 \\end{bmatrix}. \\] After performing Gaussian elimination on the matrix \\([\\mathbf{x}_1 \\ \\mathbf{x}_2 \\ \\mathbf{x}_3 \\ \\mathbf{x}_4]\\), the pivot columns correspond to \\(\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_4\\), which are linearly independent. Hence, \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_4\\}\\) is a basis of \\(U\\) and \\(\\text{dim}(U) = 3\\). Note also that the standard basis for \\(\\mathbb{R}^5\\) has 5 vectors (each one with 0s in each entry other than the \\(n^{th}\\) spot). So, \\(\\text{dim}(U) &lt; \\text{dim}(V)\\). 2.6.2 Rank Definition 2.28 The rank of a matrix \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\), denoted \\(\\text{rk}(\\mathbf{A})\\), is the number of linearly independent columns (or rows) of \\(\\mathbf{A}\\). Definition 2.29 Let \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) be a matrix. The column space of \\(\\mathbf{A}\\), denoted as \\(\\text{Col}(\\mathbf{A})\\), is the subspace of \\(\\mathbb{R}^m\\) spanned by the columns of \\(\\mathbf{A}\\). In other words, \\[ \\text{Col}(\\mathbf{A}) = \\text{span} \\{ \\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n \\}, \\] where \\(\\mathbf{a}_i\\) is the \\(i^\\text{th}\\) column of \\(\\mathbf{A}\\). The column space represents all possible vectors \\(\\mathbf{b}\\) for which the linear system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) has a solution. Definition 2.30 The row space of \\(\\mathbf{A}\\), denoted as \\(\\text{Row}(\\mathbf{A})\\), is the subspace of \\(\\mathbb{R}^n\\) spanned by the rows of \\(\\mathbf{A}\\). Equivalently, \\[ \\text{Row}(\\mathbf{A}) = \\text{span} \\{ \\mathbf{r}_1, \\mathbf{r}_2, \\ldots, \\mathbf{r}_m \\}, \\] where \\(\\mathbf{r}_i\\) is the \\(i^\\text{th}\\) row of \\(\\mathbf{A}\\). The row space contains all possible linear combinations of the rows of \\(\\mathbf{A}\\). Lemma 2.7 Properties of the rank: Let \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) matrix and \\(\\mathbf{x}\\) and \\(\\mathbf{b}\\) vectors of length \\(n\\). \\(\\text{rk}(\\mathbf{A}) = \\text{rk}(\\mathbf{A}^\\top)\\): column rank equals row rank. The column space of \\(\\mathbf{A}\\) has dimension \\(\\text{rk}(\\mathbf{A})\\). The row space of \\(\\mathbf{A}\\) also has dimension \\(\\text{rk}(\\mathbf{A})\\). \\(\\mathbf{A}\\) is invertible iff \\(\\text{rk}(\\mathbf{A}) = n\\) (for \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\)). The system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) has a solution iff \\(\\text{rk}(\\mathbf{A}) = \\text{rk}([\\mathbf{A}|\\mathbf{b}])\\). The null space of \\(\\mathbf{A}\\) (solutions of \\(\\mathbf{A}\\mathbf{x} = 0\\)) has dimension \\(n - \\text{rk}(\\mathbf{A})\\). A matrix has full rank if \\(\\text{rk}(\\mathbf{A}) = \\min(m, n)\\). If \\(\\text{rk}(\\mathbf{A}) &lt; \\min(m, n)\\), it is rank deficient. Example 2.51 \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\Rightarrow \\text{rk}(\\mathbf{A}) = 2. \\] \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 1 \\\\ -2 &amp; -3 &amp; 1 \\\\ 3 &amp; 5 &amp; 0 \\end{bmatrix} \\Rightarrow \\begin{bmatrix} 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 3 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\Rightarrow \\text{rk}(\\mathbf{A}) = 2. \\] Exercises Exercise 2.64 Prove that \\(\\text{rk}(\\mathbf{A}) = \\text{rk}(\\mathbf{A}^\\top)\\): column rank equals row rank. Solution Exercise 2.65 Prove that the column space of \\(\\mathbf{A}\\) has dimension \\(\\text{rk}(\\mathbf{A})\\). Prove that the row space of \\(\\mathbf{A}\\) also has dimension \\(\\text{rk}(\\mathbf{A})\\). Solution Exercise 2.66 Prove that \\(\\mathbf{A}\\) is invertible iff \\(\\text{rk}(\\mathbf{A}) = n\\) (for \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\)). Solution Exercise 2.67 Prove that the system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) has a solution iff \\(\\text{rk}(\\mathbf{A}) = \\text{rk}([\\mathbf{A}|\\mathbf{b}])\\). Solution Exercise 2.68 Prove that the null space of \\(\\mathbf{A}\\) (solutions of \\(\\mathbf{A}\\mathbf{x} = 0\\)) has dimension \\(n - \\text{rk}(\\mathbf{A})\\). Solution Exercise 2.69 Let \\(\\mathbf{A}\\) be a regular \\(2 \\times 2\\) (invertible) matrix. Show that \\(\\mathbf{A}\\) has rank 2. Solution Exercise 2.70 In \\(\\mathbb{R}^{2 \\times 2}\\), show that \\(rk(\\mathbf{A}) = 1\\) if \\(\\det(\\mathbf{A}) = 0\\), but \\(\\mathbf{A}\\) is not the zero matrix. Solution Exercise 2.71 Find the rank of \\(\\mathbf{A} = \\begin{bmatrix}1&amp;0&amp;2&amp;1\\\\ 0&amp;2&amp;4&amp;2\\\\0&amp;2&amp;2&amp;1 \\end{bmatrix}\\). Solution Exercise 2.72 Find the rank of \\(\\mathbf{A} = \\begin{bmatrix}1&amp;2&amp;1&amp;-1\\\\ 9&amp;5&amp;2&amp;2\\\\ 7&amp;1&amp;0&amp;4 \\end{bmatrix}\\). Solution Exercise 2.73 Let \\(\\mathbf{x}\\) be a unit vector in \\(\\mathbb{R}^n\\). Partition \\(\\mathbf{x}\\) as \\[\\mathbf{x} = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\x_n \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ \\mathbf{y} \\end{bmatrix}.\\] Let \\[\\mathbf{Q} = \\begin{bmatrix}x_1 &amp; \\mathbf{y}^T\\\\ \\mathbf{y} &amp; \\mathbf{I} - \\left(\\dfrac{1}{1 - x_1} \\right) \\mathbf{y} \\mathbf{y}^T \\end{bmatrix}.\\] \\(Q\\) is orthogonal. (This procedure gives a quick method for finding an orthonormal basis for \\(\\mathbb{R}^n\\) with a prescribed first vector \\(\\mathbf{x}\\), a construction that is frequently useful in applications). Select any non-trivial vector in \\(\\mathbb{R}^3\\) and verify that \\(\\mathbf{Q}\\) is orthogonal. Solution "],["linear-mappings.html", "2.7 Linear Mappings", " 2.7 Linear Mappings Linear mappings are functions between vector spaces that preserve vector addition and scalar multiplication. That is, for vector spaces \\(V\\) and \\(W\\), a mapping \\(\\Phi : V \\to W\\) is linear if: \\[ \\Phi(x + y) = \\Phi(x) + \\Phi(y), \\quad \\Phi(\\lambda x) = \\lambda \\Phi(x) \\] for all \\(x, y \\in V\\) and scalars \\(\\lambda \\in \\mathbb{R}\\). Definition 2.31 A linear mapping (or linear transformation) is a function \\(\\Phi : V \\to W\\) satisfying: \\[ \\Phi(\\lambda x + \\psi y) = \\lambda \\Phi(x) + \\psi \\Phi(y) \\] for all \\(x, y \\in V\\) and scalars \\(\\lambda, \\psi \\in \\mathbb{R}\\). Example 2.52 A linear mapping \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) can be represented by a matrix. Consider the matrix \\[ \\mathbf{A} = \\begin{pmatrix} 2 &amp; -1 \\\\ 3 &amp; 4 \\end{pmatrix}. \\] Define the linear map \\(\\Phi(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\\). Let \\[ \\mathbf{x} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}. \\] Then \\[ \\Phi(\\mathbf{x}) = A\\mathbf{x} = \\begin{pmatrix} 2 &amp; -1 \\\\ 3 &amp; 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 2(1) - 1(2) \\\\ 3(1) + 4(2) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 11 \\end{pmatrix}. \\] The map is linear because \\[\\Phi(\\mathbf{v} + \\mathbf{w}) = \\mathbf{A}(\\mathbf{v} + \\mathbf{w}) = \\mathbf{A}\\mathbf{v} + \\mathbf{A}\\mathbf{w} = \\Phi(\\mathbf{v}) + \\Phi(\\mathbf{w}) \\quad \\text{and} \\quad \\phi(\\lambda \\mathbf{v}) = \\mathbf{A}(\\lambda \\mathbf{v}) = \\lambda \\mathbf{A}\\mathbf{v} = \\lambda \\Phi(\\mathbf{v}).\\] Injective, surjective and bijective mappings are important in machine learning. Bijective maps are particularly useful because they are invertible. Definition 2.32 Let \\(\\Phi : V \\to W\\) be a mapping. Then \\(\\Phi\\) is injective if \\(\\Phi(x) = \\Phi(y) \\Rightarrow x = y\\). \\(\\Phi\\) is surjective if \\(\\Phi(V) = W\\). \\(\\Phi\\) is bijective if it is both injective and surjective. Example 2.53 The linear map \\(\\Phi(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\\), with \\[ \\mathbf{A} = \\begin{pmatrix} 2 &amp; -1 \\\\ 3 &amp; 4 \\end{pmatrix} \\] is bijective. One way you can determine if a function is invertible is to determine if it can be inverted. In this case, we can find the inverse function by multiplying by the inverse of \\(\\mathbf{A}\\) (if it exists). Since the determinant of \\(\\mathbf{A} \\not = 0\\), the matrix (and hence the function) is invertible. Therefore, the function is bijective. Some other special cases of linear mappings include: Isomorphism: Linear and bijective (\\(\\Phi : V \\to W\\)) Endomorphism: Linear map from \\(V\\) to itself (\\(\\Phi : V \\to V\\)) Automorphism: Linear and bijective endomorphism Example 2.54 The function \\[ f: \\mathbb{R} \\to \\mathbb{R}, \\qquad f(x) = 3x - 2, \\] is a bijection from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\). Since it is also linear, it is an automorphism on \\(\\mathbb{R}\\). The function \\[ g: \\mathbb{R} \\to \\mathbb{R}, \\qquad g(x) = x^2, \\] is not a bijection. In fact, it is neither injective or surjective. Its not injective since \\[ g(2) = 4, \\qquad g(-2) = 4. \\] Since \\[ 2 \\neq -2 \\quad \\text{but} \\quad g(2) = g(-2), \\] the function is not injective. Also, since the codomain is \\(\\mathbb{R}\\), but \\(g(x) = x^2\\) only produces nonnegative outputs, negative numbers (e.g.Â \\(-5\\)) cannot be written as \\(x^2\\) for any real \\(x\\). Thus it does not hit every value in the codomain. Therefore, it is not surjective. The identity mapping is denoted \\(\\text{id}_V(x) = x\\). Theorem 2.4 Finite-dimensional vector spaces \\(V\\) and \\(W\\) are isomorphic if and only if: \\[ \\dim(V) = \\dim(W) \\] This means that spaces of equal dimension are structurally the same, as they can be related through a linear bijective map. 2.7.1 Matrix Representation of Linear Mappings Every linear mapping between finite-dimensional vector spaces can be represented by a matrix. Lemma 2.8 Given bases \\(\\mathbf{B} = (\\mathbf{b}_1, \\dots, \\mathbf{b}_n)\\) for \\(V\\) and \\(\\mathbf{C} = (\\mathbf{c}_1, \\dots, \\mathbf{c}_m)\\) for \\(W\\), the transformation matrix \\(\\mathbf{A}_\\Phi\\) of \\(\\Phi : V \\to W\\) is defined by: \\[ \\Phi(\\mathbf{b}_j) = \\sum_{i=1}^m \\alpha_{ij} \\mathbf{c}_i \\] where \\(\\mathbf{A}_\\Phi(i, j) = \\alpha_{ij}\\). For coordinate vectors \\(\\hat{\\mathbf{x}} \\in V\\) and \\(\\hat{\\mathbf{y}} \\in W\\), \\[ \\hat{\\mathbf{y}} = \\mathbf{A}_\\Phi \\hat{\\mathbf{x}} \\] Example 2.55 Let \\(V = \\mathbb{R}^2\\) with standard basis \\[ \\mathbf{B} = (\\mathbf{b}_1, \\mathbf{b}_2) = \\left( \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}, \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} \\right) \\] \\(W = \\mathbb{R}^2\\) with a nonstandard basis \\[ \\mathbf{C} = (\\mathbf{c}_1, \\mathbf{c}_2) = \\left( \\begin{bmatrix}2 \\\\ 1\\end{bmatrix}, \\begin{bmatrix}1 \\\\ 3\\end{bmatrix} \\right). \\] Suppose the linear transformation \\(\\Phi : \\mathbb{R}^2 \\to \\mathbb{R}^2\\) is \\[ \\Phi(x,y) = (3x + y, \\; x - 2y). \\] Since \\(\\mathbf{B}\\) is the standard basis, the inputs are easy: \\[\\begin{align*} \\mathbf{b}_1 = (1,0) \\qquad &amp; \\Longrightarrow \\qquad \\Phi(\\mathbf{b}_1) = \\Phi(1,0) = (3, \\; 1)\\\\ \\mathbf{b}_2 = (0,1) \\qquad &amp; \\Longrightarrow \\qquad \\Phi(\\mathbf{b}_2) = \\Phi(0,1) = (1, \\; -2) \\end{align*}\\] These must now be written in basis C. We want to find scalars \\(\\alpha_{ij}\\) such that \\[ \\Phi(\\mathbf{b}_j) = \\alpha_{1j}\\mathbf{c}_1 + \\alpha_{2j}\\mathbf{c}_2. \\] For \\(\\Phi(\\mathbf{b}_1) = (3,1)\\), solve: \\[ \\alpha_{11}\\begin{bmatrix}2\\\\1\\end{bmatrix} + \\alpha_{21}\\begin{bmatrix}1\\\\3\\end{bmatrix} = \\begin{bmatrix}3\\\\1\\end{bmatrix} \\qquad \\Longrightarrow \\qquad \\alpha_{11} = \\frac{8}{5}, \\qquad \\alpha_{21} = -\\frac{1}{5}. \\] For \\(\\Phi(\\mathbf{b}_2) = (1,-2)\\), we solve: \\[ \\alpha_{12}\\begin{bmatrix}2\\\\1\\end{bmatrix} + \\alpha_{22}\\begin{bmatrix}1\\\\3\\end{bmatrix} = \\begin{bmatrix}1\\\\ -2\\end{bmatrix} \\qquad \\Longrightarrow \\qquad \\alpha_{12} = \\frac{5}{7}, \\qquad \\alpha_{22} = -\\frac{3}{7}. \\] Therefore, the transformation matrix is (by the lemma): \\[ A_\\Phi(i,j) = \\alpha_{ij} \\qquad \\Longrightarrow \\qquad A_\\Phi^{\\mathbf{C}\\leftarrow\\mathbf{B}} = \\begin{bmatrix} \\frac{8}{5} &amp; \\frac{5}{7} \\\\ -\\frac{1}{5} &amp; -\\frac{3}{7} \\end{bmatrix}. \\] This matrix converts B-coordinates of \\(x\\) into C-coordinates of \\(\\Phi(x)\\): \\[ [\\Phi(x)]_{\\mathbf{C}} = A_\\Phi^{\\mathbf{C}\\leftarrow\\mathbf{B}} \\,[x]_{\\mathbf{B}}. \\] For an example with a specific vector, let \\[ [x]_{\\mathbf{B}} = \\begin{bmatrix}2 \\\\ -1\\end{bmatrix}. \\] Compute: \\[ [\\Phi(x)]_{\\mathbf{C}} = \\begin{bmatrix} \\frac{8}{5} &amp; \\frac{5}{7} \\\\ -\\frac{1}{5} &amp; -\\frac{3}{7} \\end{bmatrix} \\begin{bmatrix}2\\\\ -1\\end{bmatrix} = \\begin{bmatrix} \\frac{16}{5} - \\frac{5}{7} \\\\[4pt] -\\frac{2}{5} + \\frac{3}{7} \\end{bmatrix} = \\begin{bmatrix} \\frac{87}{35} \\\\[4pt] \\frac{1}{35} \\end{bmatrix}. \\] So the coordinate vector of \\(\\Phi(x)\\) in basis C is: \\[ [\\Phi(x)]_{\\mathbf{C}} = \\begin{bmatrix}87/35 \\\\ 1/35\\end{bmatrix}. \\] If we simply want to convert a vector from \\(\\mathbf{B}\\) coordinates to \\(\\mathbf{C}\\) coordinates, use the same process with \\(\\Phi(\\mathbf{x}) = \\mathbf{x}\\) (that is, omit the beginning portion of the calculation). 2.7.2 Coordinate Systems and Bases A basis defines a coordinate system for a vector space. Definition 2.33 Given an ordered basis \\(\\mathbf{B} = ( \\mathbf{b}_1, \\dots, \\mathbf{b}_n)\\), any vector \\(x \\in V\\) can be uniquely represented as: \\[ \\mathbf{x} = \\alpha_1 \\mathbf{b}_1 + \\cdots + \\alpha_n \\mathbf{b}_n \\] The vector \\(\\alpha = [\\alpha_1, \\dots, \\alpha_n]^T\\) is the coordinate vector of \\(x\\) with respect to \\(\\mathbf{B}\\). Example 2.56 Let \\[ \\mathbf{B} = (\\mathbf{b}_1, \\mathbf{b}_2) = \\left( \\begin{bmatrix}1 \\\\ 2\\end{bmatrix}, \\begin{bmatrix}3 \\\\ 1\\end{bmatrix} \\right). \\] Let \\[ \\mathbf{x} = \\begin{bmatrix}7 \\\\ 5\\end{bmatrix}. \\] We want to express \\(\\mathbf{x}\\) as a unique linear combination of the basis vectors: \\[\\begin{align*} \\mathbf{x} &amp;= \\alpha_1 \\mathbf{b}_1 + \\alpha_2 \\mathbf{b}_2\\\\ \\begin{bmatrix}7 \\\\ 5\\end{bmatrix} &amp;= \\alpha_1 \\begin{bmatrix}1 \\\\ 2\\end{bmatrix} + \\alpha_2 \\begin{bmatrix}3 \\\\ 1\\end{bmatrix}. \\end{align*}\\] This gives the system: \\[ \\begin{cases} \\alpha_1 + 3\\alpha_2 = 7 \\\\ 2\\alpha_1 + \\alpha_2 = 5 \\end{cases} \\] Solving: \\[ \\alpha_1 = \\frac{8}{5}, \\qquad \\alpha_2 = \\frac{9}{5} \\qquad \\Longrightarrow \\qquad [\\mathbf{x}]_{\\mathbf{B}} = \\begin{bmatrix} 8/5 \\\\[4pt] 9/5 \\end{bmatrix}. \\] So, the coordinate vector of \\(\\mathbf{x} = \\begin{bmatrix}7 \\\\ 5\\end{bmatrix}\\) with respect to the basis \\(\\mathbf{B} = \\left(\\begin{bmatrix}1 \\\\ 2\\end{bmatrix}, \\begin{bmatrix}3 \\\\ 1\\end{bmatrix}\\right)\\) is \\[[\\mathbf{x}]_{\\mathbf{B}} = \\begin{bmatrix} 8/5 \\\\ 9/5 \\end{bmatrix}.\\] 2.7.3 Basis Change and Equivalence When the bases of \\(V\\) and \\(W\\) are changed, the transformation matrix changes accordingly. If \\(S\\) and \\(T\\) are the change-of-basis matrices for \\(V\\) and \\(W\\), then: \\[ \\tilde{\\mathbf{A}}_\\Phi = T^{-1} \\mathbf{A}_\\Phi S \\] Example 2.57 Let \\(V=W=\\mathbb{R}^2\\) and let \\(\\Phi:V\\to W\\) be the linear map whose matrix in the standard basis is \\[ \\mathbf{A} = \\begin{bmatrix} 2 &amp; 1\\\\[4pt] 0 &amp; 3 \\end{bmatrix}, \\qquad \\text{i.e. }\\; \\Phi(x)=\\mathbf{A}x. \\] Now pick new ordered bases for \\(V\\) and \\(W\\): New basis of \\(V\\) (columns of \\(S\\)): \\[ \\mathbf{B} = (b_1,b_2),\\qquad S = [\\,b_1\\; b_2\\,] = \\begin{bmatrix} 1 &amp; 1\\\\ 1 &amp; -1 \\end{bmatrix}. \\] (So \\(b_1=(1,1)^\\top,\\; b_2=(1,-1)^\\top\\).) New basis of \\(W\\) (columns of \\(T\\)): \\[ \\mathbf{C} = (c_1,c_2),\\qquad T = [\\,c_1\\; c_2\\,] = \\begin{bmatrix} 2 &amp; 0\\\\ 0 &amp; 3 \\end{bmatrix}. \\] (So \\(c_1=(2,0)^\\top,\\; c_2=(0,3)^\\top\\).) Recall the change-of-basis formula: \\[ \\widetilde{\\mathbf{A}}_\\Phi \\;=\\; T^{-1}\\,\\mathbf{A}_\\Phi\\,S, \\] where \\(\\widetilde{\\mathbf{A}}_\\Phi\\) is the matrix of \\(\\Phi\\) in the new bases \\(\\mathbf{B},\\mathbf{C}\\). Compute \\(\\mathbf{A}S\\): \\[ \\mathbf{A}S = \\begin{bmatrix}2&amp;1\\\\[4pt]0&amp;3\\end{bmatrix} \\begin{bmatrix}1&amp;1\\\\[4pt]1&amp;-1\\end{bmatrix} = \\begin{bmatrix}3 &amp; 1\\\\[4pt]3 &amp; -3\\end{bmatrix}. \\] Compute \\(T^{-1}\\) and then \\(\\widetilde{\\mathbf{A}}_\\Phi\\): \\[ T^{-1} = \\begin{bmatrix}1/2 &amp; 0\\\\[4pt]0 &amp; 1/3\\end{bmatrix},\\qquad \\widetilde{\\mathbf{A}}_\\Phi = T^{-1}(\\mathbf{A}S) = \\begin{bmatrix}1/2 &amp; 0\\\\[4pt]0 &amp; 1/3\\end{bmatrix} \\begin{bmatrix}3 &amp; 1\\\\[4pt]3 &amp; -3\\end{bmatrix} = \\begin{bmatrix}3/2 &amp; 1/2\\\\[4pt]1 &amp; -1\\end{bmatrix}. \\] So in the new bases \\(\\mathbf{B},\\mathbf{C}\\) the transformation matrix is [ _= \\[\\begin{bmatrix}3/2 &amp; 1/2\\\\[4pt]1 &amp; -1\\end{bmatrix}\\] . ] We can check the same result by mapping the new domain basis vectors and expressing the results in the new codomain basis: \\(\\Phi(b_1) = \\mathbf{A} b_1 = \\mathbf{A}\\begin{bmatrix}1\\\\[2pt]1\\end{bmatrix} = \\begin{bmatrix}3\\\\[2pt]3\\end{bmatrix}.\\) Solve \\(\\begin{bmatrix}3\\\\3\\end{bmatrix} = \\alpha_1 c_1 + \\alpha_2 c_2 = \\alpha_1\\begin{bmatrix}2\\\\0\\end{bmatrix}+\\alpha_2\\begin{bmatrix}0\\\\3\\end{bmatrix}\\). This gives \\(\\alpha_1=3/2,\\; \\alpha_2=1\\). So the first column of \\(\\widetilde{\\mathbf{A}}_\\Phi\\) is \\(\\begin{bmatrix}3/2\\\\[2pt]1\\end{bmatrix}\\). \\(\\Phi(b_2) = \\mathbf{A} b_2 = \\mathbf{A}\\begin{bmatrix}1\\\\[2pt]-1\\end{bmatrix} = \\begin{bmatrix}1\\\\[2pt]-3\\end{bmatrix}.\\) Solve \\(\\begin{bmatrix}1\\\\-3\\end{bmatrix} = \\beta_1 c_1 + \\beta_2 c_2\\). This gives \\(\\beta_1=1/2,\\; \\beta_2=-1\\). So the second column is \\(\\begin{bmatrix}1/2\\\\[2pt]-1\\end{bmatrix}\\). These columns match \\(\\widetilde{\\mathbf{A}}_\\Phi\\) above, confirming \\[ \\widetilde{\\mathbf{A}}_\\Phi = T^{-1}\\mathbf{A}S. \\] 2.7.4 Image and Kernel of a Linear Mapping The image and kernel are important subspaces associated with a linear mapping. Definition 2.34 For a linear mapping \\(\\Phi : V \\to W\\), the kernel / null space of a mapping is the set of values that map to \\(0_W \\in W\\). \\[ \\ker(\\Phi) := \\{ \\mathbf{v} \\in V : \\Phi(\\mathbf{v}) = \\mathbf{0}_W \\}. \\] The image / range is the set of value which get mapped to. \\[ \\mathrm{Im}(\\Phi) := \\{ \\mathbf{w} \\in W : \\exists \\mathbf{v} \\in \\mathbf{V}, \\Phi(\\mathbf{v}) = \\mathbf{w} \\} \\] \\(\\mathbf{0}_V \\in \\ker(\\Phi)\\), so the null space is never empty. \\(\\ker(\\Phi) \\subseteq V\\) and \\(\\mathrm{Im}(\\Phi) \\subseteq W\\) are subspaces. \\(\\Phi\\) is injective if and only if \\(\\ker(\\Phi) = \\{\\mathbf{0}\\}\\). Definition 2.35 For a matrix \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) representing \\(\\Phi : \\mathbb{R}^n \\to \\mathbb{R}^m\\), \\(x \\mapsto \\mathbf{A}x\\): Image / Column space \\[ \\mathrm{Im}(\\Phi) = \\mathrm{span}\\{\\mathbf{a}_1, \\dots, \\mathbf{a}_n\\} \\subseteq \\mathbb{R}^m \\] where \\(\\mathbf{a}_i\\) are the columns of \\(\\mathbf{A}\\). Kernel / Null space \\[ \\ker(\\Phi) = \\{ \\mathbf{x} \\in \\mathbb{R}^n : \\mathbf{A}\\mathbf{x} = \\mathbf{0} \\} \\subseteq \\mathbb{R}^n \\] represents all linear combinations of columns that yield zero. Example 2.58 Consider \\(\\Phi : \\mathbb{R}^4 \\to \\mathbb{R}^2\\) with \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; -1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}, \\quad \\Phi(\\mathbf{x}) = \\mathbf{A}\\mathbf{x} \\] Image: \\[ \\mathrm{Im}(\\Phi) = \\mathrm{span}\\left\\{ \\begin{bmatrix}1\\\\1\\end{bmatrix}, \\begin{bmatrix}2\\\\0\\end{bmatrix}, \\begin{bmatrix}-1\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\end{bmatrix} \\right\\} \\] Kernel: \\[ \\ker(\\Phi) = \\mathrm{span}\\left\\{ \\begin{bmatrix}0\\\\1/2\\\\1\\\\0\\end{bmatrix}, \\begin{bmatrix}-1\\\\1/2\\\\0\\\\1\\end{bmatrix} \\right\\} \\] The theorem below is known as the Rank-Nullity Theorem. Theorem 2.5 For \\(\\Phi : V \\to W\\): \\[ \\dim(\\ker(\\Phi)) + \\dim(\\mathrm{Im}(\\Phi)) = \\dim(V) \\] Corollary 2.1 For \\(\\Phi : V \\to W\\), the following two facts are true: If \\(\\dim(\\mathrm{Im}(\\Phi)) &lt; \\dim(V)\\), then \\(\\ker(\\Phi)\\) is non-trivial (\\(\\dim(\\ker(\\Phi)) \\ge 1\\)). If \\(\\dim(V) = \\dim(W)\\), then \\[ \\Phi \\text{ injective } \\iff \\Phi \\text{ surjective } \\iff \\Phi \\text{ bijective.} \\] Exercises Exercise 2.74 Consider \\(\\Phi : \\mathbb{R}^4 \\to \\mathbb{R}^2\\) with \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; -1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}, \\quad \\Phi(\\mathbf{x}) = \\mathbf{A}\\mathbf{x} \\] Verify that Image: \\[ \\mathrm{Im}(\\Phi) = \\mathrm{span}\\left\\{ \\begin{bmatrix}1\\\\1\\end{bmatrix}, \\begin{bmatrix}2\\\\0\\end{bmatrix}, \\begin{bmatrix}-1\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\end{bmatrix} \\right\\} \\] Kernel: \\[ \\ker(\\Phi) = \\mathrm{span}\\left\\{ \\begin{bmatrix}0\\\\1/2\\\\1\\\\0\\end{bmatrix}, \\begin{bmatrix}-1\\\\1/2\\\\0\\\\1\\end{bmatrix} \\right\\} \\] Solution Exercise 2.75 Prove that for \\(\\Phi : V \\to W\\), the following two facts are true: If \\(\\dim(\\mathrm{Im}(\\Phi)) &lt; \\dim(V)\\), then \\(\\ker(\\Phi)\\) is non-trivial (\\(\\dim(\\ker(\\Phi)) \\ge 1\\)). If \\(\\dim(V) = \\dim(W)\\), then \\[ \\Phi \\text{ injective } \\iff \\Phi \\text{ surjective } \\iff \\Phi \\text{ bijective.} \\] Solution Exercise 2.76 Prove the Rank-Nullity Theorem. Solution Exercise 2.77 Let \\(f: \\bbR \\rightarrow \\bbR\\) be defined as \\(f(x) = x^3\\). Show that \\(f\\) is bijective. Solution Exercise 2.78 Let \\(f: \\bbR \\rightarrow \\bbR\\) be defined as \\(f(x) = x^2\\). Show that \\(f\\) is not bijective. Solution Exercise 2.79 Let \\(f: \\bbR^+ \\rightarrow \\bbR^+\\) be defined as \\(f(x) = \\sqrt{x}\\). Show that \\(f\\) is bijective. Solution Exercise 2.80 Show that there is an isomorphism between \\(S = \\set{1,2,3,4,5}\\) and \\(\\bbZ_5 = \\set{0,1,2,3,4}\\). Solution Exercise 2.81 Write the coordinate \\(\\begin{bmatrix}2\\\\3\\end{bmatrix}\\) in terms of the standard basis vectors in \\(\\bbR^2\\). Then write it in terms of the basis \\(\\bb_1=\\begin{bmatrix}1\\\\-1\\end{bmatrix}\\) and \\(\\bb_2=\\begin{bmatrix}1\\\\1\\end{bmatrix}\\). Solution Exercise 2.82 Write the coordinate \\(\\begin{bmatrix}4\\\\-1\\end{bmatrix}\\) in terms of the standard basis vectors in \\(\\bbR^2\\). Then write it in terms of the basis \\(\\bb_1=\\begin{bmatrix}0\\\\-1\\end{bmatrix}\\) and \\(\\bb_2=\\begin{bmatrix}-1\\\\0\\end{bmatrix}\\). Solution Exercise 2.83 On graph paper, create a grid with a standard basis. Then, in a different colour, create a grid using the basis \\(\\bb_1 = \\begin{bmatrix}2\\\\1\\end{bmatrix}\\) and \\(\\bb_2 = \\begin{bmatrix}-1\\\\1\\end{bmatrix}\\). If \\(\\ba = \\begin{bmatrix}1\\\\3\\end{bmatrix}\\) is a vector in the second basis, what are the coordinates of that vector in the standard basis. Draw the vector on the grid. Solution Exercise 2.84 Prove that if \\(V\\) is a vector space with basis \\(\\{\\bb_1, \\ldots, \\bb_n\\}\\), then every vector \\(\\mathbf{v} \\in V\\) can be written uniquely as a linear combination of \\(\\bb_1, \\ldots , \\bb_n\\). Solution Exercise 2.85 Find the change of basis matrix from \\(B\\) to \\(C\\) and from \\(C\\) to \\(B\\). Show that they are inverses of each other. \\ Here, \\(V = \\bbR^2\\); \\(B = \\set{\\begin{bmatrix}9\\\\2\\end{bmatrix}, \\begin{bmatrix}4\\\\3\\end{bmatrix}}\\); \\(C = \\set{\\begin{bmatrix}2\\\\1\\end{bmatrix}, \\begin{bmatrix}3\\\\1\\end{bmatrix}}\\). Solution Exercise 2.86 Find the change of basis matrix from \\(B\\) to \\(C\\) and from \\(C\\) to \\(B\\). Show that they are inverses of each other. \\ Here, \\(V = \\bbR^3\\); \\(B=\\set{\\begin{bmatrix}2\\\\5\\\\0\\end{bmatrix},\\begin{bmatrix}3\\\\0\\\\5\\end{bmatrix}, \\begin{bmatrix}8\\\\2\\\\9\\end{bmatrix}}; C=\\set{\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}, \\begin{bmatrix}2\\\\0\\\\1\\end{bmatrix},\\begin{bmatrix}0\\\\1\\\\3\\end{bmatrix}}\\). Solution Exercise 2.87 Define \\(\\Phi(\\bA) = \\bA + \\bA^T\\). What is the \\(\\ker(\\Phi)\\)? Solution Exercise 2.88 Define \\(\\Phi(\\bA) = \\begin{bmatrix}0&amp;1\\\\0&amp;0 \\end{bmatrix}\\bA\\). What is the \\(\\ker(\\Phi)\\)? Solution Exercise 2.89 Define \\(\\Phi: \\bbR^3 \\rightarrow \\bbR^4\\) given by \\(\\Phi\\left(\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix} \\right) = \\begin{bmatrix}x\\\\x\\\\y\\\\y\\end{bmatrix}\\). What is the \\(\\ker(\\Phi)\\)? Solution Exercise 2.90 Define \\(\\Phi: \\bbR^{2 \\times 2} \\rightarrow \\bbR^{2 \\times 2}\\) given by \\(\\Phi\\left(\\begin{bmatrix}a &amp; b \\\\ c &amp; d\\end{bmatrix}\\right) = \\begin{bmatrix}a+b &amp; b+c \\\\ c+d &amp; d + a \\end{bmatrix}\\). What is the \\(\\ker(\\Phi)\\)? Solution Exercise 2.91 he matrix \\(\\bM = \\begin{bmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{bmatrix}\\) rotates a vector clockwise 90 degrees. Determine the matrix that rotates a vector 90 degrees clockwise in the basis \\(\\bB = \\set{\\begin{bmatrix}1\\\\3 \\end{bmatrix}, \\begin{bmatrix}2\\\\-1 \\end{bmatrix}}\\). Solution Exercise 2.92 Suppose Alice has standard basis vectors \\(e_1\\) and \\(e_2\\). Let Bob have basis vectors given by \\(\\bb_1 = \\begin{bmatrix}1\\\\2 \\end{bmatrix}\\) and \\(\\bb_2 = \\begin{bmatrix} -1\\\\1 \\end{bmatrix}\\). Solution Exercise 2.93 Let \\(X\\) be a random variable with PDF given by \\[f_X(x)= \\begin{cases} cx^2 &amp; |x| \\leq 1\\\\0 &amp; otherwise \\end{cases}.\\] Solution Exercise 2.94 Let \\(X\\) be a positive continuous random variable. Prove that \\(E[X] = \\int_{0}^{\\infty} P(X \\geq x) dx\\). Solution Exercise 2.95 Show that \\(cov[x,y] = E[xy] -E[x]E[y]\\). Solution "],["gaussian-distribution.html", "2.8 Gaussian Distribution", " 2.8 Gaussian Distribution The Gaussian (or normal) distribution is one of the most fundamental probability distributions for continuous-valued random variables. Its importance arises from its computational convenience and its natural appearance in many real-world and theoretical contexts  most notably due to the Central Limit Theorem, which states that the sum of many independent and identically distributed random variables tends to a Gaussian distribution. The Gaussian distribution plays a central role in machine learning, forming the foundation of many machine learning ideas such as linear regression (as the likelihood and prior), mixture models (for density estimation), Gaussian processes, etc. Definition 2.36 For a scalar random variable \\(x\\), the Gaussian (normal) probability density function (pdf) is defined as \\[ p(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right), \\] where: \\(\\mu\\) is the mean (location parameter), \\(\\sigma^2\\) is the variance (spread or scale parameter). Example 2.59 Consider a random variable \\(X\\) representing the heights (in cm) of a population of adults, modeled as: \\[ X \\sim \\mathcal{N}(\\mu = 170, \\sigma^2 = 25) \\] so that the mean height is 170 cm and the standard deviation is \\(\\sigma = 5\\) cm. The probability density function is: \\[ p(x \\mid 170, 25) = \\frac{1}{\\sqrt{2\\pi \\cdot 25}} \\exp\\left( -\\frac{(x - 170)^2}{2 \\cdot 25} \\right) = \\frac{1}{5\\sqrt{2\\pi}} \\exp\\left( -\\frac{(x - 170)^2}{50} \\right). \\] Find the density at \\(x = 175\\) cm: \\[ p(175) = \\frac{1}{5\\sqrt{2\\pi}} \\exp\\left( -\\frac{(175 - 170)^2}{50} \\right) = \\frac{1}{5\\sqrt{2\\pi}} \\exp\\left( -\\frac{25}{50} \\right) = \\frac{1}{5\\sqrt{2\\pi}} \\exp(-0.5) \\approx 0.048. \\] This tells us that a height of 175 cm has a density of approximately 0.048 under this normal distribution. This is simply the height of the curve - it is not the probability of someone having a height of 175cm. Definition 2.37 For a random vector \\(\\mathbf{x} \\in \\mathbb{R}^D\\), the multivariate Gaussian distribution is given by: \\[ p(\\mathbf{x} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = (2\\pi)^{-D/2} |\\boldsymbol{\\Sigma}|^{-1/2} \\exp\\!\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right), \\] where: \\(\\boldsymbol{\\mu}\\) is the mean vector , and \\(\\boldsymbol{\\Sigma}\\) is the covariance matrix. Example 2.60 Consider a 2-dimensional random vector \\[ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\] representing the height (in cm) and weight (in kg) of a person. Suppose the data is modeled as a 2D Gaussian with \\[ \\boldsymbol{\\mu} = \\begin{bmatrix} 170 \\\\ 65 \\end{bmatrix}, \\quad \\boldsymbol{\\Sigma} = \\begin{bmatrix} 25 &amp; 10 \\\\ 10 &amp; 16 \\end{bmatrix}. \\] Then the probability density function is: \\[ p(\\mathbf{x} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{2 \\pi \\sqrt{|\\boldsymbol{\\Sigma}|}} \\exp\\!\\Bigg( -\\frac{1}{2} (\\mathbf{x}-\\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}) \\Bigg). \\] Let \\(\\mathbf{x} = \\begin{bmatrix} 175 \\\\ 70 \\end{bmatrix}\\). Then the difference from the mean is \\[ \\mathbf{x} - \\boldsymbol{\\mu} = \\begin{bmatrix} 5 \\\\ 5 \\end{bmatrix}. \\] Compute the Mahalanobis term: \\[ (\\mathbf{x}-\\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}) \\approx 2.0 \\] and the determinant \\[ |\\boldsymbol{\\Sigma}| = 25\\cdot16 - 10^2 = 300. \\] Thus, the density is \\[ p(\\mathbf{x}) \\approx \\frac{1}{2 \\pi \\sqrt{300}} \\exp(-1) \\approx 0.0042. \\] This gives the height of the Gaussian curve at the vector \\([175, 70]^\\top\\). 2.8.1 Joint, Marginal, and Conditional Gaussians Lemma 2.9 Consider a joint Gaussian over concatenated variables: \\[ p(\\mathbf{x}, \\mathbf{y}) = \\mathcal{N}\\!\\left( \\begin{bmatrix} \\boldsymbol{\\mu}_x \\\\[4pt] \\boldsymbol{\\mu}_y \\end{bmatrix}, \\begin{bmatrix} \\boldsymbol{\\Sigma}_{xx} &amp; \\boldsymbol{\\Sigma}_{xy} \\\\[4pt] \\boldsymbol{\\Sigma}_{yx} &amp; \\boldsymbol{\\Sigma}_{yy} \\end{bmatrix} \\right). \\] Then: The marginals \\(p(\\mathbf{x})\\) and \\(p(\\mathbf{y})\\) are Gaussian. The conditional distribution \\(p(\\mathbf{x} \\mid \\mathbf{y})\\) is also Gaussian, with mean and covariance derived as: \\[ \\begin{aligned} \\boldsymbol{\\mu}_{x|y} &amp;= \\boldsymbol{\\mu}_x + \\boldsymbol{\\Sigma}_{xy} \\boldsymbol{\\Sigma}_{yy}^{-1} (\\mathbf{y} - \\boldsymbol{\\mu}_y), \\\\ \\boldsymbol{\\Sigma}_{x|y} &amp;= \\boldsymbol{\\Sigma}_{xx} - \\boldsymbol{\\Sigma}_{xy} \\boldsymbol{\\Sigma}_{yy}^{-1} \\boldsymbol{\\Sigma}_{yx}. \\end{aligned} \\] Example 2.61 Consider two random variables \\(\\mathbf{x} \\in \\mathbb{R}\\) and \\(\\mathbf{y} \\in \\mathbb{R}\\) with the joint Gaussian distribution: \\[ \\begin{bmatrix} \\mathbf{x} \\\\ \\mathbf{y} \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 3 \\end{bmatrix} \\right). \\] The marginal of \\(\\mathbf{x}\\) is Gaussian: \\[ p(\\mathbf{x}) \\= \\mathcal{N}(\\mu_x, \\Sigma_{xx}) = \\mathcal{N}(1, 2) \\] The marginal of \\(\\mathbf{y}\\) is Gaussian: \\[ p(\\mathbf{y}) \\= \\mathcal{N}(\\mu_y, \\Sigma_{yy}) = \\mathcal{N}(2, 3) \\] Suppose we observe \\(p(\\mathbf{y}) = 3\\). Then the conditional distribution \\(p(\\mathbf{x} \\mid \\mathbf{y}=3)\\) is Gaussian with: \\[ \\mu_{x|y} = \\mu_x + \\Sigma_{xy} \\Sigma_{yy}^{-1} (y - \\mu_y) = 1 + 1 \\cdot 3^{-1} \\cdot (3 - 2) = 1 + \\frac{1}{3} \\approx 1.333 \\] \\[ \\Sigma_{x|y} = \\Sigma_{xx} - \\Sigma_{xy} \\Sigma_{yy}^{-1} \\Sigma_{yx} = 2 - 1 \\cdot 3^{-1} \\cdot 1 = 2 - \\frac{1}{3} \\approx 1.667 \\] Thus, the conditional distribution is: \\[ p(\\mathbf{x} \\mid \\mathbf{y}=3) = \\mathcal{N}(1.333, 1.667) \\] 2.8.2 Product of Gaussian Densities Lemma 2.10 The product of two Gaussian densities is proportional to another Gaussian. This property is essential for Bayesian inference, where the posterior distribution is obtained by multiplying the likelihood and prior, both often modeled as Gaussians. Example 2.62 If \\(\\mathcal{N}(\\mathbf{x}|\\mathbf{a}, \\mathbf{A})\\) and \\(\\mathcal{N}(\\mathbf{x}|\\mathbf{b}, \\mathbf{B})\\) are the two Gaussians, their product is a Gaussian of the form \\(\\mathcal{N}(\\mathbf{x}|\\mathbf{c}, \\mathbf{C})\\) where \\(\\mathbf{C} = \\left( \\mathbf{A}^{-1} + \\mathbf{B}^{-1} \\right)^{-1}\\) \\(\\mathbf{c} = \\mathbf{C}\\left( \\mathbf{A}^{-1}\\mathbf{a} + \\mathbf{B}^{-1}\\mathbf{b} \\right)\\) \\(c = (2\\pi)^{-D/2} |\\mathbf{A} + \\mathbf{B}|^{-1/2} exp(-1/2 (\\mathbf{a} - \\mathbf{b})^T(\\mathbf{A} + \\mathbf{B})^{-1}(\\mathbf{a} - \\mathbf{b}))\\). 2.8.3 Mixtures of Gaussians A mixture of Gaussians combines multiple Gaussian components to form a more flexible distribution: \\[ p(x) = \\alpha p_1(x) + (1 - \\alpha)p_2(x), \\] where \\(0 &lt; \\alpha &lt; 1\\) is the mixture weight. Lemma 2.11 Let \\[ p(x) = \\alpha p_1(x) + (1 - \\alpha)p_2(x), \\] where \\(0 &lt; \\alpha &lt; 1\\). If \\(p_1(x) = \\mathcal{N}(\\mu_1, \\sigma_1^2)\\) and \\(p_2(x) = \\mathcal{N}(\\mu_2, \\sigma_2^2)\\), then: \\[ \\begin{aligned} E[x] &amp;= \\alpha \\mu_1 + (1 - \\alpha)\\mu_2, \\\\ V[x] &amp;= \\alpha \\sigma_1^2 + (1 - \\alpha)\\sigma_2^2 + \\alpha(1 - \\alpha)(\\mu_1 - \\mu_2)^2. \\end{aligned} \\] This expression illustrates the law of total variance: \\[ \\mathrm{Var}(X) = E_Y[\\mathrm{Var}(X \\mid Y)] + \\mathrm{Var}_Y(E[X \\mid Y]). \\] 2.8.4 Linear and Affine Transformations of Gaussians If \\(\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\) and \\(\\mathbf{Y} = \\mathbf{A}\\mathbf{X} + \\boldsymbol{\\mu}\\), then \\[ \\mathbf{Y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{A}\\mathbf{A}^\\top). \\] Hence, any linear or affine transformation of a Gaussian random variable is also Gaussian. This property is fundamental in probabilistic modeling, regression, and state estimation. Exercises Exercise 2.96 Derive the formula for the product of Gaussians. That is, prove \\[\\mathcal{N}(\\mathbf{x}|\\mathbf{a},\\mathbf{a})\\mathcal{N}(\\mathbf{x}|\\mathbf{B},\\mathbf{B}) = c\\mathcal{N}(\\mathbf{x},\\mathbf{c},\\mathbf{C}),\\] showing the values of \\(c, \\mathbb{c}\\) and \\(\\mathbb{C}\\). Solution Exercise 2.97 Select any two integers \\(a\\) and \\(b\\). What is \\(\\alpha a + (1-\\alpha)b\\), where \\(0 \\leq \\alpha \\leq 1\\). Select any \\(a,b \\in \\mathbb{R}^2\\). What is \\(\\alpha a + (1-\\alpha)b\\), where \\(0 \\leq \\alpha \\leq 1\\). Generalize the results from the last 2 parts to \\(a,b \\in \\mathbb{R}^n\\). Justify your answer. Solution Exercise 2.98 Suppose we have a full rank matrix \\(\\mathbf{a} \\in \\mathbb{R}^{M \\times N}\\), where \\(M \\geq N\\) and \\(\\mathbb{y} \\in \\mathbb{R}^M\\) is a Gaussian random variable with mean \\(\\mathbf{a}\\mathbf{x}\\), i.e., \\[p(\\mathbf{y}) = \\mathcal{N}(\\mathbf{y}|\\mathbf{a}\\mathbf{x}, \\mathbf{\\Sigma}).\\] If \\(\\mathbf{a}\\) is invertible, find \\(p(\\mathbf{x})\\). Solution Exercise 2.99 Justify the following statement for a full rank matrix \\(\\mathbf{a} \\in \\mathbb{R}^{M \\times N}\\), where \\(M \\geq N\\), \\(\\mathbb{y} \\in \\mathbb{R}^M\\) and \\(\\mathbf{x} \\in \\mathbb{R}^N\\): \\[\\mathbb{y} = \\mathbf{a}\\mathbf{x} \\Longleftrightarrow \\left(\\mathbf{a}^T\\mathbf{a}\\right)^{-1}\\mathbf{a}^T\\mathbb{y} = \\mathbf{x}.\\] Be careful to justify all of your steps. Solution "],["continuous-optimization.html", "Chapter 3 Continuous Optimization", " Chapter 3 Continuous Optimization Machine learning training often involves finding the best set of parameters that minimize an objective function, which measures how well a model fits the data. This search for optimal parameters is formulated as a continuous optimization problem, since the parameters typically take values in \\(\\mathbb{R}^D\\). Optimization problems can be divided into two main types: Unconstrained optimization, where parameters can take any value in \\(\\mathbb{R}^D\\). Constrained optimization, where parameters must satisfy specific restrictions. Most objective functions in machine learning are differentiable, allowing the use of gradients to find minima. The gradient points in the direction of steepest ascent, so algorithms typically move in the opposite direction (downhill) to minimize the function. The step size determines how far to move along this direction. The goal is to find points where the gradient is zero  these are stationary points. A stationary point can be: A local minimum (second derivative \\(&gt; 0\\)), A local maximum (second derivative \\(&lt; 0\\)), or A saddle point. An example function \\[ \\ell(x) = x^4 + 7x^3 + 5x^2 - 17x + 3 \\] demonstrates multiple stationary points  two minima and one maximum  found by setting the derivative \\[ \\frac{d\\ell(x)}{dx} = 4x^3 + 21x^2 + 10x - 17 = 0 \\] to zero and analyzing the sign of the second derivative. In practice, analytical solutions are often not feasible, especially for high-degree polynomials (by the AbelRuffini theorem, equations of degree 5 cannot be solved algebraically). Instead, we rely on numerical optimization algorithms that iteratively follow the negative gradient. A key concept introduced later in the chapter is convex optimization. For convex functions, every local minimum is also a global minimum, which guarantees that gradient-based methods will converge to the optimal solution regardless of the starting point. Many machine learning objectives are designed to be convex for this reason. While the examples here are one-dimensional, the same principles extend to multidimensional optimization, though visualization becomes more challenging. Gradients, step directions, and curvature generalize to higher-dimensional parameter spaces. "],["optimization-using-gradient-descent.html", "3.1 Optimization Using Gradient Descent", " 3.1 Optimization Using Gradient Descent Optimization methods are central to training machine learning models. In this section, we study gradient-based optimization, where we iteratively improve parameters by following the negative gradient of an objective function. Gradient descent is a first-order optimization algorithm used to find a local minimum of a differentiable function \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\). Definition 3.1 The gradient descent algorithm begins with an initial guess \\(\\mathbf{x}_0\\). At each iteration, parameters are updated as: \\[ \\mathbf{x}_{i+1} = \\mathbf{x}_i - \\gamma_i (\\nabla f(\\mathbf{x}_i))^\\top , \\] where \\(\\gamma_i\\) is the step-size (or learning rate). The gradient points in the direction of steepest ascent, so moving in the opposite direction decreases the function value. Example 3.1 Gradient Descent on a Simple Quadratic Function Consider the quadratic function \\[ f(x) = (x - 3)^2. \\] This function has a global minimum at \\(x = 3\\). The derivative (gradient) of \\(f\\) is \\[ \\nabla f(x) = f&#39;(x) = 2(x - 3). \\] Gradient descent updates the variable \\(x\\) according to \\[ x_{k+1} = x_k - \\gamma \\nabla f(x_k), \\] where \\(\\gamma &gt; 0\\) is the learning rate. Substituting the gradient: \\[ x_{k+1} = x_k - \\gamma \\, 2(x_k - 3). \\] Let the initial value be \\(x_0 = 0\\) and choose \\(\\gamma = 0.1\\). \\[ x_1 = 0 - 0.1 \\cdot 2(0 - 3) = 0.6 \\] \\[ x_2 = 0.6 - 0.1 \\cdot 2(0.6 - 3) = 1.08 \\] \\[ x_3 = 1.08 - 0.1 \\cdot 2(1.08 - 3) = 1.464 \\] Each step moves \\(x_k\\) closer to the minimizer \\(x = 3\\). The gradient points in the direction of steepest increase. Gradient descent moves in the opposite direction, reducing the function value. Because \\(f(x)\\) is convex, gradient descent converges to the global minimum. A simple example with a quadratic function demonstrates how iterative updates move toward the minimum value. However, if the problem is poorly conditioned (e.g., shaped like a long narrow valley), convergence becomes slow and oscillatory. Example 3.2 Gradient Descent on a Two-Variable Quadratic Function Consider the function \\[ f(x, y) = (x - 2)^2 + (y + 1)^2. \\] This is a convex quadratic function with a global minimum at \\[ (x^\\ast, y^\\ast) = (2, -1). \\] The gradient of \\(f\\) is \\[ \\nabla f(x, y) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} 2(x - 2) \\\\ 2(y + 1) \\end{bmatrix}. \\] Gradient descent updates the variables according to \\[ \\begin{bmatrix} x_{k+1} \\\\ y_{k+1} \\end{bmatrix} = \\begin{bmatrix} x_k \\\\ y_k \\end{bmatrix} - \\gamma \\begin{bmatrix} 2(x_k - 2) \\\\ 2(y_k + 1) \\end{bmatrix}, \\] where \\(\\gamma &gt; 0\\) is the learning rate. Let the initial point be \\[ (x_0, y_0) = (0, 0), \\] and choose \\(\\gamma = 0.1\\). Iteration 1 \\[ \\begin{aligned} x_1 &amp;= 0 - 0.1 \\cdot 2(0 - 2) = 0.4, \\\\ y_1 &amp;= 0 - 0.1 \\cdot 2(0 + 1) = -0.2. \\end{aligned} \\] Iteration 2 \\[ \\begin{aligned} x_2 &amp;= 0.4 - 0.1 \\cdot 2(0.4 - 2) = 0.72, \\\\ y_2 &amp;= -0.2 - 0.1 \\cdot 2(-0.2 + 1) = -0.36. \\end{aligned} \\] Iteration 3 \\[ \\begin{aligned} x_3 &amp;= 0.72 - 0.1 \\cdot 2(0.72 - 2) = 0.976, \\\\ y_3 &amp;= -0.36 - 0.1 \\cdot 2(-0.36 + 1) = -0.488. \\end{aligned} \\] The iterates move steadily toward \\((2, -1)\\). The level sets of \\(f(x,y)\\) are concentric circles centered at \\((2,-1)\\). The gradient points perpendicular to level curves in the direction of steepest increase. Gradient descent moves opposite the gradient, descending toward the minimum. 3.1.1 Step-size Selection Choosing an appropriate step-size \\(\\gamma\\) is crucial: Too small  slow convergence Too large  overshooting or divergence Adaptive methods adjust the step-size based on local function behavior: If \\(f(x_{i+1}) &gt; f(x_i)\\), the step was too large  reduce \\(\\gamma\\) If \\(f(x_{i+1}) &lt; f(x_i)\\), the step could be larger  increase \\(\\gamma\\) This ensures monotonic convergence. Monotonic convergence refers to a type of convergence where a sequence approaches its limit without oscillating, meaning it moves consistently in one direction (either always increasing or always decreasing) toward the limit. Definition 3.2 A sequence \\(\\{x_n\\}\\) converges monotonically to \\(x^\\ast\\) if it satisfies both: \\(x_n \\to x^\\ast\\) as \\(n \\to \\infty\\), and The sequence is monotonic, meaning either: \\(x_{n+1} \\le x_n\\) for all \\(n\\) (monotonic decreasing), or \\(x_{n+1} \\ge x_n\\) for all \\(n\\) (monotonic increasing). Example 3.3 Solving \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) can be formulated as minimizing: \\[ \\|\\mathbf{A}\\mathbf{x} - \\mathbf{b}\\|^2 = (\\mathbf{A}\\mathbf{x} - \\mathbf{b})^\\top (\\mathbf{A}\\mathbf{x} - \\mathbf{b}). \\] The gradient is \\[ \\nabla_\\mathbf{x} = 2(\\mathbf{A}\\mathbf{x} - \\mathbf{b})^\\top \\mathbf{A} .\\] Although this problem has a closed-form solution, gradient descent provides an iterative alternative. The convergence rate depends on the condition number \\(\\kappa = \\frac{\\sigma_{\\max}(\\mathbf{A})}{\\sigma_{\\min}(\\mathbf{A})}\\) (here, \\(\\sigma_{max}\\) and \\(\\sigma_{min}\\) refer to the maximum and minimum singular values of \\(\\mathbf{A}\\)). You can imagine a poorly conditioned problem as one similar to trying to find the minimum value of a slightly curved dinner plate versus a well conditioned problem as trying to find the minimum of an ice-cream cone. Poorly conditioned problems have small condition number while better conditioned ones have large condition numbers. Preconditioning (using a matrix \\(\\mathbf{P}^{-1}\\) or \\(\\mathbf{P}\\) in some texts) can improve convergence. The idea of preconditioning is essentially solving \\[\\mathbf{P}^{-1}\\left(\\mathbf{A}\\mathbf{x} - \\mathbf{b} \\right) = \\mathbf{0}\\] instead of the original. Example 3.4 Preconditioning Gradient Descent Consider the quadratic function \\[ f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^\\top \\mathbf{A}\\mathbf{x}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}, \\] where \\[ \\mathbf{A} = \\begin{bmatrix} 10 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}. \\] This function is strongly anisotropic: it curves much more steeply in the \\(x_1\\)-direction than in the \\(x_2\\)-direction. The gradient is \\[ \\nabla f(\\mathbf{x}) = \\mathbf{A}\\mathbf{x} = \\begin{bmatrix} 10x_1 \\\\ x_2 \\end{bmatrix}. \\] Standard gradient descent updates: \\[ \\mathbf{x}_{k+1} = \\mathbf{x}_k - \\gamma \\nabla f(\\mathbf{x}_k). \\] Because of the large eigenvalue \\(10\\), the method must take small steps to avoid overshooting, leading to slow convergence. We choose a preconditioning matrix \\[ \\mathbf{P} = \\begin{bmatrix} \\frac{1}{10} &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}. \\] The preconditioned gradient descent update becomes \\[ \\mathbf{x}_{k+1} = \\mathbf{x}_k - \\gamma \\mathbf{P}\\nabla f(\\mathbf{x}_k). \\] The impact of this is the new preconditioned gradient: \\[ \\mathbf{P}\\nabla f(\\mathbf{x}) = \\begin{bmatrix} \\frac{1}{10} &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 10x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}. \\] Now the update rule is \\[ \\mathbf{x}_{k+1} = \\mathbf{x}_k - \\gamma \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}. \\] Both components are scaled equally, eliminating the imbalance in curvature. Without preconditioning: level sets are elongated ellipses, causing zig-zag motion. With preconditioning: the problem behaves like \\[ f(\\mathbf{x}) = \\tfrac{1}{2}(x_1^2 + x_2^2), \\] which has circular level sets. Convergence is faster and more stable. 3.1.2 Gradient Descent with Momentum Gradient descent with momentum adds a memory term to smooth updates and dampen oscillations: \\[ \\mathbf{x}_{i+1} = \\mathbf{x}_i - \\gamma_i (\\nabla f(\\mathbf{x}_i))^\\top + \\alpha \\Delta \\mathbf{x}_i \\] where \\(\\Delta \\mathbf{x}_i = \\mathbf{x}_i - \\mathbf{x}_{i-1}\\) and \\(\\alpha \\in [0,1]\\) controls the contribution of past updates. This technique behaves like a heavy ball rolling downhill, accelerating along consistent directions and reducing oscillations in narrow valleys. Momentum helps especially when gradients are noisy or the optimization landscape is highly curved. Example 3.5 Gradient Descent with Momentum Consider the onedimensional quadratic function \\[ f(x) = x^2. \\] The gradient is \\[ \\nabla f(x) = 2x. \\] Momentum introduces a velocity term \\(v_k\\): \\[ \\begin{aligned} v_{k+1} &amp;= \\beta v_k + \\nabla f(x_k), \\\\ x_{k+1} &amp;= x_k - \\gamma v_{k+1}, \\end{aligned} \\] where: \\(\\gamma &gt; 0\\) is the learning rate, \\(\\beta \\in [0,1)\\) is the momentum parameter. For example, let: \\[ x_0 = 4, \\quad v_0 = 0, \\quad \\gamma = 0.1, \\quad \\beta = 0.9. \\] Gradient at \\(x_0\\): \\[ \\nabla f(x_0) = 2(4) = 8 \\] Velocity update: \\[ v_1 = 0.9(0) + 8 = 8 \\] Position update: \\[ x_1 = 4 - 0.1(8) = 3.2. \\] Momentum accumulates past gradients, helping the iterates move faster in consistent descent directions and reducing oscillations. Gradient at \\(x_1\\): \\[ \\nabla f(x_1) = 2(3.2) = 6.4 \\] Velocity update: \\[ v_2 = 0.9(8) + 8 = 7.2 \\] Position update: \\[ x_2 = 3.2 - 0.1(7.2) = 2.48. \\] 3.1.3 Stochastic Gradient Descent (SGD) Stochastic Gradient Descent (SGD) approximates the gradient using a random subset of data. Given a loss function decomposed as a sum: \\[ L(\\mathbf{\\theta}) = \\sum_{n=1}^N L_n(\\mathbf{\\theta}), \\] SGD updates parameters using only a subset (mini-batch) of terms: \\[ \\mathbf{\\theta}_{i+1} = \\mathbf{\\theta}_i - \\gamma_i \\left( \\nabla L(\\mathbf{\\theta}_i) \\right)^\\top = \\mathbf{\\theta}_i - \\gamma_i \\sum_{n=1}^N \\left( \\nabla L_{n}(\\mathbf{\\theta}_i) \\right)^\\top. \\] This reduces computational cost and allows training on large datasets. Large mini-batches  lower variance, smoother convergence, but higher cost Small mini-batches  noisier updates, faster per-step computation, better ability to escape local minima Despite noisy gradients, SGD converges to a local minimum (almost surely) under mild conditions. It has become the default optimization method in large-scale machine learning. Example 3.6 Stochastic Gradient Descent (SGD) Consider a dataset with three data points and the loss function \\[ f(x) = \\frac{1}{3}\\sum_{i=1}^3 f_i(x), \\quad f_i(x) = (x - a_i)^2, \\] where: \\[ a_1 = 1, \\quad a_2 = 3, \\quad a_3 = 5. \\] Full Gradient (Batch Gradient Descent) \\[ \\nabla f(x) = \\frac{2}{3}\\sum_{i=1}^3 (x - a_i). \\] Stochastic Gradient Descent Update SGD uses one data point at a time: \\[ x_{k+1} = x_k - \\gamma \\nabla f_{i_k}(x_k), \\] where \\(i_k\\) is chosen randomly from \\(\\{1,2,3\\}\\). For example, let: \\[ x_0 = 0, \\quad \\gamma = 0.1, \\] and randomly choose \\(i_0 = 2\\). Gradient using only \\(f_2\\): \\[ \\nabla f_2(x_0) = 2(x_0 - 3) = -6 \\] Update: \\[ x_1 = 0 - 0.1(-6) = 0.6 \\] Here, the update is noisy, but cheap. On average, SGD moves toward the minimum. It is widely used in machine learning for large datasets. Exercises Exercise 3.1 The notes talk about a function \\[f\\left(\\begin{bmatrix}x_1\\\\x_2 \\end{bmatrix}\\right) = \\dfrac{1}{2}\\begin{bmatrix}x_1\\\\x_2 \\end{bmatrix}^T\\begin{bmatrix}2 &amp; 1 \\\\ 1 &amp; 20 \\end{bmatrix} \\begin{bmatrix}x_1\\\\x_2 \\end{bmatrix} - \\begin{bmatrix}5\\\\3 \\end{bmatrix}^T\\begin{bmatrix}x_1\\\\x_2 \\end{bmatrix}.\\] What is this function? Show that the gradient is \\[\\nabla f\\left(\\begin{bmatrix}x_1\\\\x_2 \\end{bmatrix}\\right) = \\dfrac{1}{2}\\begin{bmatrix}x_1\\\\x_2 \\end{bmatrix}^T\\begin{bmatrix}2 &amp; 1 \\\\ 1 &amp; 20 \\end{bmatrix} - \\begin{bmatrix}5\\\\3 \\end{bmatrix}^T.\\] Solution Exercise 3.2 Let \\[f(x) = \\dfrac{x^2 \\cos(x) - x}{10},\\] with \\(x_0 = 6\\) and stepsize 0.2. Use gradient descent to find the local minimum. Stop when successive terms are within 0.05 of each other. Solution Exercise 3.3 Let \\[f(x) = xe^{-x^2} + \\dfrac{x^2}{20}.\\] Run 10 iterations of gradient descent from the initial position \\(x_0 = -3\\) and step size 0.1 to try to find the minimum of \\(f\\). Solution Exercise 3.4 Let \\[f(x,y) = \\left(\\dfrac{3}{4}x - \\dfrac{3}{2} \\right)^2 + (y-2)^2 + \\dfrac{1}{4} xy.\\] Run 10 iterations of gradient descent from the initial position \\((5,4)\\) with step sizes 1, 0.1 and 0.01 to try to find the minimum of \\(f\\). Solution Exercise 3.5 Let \\[f(x,y) = e^{-x^2-y^2} + \\dfrac{x^2+y^2}{20}.\\] Run 10 iterations of gradient descent from the initial position \\((1,1)\\) with step sizes 1, 0.1 and 0.01 to try to find the minimum of \\(f\\). Solution Exercise 3.6 Use gradient descent for 5 iterations and initial guess \\(\\mathbf{x}_0 = [4,2,-1]\\) to find the minimum of \\[f(x,y,z) = (x-4)^4 + (y-3)^2 + 4(z-5)^4.\\] Solution "],["constrained-optimization-and-lagrange-multipliers.html", "3.2 Constrained Optimization and Lagrange Multipliers", " 3.2 Constrained Optimization and Lagrange Multipliers In many optimization problems, we seek to minimize a function \\(f( \\mathbf{x})\\) subject to constraints on the variables \\(\\mathbf{x}\\). Definition 3.3 A constrained optimization problem has the form: \\[ \\min_{\\mathbf{x}} f( \\mathbf{x}) \\quad \\text{subject to } g_i( \\mathbf{x}) \\le 0, \\; i = 1, \\ldots, m \\] where each \\(g_i( \\mathbf{x})\\) defines a constraint. 3.2.1 From Constraints to the Lagrangian Rather than directly enforcing the constraints, another approach is to penalize violations of the constraints using an indicator function: \\[ J( \\mathbf{x}) = f( \\mathbf{x}) + \\sum_{i=1}^{m} \\mathbf{1}(g_i(\\mathbf{x})) \\quad \\text{where} \\quad \\mathbf{1}(z) = \\begin{cases} 0 &amp; \\text{if } z \\le 0,\\\\ \\infty &amp; \\text{otherwise.} \\end{cases} \\] However, this is difficult to optimize because this indicator function is non-differentiable. To overcome this, we introduce Lagrange multipliers \\(\\mathbf{\\lambda}_i \\ge 0\\) and form the Lagrangian: \\[ L(\\mathbf{x}, \\mathbf{\\mathbf{\\lambda}}) = f(\\mathbf{x}) + \\sum_{i=1}^{m} \\mathbf{\\mathbf{\\lambda}}_i g_i(\\mathbf{x}) = f(\\mathbf{x}) + \\mathbf{\\mathbf{\\lambda}}^\\top \\mathbf{g}(\\mathbf{x}) \\] To solve a Lagrange multiplier problem, we solve \\[L(\\mathbf{x}, \\mathbf{\\lambda})=0.\\] Thats the same as solving \\(\\nabla f = \\lambda \\nabla g\\) with \\(g(\\mathbf{x}) = 0\\). Example 3.7 Find the maximum and minimum of the function \\[ f(x,y) = x^2 + y^2 \\] subject to the constraint \\[ g(x,y) = x + y - 4 = 0. \\] First, we find the gradients \\[ \\nabla f(x,y) = \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix}, \\qquad \\nabla g(x,y) = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}. \\] The Lagrange multiplier condition is \\[ \\nabla f = \\lambda \\nabla g. \\] This gives: \\[ \\begin{cases} 2x = \\lambda \\\\ 2y = \\lambda \\\\ x + y = 4 \\end{cases} \\] From the first two equations: \\[ 2x = 2y \\quad \\Rightarrow \\quad x = y. \\] Substitute into the constraint: \\[ x + x = 4 \\quad \\Rightarrow \\quad x = 2. \\] Thus: \\[ (x,y) = (2,2). \\] The minimum value is \\[ f(2,2) = 2^2 + 2^2 = 8. \\] No maximum value exists since we can make \\(x^2 + y^2\\) as large as we like when \\(x+y-4 = 0\\). We would need a restriction on the size of \\(x\\) and \\(y\\) to find both the max and min values. 3.2.1.1 Lagrangian Duality Definition 3.4 The primal problem is: \\[ \\min_x f(\\mathbf{x}) \\quad \\text{subject to } g_i(\\mathbf{x}) \\le 0 \\] Definition 3.5 The corresponding Lagrangian dual problem is: \\[ \\max_{\\mathbf{\\lambda} \\ge 0} D(\\mathbf{\\mathbf{\\lambda}}) \\quad \\text{where} \\quad D(\\mathbf{\\mathbf{\\lambda}}) = \\min_x L(\\mathbf{x}, \\mathbf{\\mathbf{\\lambda}}) \\] This dual formulation often simplifies computation since \\(D(\\mathbf{\\mathbf{\\lambda}})\\) is concave, even if \\(f\\) and \\(g_i\\) are not. Example 3.8 Consider the constrained optimization problem: \\[ \\begin{aligned} \\min_{x \\in \\mathbb{R}} \\quad &amp; f(x) = x^2 \\\\ \\text{subject to} \\quad &amp; g(x) = x - 1 \\le 0. \\end{aligned} \\] This means we want to minimize \\(x^2\\) subject to \\(x \\le 1\\). We introduce a Lagrange multiplier \\(\\lambda \\ge 0\\) for the inequality constraint: \\[ \\mathcal{L}(x, \\lambda) = x^2 + \\lambda(x - 1). \\] The dual function is obtained by minimizing the Lagrangian over \\(x\\): \\[ g(\\lambda) = \\inf_{x \\in \\mathbb{R}} \\mathcal{L}(x, \\lambda). \\] Take the derivative with respect to \\(x\\) and set it to zero: \\[ \\frac{d}{dx}(x^2 + \\lambda x - \\lambda) = 2x + \\lambda = 0 \\quad \\Rightarrow \\quad x^* = -\\frac{\\lambda}{2}. \\] Substitute back: \\[ \\begin{aligned} g(\\lambda) &amp;= \\left(-\\frac{\\lambda}{2}\\right)^2 + \\lambda\\left(-\\frac{\\lambda}{2} - 1\\right) \\\\ &amp;= \\frac{\\lambda^2}{4} - \\frac{\\lambda^2}{2} - \\lambda \\\\ &amp;= -\\frac{\\lambda^2}{4} - \\lambda. \\end{aligned} \\] The dual problem is: \\[ \\begin{aligned} \\max_{\\lambda \\ge 0} \\quad &amp; g(\\lambda) = -\\frac{\\lambda^2}{4} - \\lambda. \\end{aligned} \\] We differentiate: \\[ \\frac{d}{d\\lambda}\\left(-\\frac{\\lambda^2}{4} - \\lambda\\right) = -\\frac{\\lambda}{2} - 1, \\] and set equal to zero: \\[ -\\frac{\\lambda}{2} - 1 = 0 \\quad \\Rightarrow \\quad \\lambda^* = -2. \\] Since the dual constraint requires \\(\\lambda \\ge 0\\), the maximum occurs at the boundary: \\[ \\lambda^* = 0. \\] Thus: \\[ g(0) = 0. \\] Primal optimum: \\[ \\min_{x \\le 1} x^2 \\quad \\Rightarrow \\quad x^* = 0, \\quad f(x^*) = 0. \\] Dual optimum: \\[ \\max_{\\lambda \\ge 0} g(\\lambda) = 0. \\] Therefore, \\[ f^* = g^* = 0 \\] Example 3.9 Consider the linear program \\[ \\begin{aligned} \\text{maximize } \\quad &amp; P = 3x + 5y \\\\ \\text{subject to } \\quad &amp; 2x + y \\le 20, \\\\ &amp; x + 2y \\le 12, \\\\ &amp; x + 3y \\le 15, \\\\ &amp; x \\ge 0,\\; y \\ge 0. \\end{aligned} \\] Solve the problem, reformulate the problem using matrices, find the dual problem. Since this is a linear program in two variables, the optimum occurs at a corner point of the feasible region. Corner points: \\((0,0) \\Longrightarrow P = 0\\) \\((10,0) \\Longrightarrow P = 3(10) = 30\\) \\((0,4) \\Longrightarrow P = 5(4) = 20\\) \\((6,3) \\Longrightarrow P = 3(6) + 5(3) = 33\\) \\[ (x^*, y^*) = (6,3), \\quad P^* = 33 \\] Matrix Form of the Primal Problem Let \\[ \\mathbf{x} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\quad \\mathbf{c} = \\begin{bmatrix} 3 \\\\ 5 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} 20 \\\\ 12 \\\\ 15 \\end{bmatrix}, \\] \\[ \\mathbf{A} = \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\end{bmatrix}. \\] Then the primal problem is \\[ \\begin{aligned} \\max_{\\mathbf{x}} \\quad &amp; \\mathbf{c}^\\top \\mathbf{x} \\\\ \\text{subject to } \\quad &amp; \\mathbf{A}\\mathbf{x} \\le \\mathbf{b}, \\\\ &amp; \\mathbf{x} \\ge 0. \\end{aligned} \\] For the dual problem, we introduce Lagrange multipliers \\[ \\boldsymbol{\\lambda} = \\begin{bmatrix} \\lambda_1 \\\\ \\lambda_2 \\\\ \\lambda_3 \\end{bmatrix} \\ge 0 \\] for the inequality constraints. For a primal maximization problem with \\(\\le\\) constraints and \\(\\mathbf{x} \\ge 0\\), the dual is a minimization problem: \\[ \\begin{aligned} \\min_{\\boldsymbol{\\lambda}} \\quad &amp; \\mathbf{b}^\\top \\boldsymbol{\\lambda} = 20\\lambda_1 + 12\\lambda_2 + 15\\lambda_3 \\\\ \\text{subject to } \\quad &amp; \\mathbf{A}^\\top \\boldsymbol{\\lambda} \\ge \\mathbf{c}, \\\\ &amp; \\boldsymbol{\\lambda} \\ge 0. \\end{aligned} \\] That is, \\[ \\begin{aligned} \\min \\quad &amp; 20\\lambda_1 + 12\\lambda_2 + 15\\lambda_3 \\\\ \\text{subject to } \\quad &amp; 2\\lambda_1 + \\lambda_2 + \\lambda_3 \\ge 3, \\\\ &amp; \\lambda_1 + 2\\lambda_2 + 3\\lambda_3 \\ge 5, \\\\ &amp; \\lambda_1, \\lambda_2, \\lambda_3 \\ge 0. \\end{aligned} \\] Primal optimal value: \\(P^* = 33\\) Dual optimal value: \\(D^* = 33\\) This confirms strong duality for this linear program. 3.2.1.2 Weak Duality and the Minimax Inequality The minimax inequality is given in the following theorem. F Theorem 3.1 For any function with two arguments \\(\\phi(\\mathbf{x}, \\mathbf{y})\\), the maximin is less than the minimax. That is, \\[ \\max_{\\mathbf{y}} \\min_{\\mathbf{x}} \\phi(\\mathbf{x}, \\mathbf{y}) \\le \\min_{x}\\mathbf{x} \\max_{\\mathbf{y}} \\phi(\\mathbf{x}, \\mathbf{y}) \\] Using this, we derive weak duality. Theorem 3.2 The optimal value of the dual problem is always less than or equal to the optimal value of the primal problem: \\[ \\min_{\\mathbf{x}} \\max_{\\mathbf{\\mathbf{\\lambda}} \\ge 0} L(\\mathbf{x}, \\mathbf{\\mathbf{\\lambda}}) \\ge \\max_{\\mathbf{\\mathbf{\\lambda}} \\ge 0} \\min_{\\mathbf{x}} L(\\mathbf{x}, \\mathbf{\\mathbf{\\lambda}}) \\] Thus, duality provides a lower bound on the primal problems optimal value. 3.2.1.3 Equality Constraints If equality constraints are present, such as \\(h_j(\\mathbf{x}) = 0\\), they can be rewritten as two inequalities: \\[ h_j(\\mathbf{x}) \\le 0 \\quad \\text{and} \\quad h_j(\\mathbf{x}) \\ge 0 \\] The associated Lagrange multipliers for equality constraints are unconstrained, while those for inequality constraints remain non-negative. Exercises Exercise 3.7 Find the maximum and minimum values of \\(f\\left( {x,y} \\right) = 81{x^2} + {y^2}\\) subject to the constraint \\(4{x^2} + {y^2} = 9\\). Solution Exercise 3.8 Find the maximum and minimum values of \\(f\\left( {x,y} \\right) = 8{x^2} - 2y\\) subject to the constraint \\({x^2} + {y^2} = 1\\). Solution Exercise 3.9 Find the maximum and minimum values of \\(f\\left( {x,y,z} \\right) = {y^2} - 10z\\) subject to the constraint \\({x^2} + {y^2} + {z^2} = 36\\). Solution Exercise 3.10 Find the maximum and minimum values of \\(f\\left( {x,y,z} \\right) = xyz\\) subject to the constraint \\(x + 9{y^2} + {z^2} = 4\\). Assume \\(x \\geq 0\\) for this problem. Why is this needed? Solution "],["convex-optimization.html", "3.3 Convex Optimization", " 3.3 Convex Optimization Convex optimization focuses on a special class of optimization problems where global optimality can be guaranteed. These problems occur when the objective function is convex and the constraints define convex sets. In this setting, strong duality holds  meaning the optimal values of the primal and dual problems are equal. 3.3.1 Convex Sets and Functions Definition 3.6 A convex set \\(C\\) is one in which the line segment between any two points lies entirely within the set: \\[ \\theta x + (1 - \\theta)y \\in C, \\quad \\text{for all } x, y \\in C \\text{ and } 0 \\le \\theta \\le 1. \\] Example 3.10 A Convex Set in \\(\\mathbb{R}^2\\) Consider the set \\[ C = \\left\\{ (x,y) \\in \\mathbb{R}^2 \\;\\middle|\\; x^2 + y^2 \\le 4 \\right\\}, \\] which is the closed disk of radius 2 centered at the origin. We will demonstrate that the set is convex using two example points. A complete proof requires selecting arbitrary points in \\(C\\). Let \\[ \\mathbf{x} = (1,1), \\qquad \\mathbf{y} = (-1,0). \\] For any \\(0 \\le \\theta \\le 1\\), consider the line connection the points \\[ \\theta \\mathbf{x} + (1-\\theta)\\mathbf{y} = \\theta(1,1) + (1-\\theta)(-1,0) = (2\\theta - 1,\\; \\theta). \\] We compute the squared norm: \\[ (2\\theta - 1)^2 + \\theta^2 = 4\\theta^2 - 4\\theta + 1 + \\theta^2 = 5\\theta^2 - 4\\theta + 1. \\] For \\(0 \\le \\theta \\le 1\\), \\[ 5\\theta^2 - 4\\theta + 1 \\le 2 &lt; 4. \\] Thus, \\[ (2\\theta - 1,\\; \\theta) \\in C \\quad \\text{for all } 0 \\le \\theta \\le 1. \\] Since every convex combination of any two points in \\(C\\) remains in \\(C\\), we know that it is a convex set. Geometrically, this means every line segment between two points inside the disk stays entirely inside the disk, illustrating the definition of convexity for a set. Definition 3.7 A convex function \\(f: \\mathbb{R}^D \\to \\mathbb{R}\\) satisfies: \\[ f(\\theta x + (1 - \\theta)y) \\le \\theta f(x) + (1 - \\theta) f(y), \\] which means a straight line between two points on the function lies above the function. Example 3.11 A Convex Function in \\(\\mathbb{R}^2\\) Consider the function \\[ f(x, y) = x^2 + y^2, \\] defined on \\(\\mathbb{R}^2\\). This function is a paraboloid, and we will verify with an example that it satisfies the definition of convexity. A more general proof would use arbitrary points in \\(\\mathbb{R}^2\\). Let \\[ \\mathbf{x} = (1, 2), \\qquad \\mathbf{y} = (-1, 0). \\] Then \\[ f(\\mathbf{x}) = 1^2 + 2^2 = 5, \\qquad f(\\mathbf{y}) = (-1)^2 + 0^2 = 1. \\] For \\(0 \\le \\theta \\le 1\\), \\[ \\theta \\mathbf{x} + (1-\\theta)\\mathbf{y} = \\big(2\\theta - 1,\\; 2\\theta\\big). \\] At this point, the function evaluates to \\[ \\begin{aligned} f(\\theta \\mathbf{x} + (1-\\theta)\\mathbf{y}) &amp;= (2\\theta - 1)^2 + (2\\theta)^2 \\\\ &amp;= 4\\theta^2 - 4\\theta + 1 + 4\\theta^2 \\\\ &amp;= 8\\theta^2 - 4\\theta + 1. \\end{aligned} \\] The weighted average of the function values is \\[ \\theta f(\\mathbf{x}) + (1-\\theta)f(\\mathbf{y}) = \\theta(5) + (1-\\theta)(1) = 4\\theta + 1. \\] We compare: \\[ f(\\theta \\mathbf{x} + (1-\\theta)\\mathbf{y}) \\le \\theta f(\\mathbf{x}) + (1-\\theta)f(\\mathbf{y}) \\] This becomes: \\[ 8\\theta^2 - 4\\theta + 1 \\le 4\\theta + 1, \\] or equivalently, \\[ 8\\theta^2 - 8\\theta \\le 0 \\quad \\Longleftrightarrow \\quad 8\\theta(\\theta - 1) \\le 0. \\] This inequality holds for all \\(0 \\le \\theta \\le 1\\). So, \\(f\\) is a convex function. Geometrically, this confirms that the straight line connecting two points on the surface of the paraboloid lies above the surface itself, exactly matching the definition of a convex function. A concave function is simply the negative of a convex function. Convexity can also be checked using gradients or Hessians: If \\(f\\) is differentiable, it is convex if \\[ f(y) \\ge f(x) + \\nabla f(x)^{\\top}(y - x). \\] If \\(f\\) is twice differentiable, convexity holds when \\[ \\nabla^2 f(x) \\text{ is positive semidefinite for all } x. \\] The epigraph of a convex function  the region lying above its graph  is itself a convex set. Example 3.12 Negative entropy: \\(f(x) = x \\log_2 x\\) is convex for \\(x &gt; 0\\). Example 3.13 Closure property: A nonnegative weighted sum of convex functions is convex. For \\(\\alpha, \\beta \\ge 0\\), if \\(f_1\\) and \\(f_2\\) are convex, then \\[ \\alpha f_1(x) + \\beta f_2(x) \\text{ is also convex.} \\] This is related to Jensens inequality. 3.3.2 General Convex Optimization Problem Definition 3.8 A convex optimization problem has the form: \\[ \\begin{aligned} \\min_x \\quad &amp; f(x) \\\\ \\text{subject to} \\quad &amp; g_i(x) \\le 0, \\quad i = 1, \\ldots, m, \\\\ &amp; h_j(x) = 0, \\quad j = 1, \\ldots, n, \\end{aligned} \\] where \\(f\\) and all \\(g_i\\) are convex functions, and \\(h_j\\) define affine (convex) sets. Example 3.14 Minimize the function \\[ f(x, y) = x^2 + y^2 - 4x - 6y \\] subject to the constraint \\[ x + y = 4. \\] The objective function \\(f(x,y)\\) is convex because it is a quadratic function with a positive definite Hessian: \\[ \\nabla^2 f = \\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 2 \\end{bmatrix} \\succ 0. \\] The constraint \\(x + y = 4\\) is affine, hence convex. Therefore, this is a convex optimization problem, and any local minimum is a global minimum. We can use the constraint to eliminate one variable: \\[ y = 4 - x. \\] Substitute into \\(f\\): \\[ \\begin{aligned} f(x, 4-x) &amp;= x^2 + (4-x)^2 - 4x - 6(4-x) \\\\ &amp;= x^2 + 16 - 8x + x^2 - 4x - 24 + 6x \\\\ &amp;= 2x^2 - 6x - 8. \\end{aligned} \\] Differentiate with respect to \\(x\\): \\[ \\frac{d}{dx}(2x^2 - 6x - 8) = 4x - 6. \\] Set equal to zero: \\[ 4x - 6 = 0 \\quad \\Rightarrow \\quad x = \\frac{3}{2}. \\] Find \\(y\\): \\[ y = 4 - \\frac{3}{2} = \\frac{5}{2}. \\] The minimum value is \\[ \\begin{aligned} f\\!\\left(\\frac{3}{2}, \\frac{5}{2}\\right) &amp;= \\left(\\frac{3}{2}\\right)^2 + \\left(\\frac{5}{2}\\right)^2 - 4\\left(\\frac{3}{2}\\right) - 6\\left(\\frac{5}{2}\\right) \\\\ &amp;= \\frac{9}{4} + \\frac{25}{4} - 6 - 15 \\\\ &amp;= \\frac{34}{4} - 21 \\\\ &amp;= -\\frac{25}{2}. \\end{aligned} \\] This solution is the unique global minimizer due to convexity. Definition 3.9 A linear program (LP) has a linear objective and linear constraints: \\[ \\begin{aligned} \\min_{\\mathbf{x} \\in \\mathbb{R}^d} \\quad &amp; \\mathbf{c}^{\\top}\\mathbf{x} \\\\ \\text{subject to} \\quad &amp; \\mathbf{A}\\mathbf{x} \\le \\mathbf{b}. \\end{aligned} \\] Here \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times d}\\) and \\(\\mathbf{b} \\in \\mathbb{R}^m\\). The dual problem is also a linear program: \\[ \\begin{aligned} \\max_{\\mathbf{\\lambda} \\ge 0} \\quad &amp; -\\mathbf{b}^{\\top}\\mathbf{\\lambda} \\\\ \\text{subject to} \\quad &amp; \\mathbf{c} + \\mathbf{A}^{\\top}\\mathbf{\\lambda} = 0. \\end{aligned} \\] Depending on whether \\(d\\) (number of variables) or \\(m\\) (number of constraints) is larger, we may solve the primal or dual form. Example 3.15 Linear Programming with Primal, Dual, and Solution Consider the linear program: \\[ \\begin{aligned} \\text{maximize } \\quad &amp; z = 3x + 2y \\\\ \\text{subject to } \\quad &amp; x + y \\le 4, \\\\ &amp; 2x + y \\le 5, \\\\ &amp; x \\ge 0,\\; y \\ge 0. \\end{aligned} \\] This is a linear program because both the objective function and the constraints are linear. The original problem is the primal problem. The feasible region, defined by the constraints (the corner points), is: \\((0,0)\\) \\((0,4)\\) from \\(x+y=4\\) \\((2.5,0)\\) from \\(2x+y=5\\) Intersection of \\(x+y=4\\) and \\(2x+y=5\\) Solve the intersection: \\[ \\begin{aligned} x + y &amp;= 4 \\\\ 2x + y &amp;= 5 \\end{aligned} \\quad \\Rightarrow \\quad x = 1,\\; y = 3. \\] We evaluate the objective function at each corner: Point \\(z = 3x + 2y\\) \\((0,0)\\) 0 \\((0,4)\\) 8 \\((2.5,0)\\) 7.5 \\((1,3)\\) 9 Therefore, the optimal solution is \\[ x^* = 1,\\quad y^* = 3,\\quad z^* = 9 \\] Writing the primal in matrix form: let \\[ \\mathbf{x} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\quad \\mathbf{c} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{A} = \\begin{bmatrix} 1 &amp; 1 \\\\ 2 &amp; 1 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} 4 \\\\ 5 \\end{bmatrix}. \\] Then the primal problem is: \\[ \\begin{aligned} \\text{maximize } &amp; \\mathbf{c}^\\top \\mathbf{x} \\\\ \\text{subject to } &amp; \\mathbf{A}\\mathbf{x} \\le \\mathbf{b}, \\\\ &amp; \\mathbf{x} \\ge 0. \\end{aligned} \\] For a primal of the form \\[ \\max \\{ \\mathbf{c}^\\top \\mathbf{x} : \\mathbf{A}\\mathbf{x} \\le \\mathbf{b},\\; \\mathbf{x} \\ge 0 \\}, \\] the dual is: \\[ \\min \\{ \\mathbf{b}^\\top \\mathbf{y} : \\mathbf{A}^\\top \\mathbf{y} \\ge \\mathbf{c},\\; \\mathbf{y} \\ge 0 \\}. \\] Let \\(\\mathbf{y} = (y_1, y_2)^\\top\\). The dual is: \\[ \\begin{aligned} \\text{minimize } \\quad &amp; 4y_1 + 5y_2 \\\\ \\text{subject to } \\quad &amp; y_1 + 2y_2 \\ge 3, \\\\ &amp; y_1 + y_2 \\ge 2, \\\\ &amp; y_1 \\ge 0,\\; y_2 \\ge 0. \\end{aligned} \\] Check the intersection where both constraints are tight: \\[ \\begin{aligned} y_1 + 2y_2 &amp;= 3 \\\\ y_1 + y_2 &amp;= 2 \\end{aligned} \\quad \\Rightarrow \\quad y_1 = 1,\\; y_2 = 1. \\] Compute the objective value: \\[ 4(1) + 5(1) = 9. \\] Therefore, \\[ y_1^* = 1,\\quad y_2^* = 1,\\quad \\text{minimum value} = 9 \\] The primal and dual have the same optimal value. 3.3.3 Quadratic Programming Definition 3.10 A quadratic program (QP) minimizes a convex quadratic function under affine constraints: \\[ \\begin{aligned} \\min_{x \\in \\mathbb{R}^d} \\quad &amp; \\tfrac{1}{2}x^{\\top}Qx + c^{\\top}x \\\\ \\text{subject to} \\quad &amp; Ax \\le b, \\end{aligned} \\] where \\(Q\\) is positive definite, ensuring convexity. Example 3.16 Consider the quadratic program: \\[ \\begin{aligned} \\text{minimize } \\quad &amp; f(x, y) = x^2 + y^2 \\\\ \\text{subject to } \\quad &amp; x + y = 1. \\end{aligned} \\] This is a quadratic program because: The objective function is quadratic. The constraint is linear. The objective can be written in matrix form as: \\[ f(\\mathbf{x}) = \\mathbf{x}^\\top \\mathbf{Q}\\mathbf{x}, \\quad \\mathbf{x} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\quad \\mathbf{Q} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}. \\] Since \\(\\mathbf{Q}\\) is positive definite, this is a convex quadratic program, and any local minimum is a global minimum. Form the Lagrangian: \\[ \\mathcal{L}(x,y,\\lambda) = x^2 + y^2 + \\lambda(x + y - 1). \\] Take partial derivatives: \\[ \\begin{aligned} \\frac{\\partial \\mathcal{L}}{\\partial x} &amp;= 2x + \\lambda = 0, \\\\ \\frac{\\partial \\mathcal{L}}{\\partial y} &amp;= 2y + \\lambda = 0, \\\\ \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} &amp;= x + y - 1 = 0. \\end{aligned} \\] From the first two equations: \\[ 2x = 2y \\quad \\Rightarrow \\quad x = y. \\] Substitute into the constraint: \\[ x + x = 1 \\quad \\Rightarrow \\quad x = \\frac{1}{2}. \\] Thus: \\[ x^* = \\frac{1}{2}, \\quad y^* = \\frac{1}{2}. \\] This gives an optimal solution of \\[ f\\!\\left(\\frac{1}{2}, \\frac{1}{2}\\right) = \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{2}. \\] Definition 3.11 The dual form of the QP is: \\[ \\begin{aligned} \\max_{\\mathbf{\\lambda} \\ge 0} \\quad &amp; -\\tfrac{1}{2}(c + A^{\\top}\\mathbf{\\lambda})^{\\top}Q^{-1}(c + A^{\\top}\\mathbf{\\lambda}) - b^{\\top}\\mathbf{\\lambda}. \\end{aligned} \\] Example 3.17 We revisit the quadratic program: \\[ \\begin{aligned} \\text{minimize } \\quad &amp; f(x,y) = x^2 + y^2 \\\\ \\text{subject to } \\quad &amp; x + y = 1. \\end{aligned} \\] In vector form, let \\[ \\mathbf{x} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\quad \\mathbf{Q} = \\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 2 \\end{bmatrix}, \\quad \\mathbf{A} = \\begin{bmatrix} 1 &amp; 1 \\end{bmatrix}, \\quad \\mathbf{b} = 1. \\] Then the primal problem is: \\[ \\begin{aligned} \\text{minimize } \\quad &amp; \\frac{1}{2}\\mathbf{x}^\\top \\mathbf{Q}\\mathbf{x} \\\\ \\text{subject to } \\quad &amp; \\mathbf{A}\\mathbf{x} = \\mathbf{b}. \\end{aligned} \\] Introduce a Lagrange multiplier \\(\\lambda \\in \\mathbb{R}\\). The Lagrangian is: \\[ \\mathcal{L}(\\mathbf{x}, \\lambda) = \\frac{1}{2}\\mathbf{x}^\\top \\mathbf{Q}\\mathbf{x} + \\lambda(\\mathbf{A}\\mathbf{x} - \\mathbf{b}). \\] The dual function is obtained by minimizing the Lagrangian with respect to \\(\\mathbf{x}\\): \\[ g(\\lambda) = \\inf_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\lambda). \\] Take the gradient with respect to \\(\\mathbf{x}\\): \\[ \\nabla_{\\mathbf{x}} \\mathcal{L} = \\mathbf{Q}\\mathbf{x} + \\mathbf{A}^\\top \\lambda. \\] Setting this to zero gives: \\[ \\mathbf{x} = -\\mathbf{Q}^{-1}\\mathbf{A}^\\top \\lambda. \\] Since \\[ \\mathbf{Q}^{-1} = \\begin{bmatrix} \\frac{1}{2} &amp; 0 \\\\ 0 &amp; \\frac{1}{2} \\end{bmatrix}, \\quad \\mathbf{A}^\\top = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\] we obtain: \\[ \\mathbf{x} = -\\frac{\\lambda}{2} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}. \\] Substitute this expression for \\(\\mathbf{x}\\) into the Lagrangian: \\[ \\begin{aligned} g(\\lambda) &amp;= \\frac{1}{2}\\mathbf{x}^\\top \\mathbf{Q}\\mathbf{x} + \\lambda(\\mathbf{A}\\mathbf{x} - \\mathbf{b}) \\\\ &amp;= -\\frac{1}{4}\\lambda^2 - \\lambda. \\end{aligned} \\] Thus, the dual problem is: \\[ \\boxed{ \\begin{aligned} \\text{maximize } \\quad &amp; g(\\lambda) = -\\frac{1}{4}\\lambda^2 - \\lambda \\\\ \\text{subject to } \\quad &amp; \\lambda \\in \\mathbb{R}. \\end{aligned} } \\] To solve the dual, differentiate: \\[ \\frac{d}{d\\lambda} g(\\lambda) = -\\frac{1}{2}\\lambda - 1. \\] Set to zero: \\[ -\\frac{1}{2}\\lambda - 1 = 0 \\quad \\Rightarrow \\quad \\lambda^* = -2. \\] Using \\[ \\mathbf{x} = -\\mathbf{Q}^{-1}\\mathbf{A}^\\top \\lambda, \\] we obtain: \\[ \\mathbf{x}^* = -\\frac{-2}{2} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{bmatrix}. \\] Therefore, the primal optimal solution is \\[ \\mathbf{x}^* = \\begin{bmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{bmatrix}, \\quad f^* = \\frac{1}{2}. \\] The dual optimal solution is \\[ \\lambda^* = -2. \\] Strong duality holds because the problem is convex with linear equality constraints. Quadratic programs are common in machine learning  for example, in support vector machines. 3.3.4 LegendreFenchel Transform and Convex Conjugate Convex functions can also be characterized using their supporting hyperplanes (tangents). The LegendreFenchel transform (or convex conjugate) formalizes this relationship. Definition 3.12 For a function \\(f: \\mathbb{R}^D \\to \\mathbb{R}\\), the convex conjugate is defined as: \\[ f^*(s) = \\sup_{x \\in \\mathbb{R}^D} \\big( \\langle s, x \\rangle - f(x) \\big). \\] If \\(f\\) is convex and differentiable, the supremum occurs where \\(s = \\nabla f(x)\\), and we have: \\[ f^*(s) = s^{\\top}x - f(x). \\] For convex functions, applying the transform twice returns the original function: \\[ f^{**}(x) = f(x). \\] 3.3.5 Connection to Duality Using the LegendreFenchel transform, a general equality-constrained convex optimization problem: \\[ \\min_x f(Ax) + g(x) \\] can be expressed equivalently as a dual problem: \\[ \\max_u -f^*(u) - g^*(-A^{\\top}u). \\] This relationship shows how convex conjugates naturally lead to the formulation of dual problems in optimization. Exercises Exercise 3.11 Consider the linear program \\[ \\min_{\\mathbf{x} \\in \\mathbb{R}^2} -\\begin{bmatrix}5\\\\3 \\end{bmatrix}^T\\begin{bmatrix}x_1\\\\x_2 \\end{bmatrix} \\;\\;\\;\\;\\; \\text{subject to} \\;\\;\\;\\;\\; \\begin{bmatrix} 2 &amp; 2 \\\\ 2 &amp; -4 \\\\ -2 &amp; 1 \\\\ 0 &amp; -1 \\\\ 0 &amp; 1 \\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix} \\leq \\begin{bmatrix} 33 \\\\ 8 \\\\ -5 \\\\ -1 \\\\ 8 \\end{bmatrix}.\\] Solve this problem showing your work. Derive the dual using Lagrange duality. Solution Exercise 3.12 Consider the linear program \\[\\begin{align*} \\text{maximize } &amp; P = 5x + 12y\\\\ \\text{subject to } &amp; 20x + 10y \\leq 200\\\\ &amp; 10x + 20 y \\leq 120\\\\ &amp; 10x + 30y \\leq 150\\\\ &amp;x \\geq 0\\\\ &amp;y \\geq 0. \\end{align*}\\] Illustrate the problem. Find the optimal solution. Recreate this problem in matrix form. Derive the dual using Lagrange duality. Solution Exercise 3.13 Consider the linear program \\[\\begin{align*} \\text{maximize } &amp; P = 100x + 120y\\\\ \\text{subject to } &amp; x + y \\leq 220\\\\ &amp; 30x + 20y \\leq 480\\\\ &amp; x + 2y \\leq 36\\\\ &amp;x \\geq 0\\\\ &amp;y \\geq 0. \\end{align*}\\] Illustrate the problem. Find the optimal solution. Recreate this problem in matrix form. Derive the dual using Lagrange duality. Solution "],["data-models-and-learning.html", "Chapter 4 Data, Models, and Learning", " Chapter 4 Data, Models, and Learning The second part of the book introduces the four pillars of machine learning: Regression (Chapter 9) Dimensionality Reduction (Chapter 10) Density Estimation (Chapter 11) Classification (Chapter 12) This part connects the mathematical foundations from earlier chapters to practical algorithms that solve real-world tasks. The goal is to show how concepts from linear algebra, probability, and optimization enable the design of learning systems  not to explore every advanced method, but to establish a practical and mathematical grounding. "],["the-three-components-of-machine-learning.html", "4.1 The Three Components of Machine Learning", " 4.1 The Three Components of Machine Learning Machine learning systems involve three fundamental components: Data  numerical or structured information used for training and testing. Models  mathematical structures or functions that represent relationships in the data. Learning  the process of adjusting model parameters to improve performance. A good model is one that performs well on unseen data, judged by performance metrics such as accuracy or distance from a ground truth. This chapter outlines common frameworks for training and evaluating models, including: Empirical risk minimization (ERM) Maximum likelihood estimation (MLE) Probabilistic modeling Graphical models Model selection techniques 4.1.1 Data as Vectors Machine learning assumes data can be represented numerically, typically in tabular form where: Rows correspond to examples (or data points). Columns correspond to features (also called attributes or covariates). Each example is a vector: \\[ \\mathbf{x}_n \\in \\mathbb{R}^D, \\quad n = 1, \\ldots, N \\] where \\(D\\) is the number of features and \\(N\\) the number of samples. It is also important to remember that: Categorical data must be encoded numerically (e.g., gender as 0/1). Scaling: Data should typically be standardized so each feature has mean 0 and variance 1. Identifiers (e.g., names) are often dropped for privacy and because they provide no predictive power. Therefore, the dataset is represented as: \\[ \\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_N, y_N)\\}, \\quad \\mathcal{X} \\in \\mathbb{R}^{N \\times D} \\] Example 4.1 Predicting annual salary \\(y\\) from age \\(x\\) is a supervised learning problem, where each data point has an associated label. 4.1.2 Models as Functions Definition 4.1 A model defines a mapping from inputs to outputs  a predictor: \\[ f: \\mathbb{R}^D \\to \\mathbb{R} \\] For simplicity, many algorithms use linear predictors: \\[ f(x) = \\mathbf{\\mathbf{\\theta}}^{\\top}\\mathbf{x} + \\mathbf{\\theta}_0 \\] where \\(\\mathbf{\\mathbf{\\theta}}\\) and \\(\\mathbf{\\theta}_0\\) are parameters to be learned. Linear models balance mathematical simplicity and expressive power, forming the foundation for regression and classification methods. 4.1.3 Models as Probability Distributions Real-world data is noisy, so models must handle uncertainty. Definition 4.2 A probabilistic model represents predictions not as fixed outputs but as distributions over possible outcomes. Instead of a single predictor \\(f(\\mathbf{x})\\), we consider a distribution over functions parameterized by finite-dimensional variables. These models express: Uncertainty in predictions (e.g., confidence intervals) Uncertainty in parameters Probability theory (Chapter 6) provides the foundation for these ideas. Probabilistic modeling is used to describe machine learning systems in Section 8.4, and represent them compactly with graphical models in Section 8.5. 4.1.4 Learning as Finding Parameters Definition 4.3 Learning is the process of finding model parameters that perform well on unseen data. This involves three algorithmic phases: Prediction (Inference)  Using a trained model on new data. Training (Parameter Estimation)  Adjusting parameters based on training data. Model Selection (Hyperparameter Tuning)  Choosing among competing models or configurations. There are many different training strategies that may be utilized. Some of those include: Empirical Risk Minimization (ERM)  Optimize parameters by minimizing prediction error on training data (Section 8.2). Maximum Likelihood Estimation (MLE)  Choose parameters that make observed data most probable (Section 8.3). Bayesian Inference  Model uncertainty over parameters using probability distributions (Section 8.4). Training typically involves numerical optimization (Chapter 7), often framed as minimizing a cost function. Cross-validation (Section 8.2.4) is used to simulate performance on unseen data. 4.1.5 Regularization and Model Complexity To achieve good generalization, we balance: Fit to training data, and Model simplicity. This is achieved through: Regularization  adding penalty terms to discourage complexity (Section 8.2.3) Bayesian priors  probabilistic constraints on parameters (Section 8.3.2) This process reflects abduction  inference to the best explanation  rather than strict induction or deduction. 4.1.6 Model Selection and Hyperparameters Model selection chooses the best model or hyperparameters (e.g., number of components, type of distribution). This can be done using: Cross-validation Nested cross-validation for hyperparameter tuning. Exercises Exercise 4.1 Suppose we have a model \\[\\hat{y}_i = \\beta_0 + \\beta_1 \\mathbf{x}_i.\\] The least squares problem aims to minimize the loss function \\[l(\\mathbf{x}) = (y_i - (\\beta_0 + \\beta_1\\mathbf{x}_i))^2.\\] Suppose we add a regularization term \\(\\lambda ||\\mathbf{ \\beta }||^2\\) to the function we want to minimize. Construct the function we want to now minimize and find the equations that minimize the sum of squared residuals. This is known as L2 regularization. Solution Exercise 4.2 Suppose we have a model \\[\\hat{y}_i = \\beta_0 + \\beta_1 \\mathbf{x}_i.\\] The least squares problem aims to minimize the loss function \\[l(\\mathbf{x}) = (y_i - (\\beta_0 + \\beta_1\\mathbf{x}_i))^2.\\] Suppose we add a regularization term \\(\\lambda \\sum_{i=0}^1 |\\beta_i |\\) to the function we want to minimize. Construct the function we want to now minimize and find the equations that minimize the sum of squared residuals. This is known as L1 regularization. Solution "],["empirical-risk-minimization.html", "4.2 Empirical Risk Minimization", " 4.2 Empirical Risk Minimization Empirical Risk Minimization (ERM) is the foundation of learning in machine learning. It focuses on estimating model parameters from training data to minimize prediction error. The section introduces four main design components: Hypothesis Class: What kind of functions can the predictor take? Loss Function: How do we measure how well the predictor fits the training data? Regularization: How do we prevent overfitting and ensure good generalization? Model Search: How do we assess and choose the best model? 4.2.1 Hypothesis Class of Functions In supervised learning, we are given a dataset of input-output pairs: \\[ \\mathcal{D} = \\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\ldots, (\\mathbf{x}_N, y_N)\\}, \\] where each \\(\\mathbf{x}_n \\in \\mathbb{R}^D\\) represents an input feature vector, and \\(y_n \\in \\mathbb{R}\\) (or a discrete set) represents the target label. The goal of learning is to find a predictor function \\(f(\\mathbf{x}, \\mathbf{\\mathbf{\\theta}})\\) that maps input features to outputs, such that the predictions are as close as possible to the true values \\(y_n\\). Here, \\(\\mathbf{\\mathbf{\\theta}}\\) denotes the parameters (or weights) that define the specific function within a family of possible functions. Definition 4.4 The hypothesis class is the set of all functions that the learning algorithm can choose from, based on the model type and its parameters. Formally: \\[ \\mathcal{H} = \\{ f(\\mathbf{x}, \\mathbf{\\mathbf{\\theta}}) : \\mathbf{\\mathbf{\\theta}} \\in \\Theta \\}, \\] where: \\(\\mathcal{H}\\) is the hypothesis class (the space of possible predictors), \\(f(\\mathbf{x}, \\mathbf{\\mathbf{\\theta}})\\) is the model function, \\(\\Theta\\) is the parameter space that determines which functions are possible. The hypothesis class defines the capacity of a model  how flexible or expressive it can be in fitting data. Example 4.2 Linear (Affine) Models: A common and simple choice for \\(f(\\mathbf{x}, \\mathbf{\\mathbf{\\theta}})\\) is a linear model: \\[ f(\\mathbf{x}, \\mathbf{\\mathbf{\\theta}}) = \\mathbf{\\mathbf{\\theta}}^\\top \\mathbf{x} + b, \\] where: \\(\\mathbf{\\mathbf{\\theta}} \\in \\mathbb{R}^D\\) is a weight vector, \\(b \\in \\mathbb{R}\\) is a bias term. The hypothesis class for linear regression is: \\[ \\mathcal{H}_{\\text{linear}} = \\{ f(\\mathbf{x}) = \\mathbf{\\theta}^\\top \\mathbf{x} + b : \\mathbf{\\theta} \\in \\mathbb{R}^D, b \\in \\mathbb{R} \\}. \\] This means all hyperplanes in \\(\\mathbb{R}^D\\) are possible predictors. Linear models are interpretable and efficient but limited in representing nonlinear relationships. To capture more complex patterns, we can expand the hypothesis class by transforming the inputs or using nonlinear functions. Example 4.3 Nonlinear Models: Polynomial models: \\[ f(\\mathbf{x}, \\mathbf{\\theta}) = \\mathbf{\\theta}_0 + \\mathbf{\\theta}_1 \\mathbf{x} + \\mathbf{\\theta}_2 \\mathbf{x}^2 + \\cdots + \\mathbf{\\theta}_k \\mathbf{x}^k \\] Neural networks: \\[ f(\\mathbf{x}, \\mathbf{\\theta}) = \\sigma(W_2 \\, \\sigma(W_1 \\mathbf{x} + b_1) + b_2) \\] where \\(\\sigma(\\cdot)\\) is a nonlinear activation function. These types of models can approximate complex relationships but risk overfitting if the hypothesis class is too flexible relative to the data size. A small hypothesis class (e.g., linear functions) may lead to underfitting, failing to capture important patterns. A large hypothesis class (e.g., deep neural networks) can represent almost any function but may overfit training data. The key is to choose a hypothesis class that is rich enough to capture underlying relationships but constrained enough to generalize well. Geometrically, each function \\(f(\\mathbf{x}, \\mathbf{\\theta})\\) in \\(\\mathcal{H}\\) corresponds to a surface (in regression) or decision boundary (in classification) in feature space. Learning means selecting one of these functions (via parameter estimation) that best fits the observed data under a chosen loss function. 4.2.2 Loss Function for Training Definition 4.5 A loss function \\(\\ell(y_n, \\hat{y}_n)\\) measures the discrepancy between the true label \\(y_n\\) and prediction \\(\\hat{y}_n = f(\\mathbf{x}_n, \\mathbf{\\theta})\\). Definition 4.6 The empirical risk is the average loss across all training examples: \\[ \\mathbf{R}_{\\text{emp}}(f, \\mathbf{\\mathbf{X}}, \\mathbf{y}) = \\frac{1}{N} \\sum_{n=1}^N \\ell(y_n, f(\\mathbf{x}_n, \\mathbf{\\theta})) \\] ERM (Empirical Risk Minimization) seeks to minimize this risk: \\[ \\min_{\\mathbf{\\theta}} \\mathbf{R}_{\\text{emp}}(f, \\mathbf{\\mathbf{X}}, \\mathbf{y}). \\] Example 4.4 In least squares regression, the average empirical risk is given by \\[ \\frac{1}{N} \\sum_{n=1}^N (y_n - \\mathbf{\\theta}^\\top \\mathbf{x}_n)^2. \\] ERM seeks to find \\[ \\min_{\\mathbf{\\theta}} \\frac{1}{N} \\sum_{n=1}^N (y_n - \\mathbf{\\theta}^\\top \\mathbf{x}_n)^2 \\] or equivalently, in matrix form: \\[ \\min_{\\mathbf{\\theta}} \\frac{1}{N} \\|\\mathbf{y} - \\mathbf{\\mathbf{X}}\\mathbf{\\theta}\\|^2 \\] Definition 4.7 The true (expected) risk measures performance on unseen data: \\[ \\mathbf{R}_{\\text{true}}(f) = \\mathbb{E}_{\\mathbf{x},y}[\\ell(y, f(\\mathbf{x}))] \\] ERM approximates true risk with finite training data. 4.2.3 Regularization to Reduce Overfitting Regularization reduces overfitting and improves generalization. Once a hypothesis class \\(\\mathcal{H}\\) is chosen, the next challenge is ensuring that the model not only fits the training data well but also generalizes effectively to unseen data. This problem is known as the bias-variance trade-off, and one of the most powerful tools to manage it is regularization. Definition 4.8 A model overfits when it learns patterns that exist only in the training data  including noise or random fluctuations  rather than the underlying structure of the problem. For example: A high-degree polynomial may perfectly interpolate training points but produce wild oscillations on new inputs. A deep neural network with too many parameters may memorize the data rather than learn general features. Overfitting leads to low training error but high test error. Definition 4.9 Regularization introduces a penalty for model complexity directly into the optimization objective. It biases the learning algorithm toward simpler models that are less likely to overfit. The general form of a regularized optimization problem is: \\[ \\min_{\\mathbf{\\theta}} \\; \\mathcal{L}_{\\text{train}}(\\mathbf{\\theta}) + \\lambda \\, \\Omega(\\mathbf{\\theta}) \\] where: \\(\\mathcal{L}_{\\text{train}}(\\mathbf{\\theta})\\): training loss (e.g., mean squared error, cross-entropy) \\(\\Omega(\\mathbf{\\theta})\\): regularization term that penalizes large or complex parameter values \\(\\lambda \\ge 0\\): regularization coefficient controlling the strength of the penalty A large \\(\\lambda\\) enforces more simplicity (strong regularization), while a small \\(\\lambda\\) allows more flexibility. 4.2.4 Cross-Validation to Assess Generalization Even after applying regularization, a key question remains: How well does our model generalize to unseen data? In machine learning, generalization refers to a models ability to perform well not just on the training data (used to learn parameters), but also on new, unseen data drawn from the same underlying distribution. Because we only have access to a finite dataset, we must estimate generalization performance empirically. Cross-validation provides a systematic way to do this. 4.2.4.1 The Challenge of Estimating Generalization If we train and test on the same dataset, the model will almost always appear to perform well  often unrealistically so. This is because the model has already seen the data during training, and may have overfit to it. To better approximate the true risk \\[ R_{\\text{true}}(f) = \\mathbb{E}_{\\mathbf{x}, y}[\\ell(y, f(\\mathbf{x}))],\\] we need a separate dataset on which the model has not been trained. A simple approach is to split the data into: a training set for fitting parameters, and a test (or validation) set for evaluating generalization. However, when data is limited, such a simple split can be unreliable  the test performance may depend heavily on which points happened to be held out. This motivates cross-validation. 4.2.4.2 The Idea of Cross-Validation Cross-validation (CV) is a statistical resampling method that allows us to: Use all the data for both training and validation (just not at the same time), and Obtain a more robust estimate of model performance by averaging across multiple train/test splits. The most widely used form is K-fold cross-validation. 4.2.4.3 K-Fold Cross-Validation In K-fold cross-validation, the dataset is randomly partitioned into \\(K\\) equally (or nearly equally) sized subsets, or folds. Then, the procedure is: For each \\(k = 1, \\ldots, K\\): Hold out fold \\(k\\) as the validation set \\(V^{(k)}\\). Use the remaining \\(K - 1\\) folds as the training set \\(R^{(k)}\\). Train the model on \\(R^{(k)}\\), producing a predictor \\(f^{(k)}\\). Compute the empirical risk (validation error) on \\(V^{(k)}\\): \\[ R_{\\text{emp}}(f^{(k)}, V^{(k)}) = \\frac{1}{|V^{(k)}|} \\sum_{(\\mathbf{x}_i, y_i) \\in V^{(k)}} \\ell(y_i, f^{(k)}(\\mathbf{x}_i)) \\] Average the validation errors across all folds: \\[ E_V[R(f, V)] \\approx \\frac{1}{K} \\sum_{k=1}^{K} R_{\\text{emp}}(f^{(k)}, V^{(k)}) \\] The resulting average gives an estimate of the expected generalization error. 4.2.4.4 Choosing \\(K\\) The choice of \\(K\\) controls the biasvariance tradeoff of the cross-validation estimate: K Training Proportion Bias Variance Computational Cost 2 50% High Low Low 5 80% Moderate Moderate Moderate 10 90% Low Moderate Higher N (Leave-One-Out CV) ~100% Very Low High Very High Generally speaking, a smaller \\(K\\) leads to less computation, but a higher bias (since less data per training run). A larger \\(K\\) has better performance estimates, but more computationally expensive. In practice, \\(K = 5\\) or \\(K = 10\\) are common choices. Exercises Exercise 4.3 Verify that \\[\\mathcal{L}(\\mathbf{\\theta})=-\\log(p(\\mathcal{Y}|\\mathcal{X},\\mathbf{\\theta}) = -\\sum_{n=1}^N \\log(p(y_n|\\mathbf{x}_n,\\mathbf{\\theta}).\\] Make sure to justify all of your steps. Solution Exercise 4.4 Verify that \\[\\mathcal{L}(\\mathbf{\\theta})=\\dfrac{1}{2\\sigma^2}\\sum_{n=1}^N (y_n - \\mathbf{x}_n^T\\mathbf{\\theta})^2 - \\sum_{n=1}^N \\log \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}.\\] Make sure to justify all of your steps. Solution Exercise 4.5 If we have prior knowledge about a distribution of the parameters, \\(\\theta\\), we can multiply an additional term to our likelihood, \\(p(\\mathbf{x} | \\mathbf{\\theta})\\). We know \\[p(\\mathbf{\\theta}|\\mathbf{x})=\\dfrac{p(\\mathbf{x} | \\mathbf{\\theta})p(\\mathbf{\\theta})}{p(\\mathbf{x})}.\\] However, we can ignore \\(p(\\mathbf{x})\\) and we write \\[p(\\mathbf{\\theta}|\\mathbf{x}) \\propto p(\\mathbf{x} | \\mathbf{\\theta})p(\\mathbf{\\theta}).\\] Why? Solution "],["parameter-estimation.html", "4.3 Parameter Estimation", " 4.3 Parameter Estimation In this section, we introduce how probability distributions can model uncertainty in data and parameters. This extends the concepts from empirical risk minimization to a probabilistic framework, allowing us to reason about both the data-generating process and the model parameters. 4.3.1 8.3.1 Maximum Likelihood Estimation (MLE) Definition 4.10 Maximum Likelihood Estimation (MLE) is a statistical method used to estimate the parameters of a model by finding the values that make the observed data most probable. Suppose we have data \\(\\mathbf{x} = (\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N)\\) drawn from a probability distribution \\(p(\\mathbf{x} | \\mathbf{\\theta})\\) that depends on some unknown parameter(s) \\(\\mathbf{\\theta}\\). The likelihood function represents how likely the observed data is for different parameter values: \\[ L(\\mathbf{\\theta}) = p(\\mathbf{x} | \\mathbf{\\theta}) = \\prod_{n=1}^{N} p(\\mathbf{x}_n | \\mathbf{\\theta}) \\] The goal of MLE is to find the parameter value \\(\\hat{\\mathbf{\\theta}}\\) that maximizes this likelihood: \\[ \\hat{\\mathbf{\\theta}}_{\\text{MLE}} = \\arg\\max_{\\mathbf{\\theta}} L(\\mathbf{\\theta}) \\] Because products of probabilities can become very small, it is common to work with the log-likelihood, which turns products into sums and simplifies computation: \\[ \\ell(\\mathbf{\\theta}) = \\log L(\\mathbf{\\theta}) = \\sum_{n=1}^{N} \\log p(\\mathbf{x}_n | \\mathbf{\\theta}) \\] Maximizing the log-likelihood is equivalent to maximizing the likelihood itself, since the logarithm is a monotonic transformation. In practice, optimization algorithms often minimize rather than maximize functions. Therefore, we typically minimize the negative log-likelihood (NLL): \\[ \\mathcal{L}(\\mathbf{\\theta}) = -\\ell(\\mathbf{\\theta}) = - \\sum_{n=1}^{N} \\log p(\\mathbf{x}_n | \\mathbf{\\theta}) \\] When we maximize the log-likelihood (or minimize the negative log-likelihood), \\(\\mathbf{\\theta}\\) varies while the data \\(\\mathbf{x}\\) is fixed. Minimizing \\(L(\\mathbf{\\theta})\\) corresponds to maximizing the likelihood  that is, finding the parameters most likely to have produced the observed data. Example 4.5 If \\(p(y_n | \\mathbf{x}_n, \\mathbf{\\theta})\\) is Gaussian, MLE corresponds to minimizing the sum of squared residuals  the classic linear regression case. Although MLE can yield closed-form solutions in simple cases, it may suffer from overfitting and may require numerical optimization when closed forms are unavailable. 4.3.2 Maximum A Posteriori (MAP) Estimation When prior knowledge about parameters is available, we can combine it with the likelihood using Bayes theorem: \\[ p(\\mathbf{\\theta} | \\mathbf{x}) = \\frac{p(\\mathbf{x} | \\mathbf{\\theta}) p(\\mathbf{\\theta})}{p(\\mathbf{x})}. \\] Since \\(p(\\mathbf{x})\\) does not depend on \\(\\mathbf{\\theta}\\), maximizing the posterior is equivalent to maximizing \\(p(\\mathbf{x} | \\mathbf{\\theta})p(\\mathbf{\\theta})\\). This leads to Maximum A Posteriori (MAP) Estimation, where we minimize the negative log-posterior: \\[ L_{\\text{MAP}}(\\mathbf{\\theta}) = -\\log p(\\mathbf{x} | \\mathbf{\\theta}) - \\log p(\\mathbf{\\theta}) \\] MAP estimation adds a regularizing effect, since the prior \\(p(\\mathbf{\\theta})\\) discourages implausible parameter values. Example 4.6 With a Gaussian likelihood and a Gaussian prior on parameters (e.g., zero-mean prior), the MAP estimate resembles ridge regression  balancing data fit and parameter simplicity. 4.3.3 Model Fitting Model fitting involves optimizing parameters \\(\\mathbf{\\theta}\\) to minimize a loss (e.g., the negative log-likelihood). The model class \\(\\mathcal{M}_\\mathbf{\\theta}\\) defines the family of possible predictors, and fitting finds the instance within this class that best approximates the true data-generating process \\(\\mathcal{M}^*\\). There are three main fitting outcomes: Overfitting: The model class is too flexible. Captures noise as if it were signal. Low training error but high test error. Underfitting: The model class is too simple. Fails to capture the true data structure. High error on both training and test data. Good Fit: The model class is appropriately complex. Balances bias and variance. Exhibits good generalization. To mitigate overfitting, we can apply: Regularization (Section 8.2.3), or Priors (Section 8.3.2). In practice, large model classes such as deep neural networks rely on these techniques to control generalization and improve performance. Exercises Exercise 4.6 Suppose that you would like to estimate the portion of voters in your town that plan to vote for Party A in an upcoming election. To do so, you take a random sample of size \\(n\\) from the likely voters in the town. Since you have a limited amount of time and resources, your sample is relatively small. Specifically, suppose that \\(n=20\\). After doing your sampling, you find out that 6 people in your sample say they will vote for Party A. In the previous election, 40% of voters voted for party A. Provide both a frequentist and Bayesian approach to dealing with this problem. Solution "],["probabilistic-modeling-and-inference.html", "4.4 Probabilistic Modeling and Inference", " 4.4 Probabilistic Modeling and Inference In machine learning, we use probabilistic models to represent uncertainty in data and parameters. These models describe how observed data are generated from underlying parameters, allowing us to reason about prediction, inference, and decision-making under uncertainty. 4.4.1 Probabilistic Models Definition 4.11 A probabilistic model is defined by the joint distribution of all random variables: \\[ p(\\mathbf{x}, \\mathbf{\\theta}) \\] where \\(\\mathbf{x}\\) represents the observed data, and \\(\\mathbf{\\theta}\\) represents the model parameters. This joint distribution encapsulates: The likelihood \\(p(\\mathbf{x} | \\mathbf{\\theta})\\) The prior \\(p(\\mathbf{\\theta})\\) The posterior \\(p(\\mathbf{\\theta} | \\mathbf{x})\\) The marginal likelihood \\(p(\\mathbf{x}) = \\int p(\\mathbf{x} | \\mathbf{\\theta})p(\\mathbf{\\theta}) d\\mathbf{\\theta}\\) The probabilistic framework provides a consistent way to model, infer, and predict using probability theory. 4.4.2 Bayesian Inference Bayesian inference is concerned with computing the posterior distribution of parameters given data: \\[ p(\\mathbf{\\theta} | \\mathcal{X}) = \\frac{p(\\mathcal{X} | \\mathbf{\\theta}) p(\\mathbf{\\theta})}{p(\\mathcal{X})} \\] where \\[ p(\\mathcal{X}) = \\int p(\\mathcal{X} | \\mathbf{\\theta}) p(\\mathbf{\\theta}) d\\mathbf{\\theta} \\] acts as a normalization constant (marginal likelihood). Bayesian inference inverts the relationship between parameters and data. Instead of finding a single best parameter estimate (as in MLE or MAP), it computes a distribution over parameters, capturing full uncertainty. Predictions are then made by marginalizing over parameters: \\[ p(\\mathbf{x}) = \\int p(\\mathbf{x} | \\mathbf{\\theta})p(\\mathbf{\\theta})d\\mathbf{\\theta} = \\mathbb{E}_{\\mathbf{\\theta}}[p(\\mathbf{x} | \\mathbf{\\theta})] \\] 4.4.2.1 Comparison with Parameter Estimation: Approach Output Main Computation Example Methods MLE / MAP Point estimate \\(\\mathbf{\\theta}^*\\) Optimization Gradient Descent, Least Squares Bayesian Inference Distribution \\(p(\\mathbf{\\theta} | \\mathcal{X})\\) Integration MCMC, Laplace, Variational Inference Bayesian methods allow: Incorporation of prior knowledge Uncertainty propagation in predictions Better handling of small datasets or noisy data However, Bayesian inference often requires approximations since integrals for our posterior distribution and our parameters are rarely analytic. Common approximation methods include: Stochastic methods: Markov Chain Monte Carlo (MCMC) Deterministic methods: Laplace approximation, Variational Inference, Expectation Propagation 4.4.3 Latent-Variable Models Definition 4.12 A latent-variable model introduces hidden variables \\(z\\) that help explain the data. These variables are not directly observed but simplify or enrich the models structure. The generative process is: \\[ p(\\mathbf{x} | \\mathbf{z}, \\mathbf{\\theta}) \\] with a prior on latent variables: \\[ p(\\mathbf{z}). \\] To obtain the likelihood of observed data, we integrate out the latent variables: \\[ p(\\mathbf{x} | \\mathbf{\\theta}) = \\int p(\\mathbf{x} | \\mathbf{z}, \\mathbf{\\theta})p(\\mathbf{z})d\\mathbf{z}. \\] Once the likelihood is known, we can: Perform maximum likelihood or MAP estimation for parameters Conduct Bayesian inference to obtain posterior distributions The posterior over parameters is: \\[ p(\\mathbf{\\theta} | \\mathcal{X}) = \\frac{p(\\mathcal{X} | \\mathbf{\\theta})p(\\mathbf{\\theta})}{p(\\mathcal{X})} \\] and the posterior over latent variables is: \\[ p(\\mathbf{z} | \\mathcal{X}, \\mathbf{\\theta}) = \\frac{p(\\mathcal{X} | \\mathbf{z}, \\mathbf{\\theta})p(\\mathbf{z})}{p(\\mathcal{X} | \\mathbf{\\theta})}. \\] 4.4.4 Examples of Latent-Variable Models Principal Component Analysis (PCA)  dimensionality reduction Gaussian Mixture Models (GMMs)  density estimation Hidden Markov Models (HMMs)  time-series analysis Dynamical Systems  modeling temporal dependencies Meta-Learning / Task Generalization  learning across tasks Although introducing latent variables can make models more interpretable and flexible, inference becomes more challenging because marginalization is often intractable and requires approximation methods. Exercises Put some exercises here. "],["directed-graphical-models.html", "4.5 Directed Graphical Models", " 4.5 Directed Graphical Models Directed graphical models, also known as Bayesian networks, provide a graphical language for specifying probabilistic models. They offer a compact and intuitive way to visualize dependencies between random variables and to represent how a joint distribution can be decomposed into simpler conditional distributions. The key idea is that nodes represent random variables, and edges (arrows) represent probabilistic or conditional relationships. The joint distribution is expressed as a product of conditional probabilities. Graphical models help identify independence and conditional independence relationships among variables. Some of the benefits of graphical models are that they: Provide a simple way to visualize model structure. Facilitate the design and understanding of statistical models. Make independence properties easy to identify. Enable simplified inference and learning computations through graphical manipulations. 4.5.1 Graph Semantics A directed graphical model (or Bayesian network) represents conditional dependencies among random variables. Example 4.7 A joint distribution: \\[ p(a, b, c) = p(c | a, b) \\, p(b | a) \\, p(a) \\] implies the following: \\(c\\) depends on \\(a\\) and \\(b\\) \\(b\\) depends on \\(a\\) \\(a\\) is independent of \\(b\\) and \\(c\\) This factorization can be represented as a directed graph with arrows from \\(a \\to b\\) and \\(a, b \\to c\\). 4.5.1.1 General Rule for Constructing Graphs Create a node for each random variable. For each conditional probability, draw arrows from the conditioning variables to the conditioned variable. 4.5.1.2 General Form of the Joint Distribution For random variables \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_K\\), the joint distribution factorizes as: \\[ p(\\mathbf{x}) = p(\\mathbf{x}_1, \\ldots, \\mathbf{x}_K) = \\prod_{k=1}^K p(\\mathbf{x}_k | \\text{Pa}_k) \\] where \\(\\text{Pa}_k\\) denotes the parent nodes of \\(\\mathbf{x}_k\\) (nodes with arrows pointing to \\(\\mathbf{x}_k\\)). Example 4.8 Coin Flip Experiment For a Bernoulli trial with parameter \\(\\mu\\): \\[ p(\\mathbf{x} | \\mu) = \\text{Ber}(\\mu). \\] Repeating the experiment \\(N\\) times: \\[ p(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N | \\mu) = \\prod_{n=1}^N p(\\mathbf{x}_n | \\mu). \\] This can be represented graphically: Each outcome \\(\\mathbf{x}_n\\) depends on the same latent variable \\(\\mu\\). Plate notation is used to represent the repetition \\(N\\) times. We can also assign a hyperprior to \\(\\mu\\), for instance: \\[ \\mu \\sim \\text{Beta}(\\alpha, \\beta) \\] where \\(\\alpha\\) and \\(\\beta\\) may be treated as deterministic parameters. 4.5.2 Conditional Independence and d-Separation Directed graphical models can reveal conditional independence relationships directly from the graph using the concept of d-separation (Pearl, 1988). Definition 4.13 Given disjoint node sets \\(A, B, C\\) in a directed acyclic graph, \\(A\\) is d-separated from \\(B\\) given \\(C\\), written as: \\[ A \\perp\\!\\!\\!\\perp B \\,|\\, C \\] if all paths between nodes in \\(A\\) and \\(B\\) are blocked. A path is blocked if: The arrows meet head-to-tail or tail-to-tail at a node in \\(C\\) (a path of the form \\(a \\rightarrow c \\rightarrow b\\) or \\(a \\leftarrow c \\rightarrow b\\)), or The arrows meet head-to-head at a node not in \\(C\\), and none of its descendants are in \\(C\\) (a path of the form \\(a \\rightarrow c \\leftarrow b\\)). If all paths are blocked, \\(A\\) and \\(B\\) are conditionally independent given \\(C\\). Example 4.9 In the graphical model: We can infer: \\(b \\perp d \\,|\\, a, c\\) \\(a \\perp c \\,|\\, b\\) \\(b \\not\\!\\perp d \\,|\\, c\\) since \\(d\\) is a descendant of \\(c\\) \\(a \\not\\!\\perp c \\,|\\, b, e\\) since \\(d\\) is a descendant of \\(c\\) Example 4.10 d-Separation in a Bayesian Network Consider the following directed acyclic graph (DAG): \\[ A \\longrightarrow C \\longrightarrow D \\longleftarrow B \\] We will analyze conditional independence relationships using d-separation. We examine whether variables \\(A\\) and \\(B\\) are independent under different conditioning sets. There is exactly one undirected path between \\(A\\) and \\(B\\): \\[ A \\rightarrow C \\rightarrow D \\leftarrow B. \\] This path contains a collider at node \\(D\\). If we condition on nothing: The path contains a collider \\(D\\) Colliders block paths unless conditioned on (or their descendants are) The path is blocked \\[ A \\;\\perp\\!\\!\\!\\perp\\; B \\] Now condition on \\(C\\): \\(C\\) is a chain node Conditioning on a chain node blocks the path The collider at \\(D\\) remains unconditioned The path is still blocked \\[ A \\;\\perp\\!\\!\\!\\perp\\; B \\mid C \\] Now condition on \\(D\\): Conditioning on a collider opens the path The path is now unblocked \\[ A \\;\\not\\!\\perp\\!\\!\\!\\perp\\; B \\mid D \\] Knowing \\(A\\) gives information about \\(B\\) once \\(D\\) is known. Suppose \\(E\\) is a descendant of \\(D\\): \\[ A \\rightarrow C \\rightarrow D \\leftarrow B, \\quad D \\rightarrow E \\] Condition on \\(E\\): Conditioning on a descendant of a collider also opens the path The path is unblocked \\[ A \\;\\not\\!\\perp\\!\\!\\!\\perp\\; B \\mid E \\] Summary of Results Conditioning Set Are \\(A\\) and \\(B\\) d-separated? None Yes \\(C\\) Yes \\(D\\) No \\(E\\) No Exercises Exercise 4.7 Chain (No Conditioning) \\[ X \\rightarrow Y \\rightarrow Z \\] Question: Are \\(X\\) and \\(Z\\) independent? The path is a chain No conditioning blocks the path \\[ X \\;\\not\\!\\perp\\!\\!\\!\\perp\\; Z \\] Solution Exercise 4.8 Chain (Condition on the Middle) \\[ X \\rightarrow Y \\rightarrow Z \\] Condition on \\(Y\\): Conditioning on a chain node blocks the path \\[ X \\;\\perp\\!\\!\\!\\perp\\; Z \\mid Y \\] Solution Exercise 4.9 Fork (Common Cause) \\[ X \\leftarrow Y \\rightarrow Z \\] No conditioning: \\(Y\\) is a common cause Path is open \\[ X \\;\\not\\!\\perp\\!\\!\\!\\perp\\; Z \\] Solution Exercise 4.10 Fork (Condition on the Common Cause) \\[ X \\leftarrow Y \\rightarrow Z \\] Condition on \\(Y\\): Conditioning on a fork node blocks the path \\[ X \\;\\perp\\!\\!\\!\\perp\\; Z \\mid Y \\] Solution Exercise 4.11 Collider (No Conditioning) \\[ X \\rightarrow Y \\leftarrow Z \\] No conditioning: \\(Y\\) is a collider Colliders block paths by default \\[ X \\;\\perp\\!\\!\\!\\perp\\; Z \\] Solution Exercise 4.12 Collider (Condition on the Collider) \\[ X \\rightarrow Y \\leftarrow Z \\] Condition on \\(Y\\): Conditioning on a collider opens the path \\[ X \\;\\not\\!\\perp\\!\\!\\!\\perp\\; Z \\mid Y \\] Solution "],["model-selection.html", "4.6 Model Selection", " 4.6 Model Selection Model selection in machine learning involves choosing among different models or configurations that can best generalize to unseen data. The goal is to balance model complexity and data fit  avoiding both underfitting and overfitting. Complex models (e.g., higher-degree polynomials) are more expressive and can describe a wider variety of datasets. However, greater flexibility often leads to overfitting on the training set, which reduces performance on unseen data. Therefore, model selection aims to identify the simplest model that explains the data sufficiently well  a concept known as Occams Razor. 4.6.1 Nested Cross-Validation Definition 4.14 Cross-validation estimates a models generalization error by repeatedly splitting data into training and validation sets. Definition 4.15 Nested cross-validation extends the idea of cross validation by using two levels of cross-validation: Inner loop: Chooses the best model or hyperparameters based on validation performance. Outer loop: Estimates the generalization performance of the chosen model on unseen test data. Lemma 4.1 The expected validation error estimate is given by: \\[ E_V[\\mathbf{R}(\\mathcal{V} | M)] \\approx \\frac{1}{K} \\sum_{k=1}^{K} \\mathbf{R}(\\mathcal{V}^{(k)} | M), \\] where \\(\\mathbf{R}(\\mathcal{V} | M)\\) is the empirical risk (e.g., RMSE) of model \\(M\\) on validation set \\(\\mathcal{V}\\). This process yields both a mean generalization estimate and a standard error for uncertainty quantification. 4.6.2 Bayesian Model Selection Bayesian model selection provides a probabilistic framework for comparing models. It incorporates both data fit and model complexity via Bayes theorem: \\[ p(M_k | \\mathcal{D}) \\propto p(M_k) \\, p(\\mathcal{D} | M_k). \\] Here: \\(p(M_k)\\): prior probability of model \\(M_k\\) \\(p(\\mathcal{D} | M_k)\\): model evidence or marginal likelihood \\[ p(\\mathcal{D} | M_k) = \\int p(\\mathcal{D} | \\mathbf{\\theta}_k) \\, p(\\mathbf{\\theta}_k | M_k) \\, d\\mathbf{\\theta}_k \\] This integral marginalizes over model parameters \\(\\mathbf{\\theta}_k\\), automatically penalizing overly complex models. Definition 4.16 Model evidence quantifies how well a model predicts observed data after accounting for parameter uncertainty. Thus, the MAP estimate of the best model is: \\[ M^* = \\arg\\max_{M_k} p(M_k | \\mathcal{D}) \\] 4.6.3 Bayes Factors for Model Comparison To compare two models \\(M_1\\) and \\(M_2\\), we consider their posterior odds: \\[ \\frac{p(M_1 | \\mathcal{D})}{p(M_2 | \\mathcal{D})} = \\underbrace{\\frac{p(M_1)}{p(M_2)}}_{\\text{prior odds}} \\times \\underbrace{\\frac{p(\\mathcal{D} | M_1)}{p(\\mathcal{D} | M_2)}}_{\\text{Bayes factor}} \\] The Bayes factor \\(\\frac{p(\\mathcal{D} | M_1)}{p(\\mathcal{D} | M_2)}\\) measures relative model support from the data. With uniform priors, model selection depends only on the Bayes factor. If the Bayes factor &gt; 1, \\(M_1\\) is preferred; otherwise, \\(M_2\\) is chosen. The Jeffreys-Lindley paradox is a phenomenon in Bayesian statistics that highlights a surprising difference between Bayesian model comparison and classical (frequentist) hypothesis testing. Suppose we want to test: \\[ H_0: \\mathbf{\\theta} = \\mathbf{\\theta}_0 \\quad \\text{vs.} \\quad H_1: \\mathbf{\\theta} \\neq \\mathbf{\\theta}_0 \\] In frequentist hypothesis testing, we might reject \\(H_0\\) if the p-value is small. In Bayesian model selection, we compute the Bayes factor: \\[ \\text{BF} = \\frac{p(\\text{data} | H_0)}{p(\\text{data} | H_1)} \\] The paradox arrises when, even if the data strongly rejects \\(H_0\\) according to a classical test (small p-value), the Bayes factor may favor the null hypothesis \\(H_0\\). This happens especially when the prior for the alternative hypothesis \\(H_1\\) is diffuse (spread over a wide range of possible parameter values). As a result, \\(H_0\\) can appear more probable in the Bayesian sense, even if the observed data seems extreme. The result is that Bayesian and frequentist conclusions can disagree. Prior choices in Bayesian analysis have a strong influence on model comparison. The paradox emphasizes the importance of carefully selecting priors for alternative hypotheses. 4.6.4 Computing the Marginal Likelihood The marginal likelihood integral: \\[ p(\\mathcal{D} | M_k) = \\int p(\\mathcal{D} | \\mathbf{\\theta}_k) p(\\mathbf{\\theta}_k | M_k) \\, d\\mathbf{\\theta}_k \\] is often analytically intractable. Common approximation techniques include: Numerical integration Monte Carlo sampling Bayesian Monte Carlo methods However, when using conjugate priors, this term can sometimes be computed in closed form. Exercises Put some exercises here. "],["linear-regression.html", "Chapter 5 Linear Regression", " Chapter 5 Linear Regression In linear regression, the goal is to find a function \\(f\\) that maps inputs \\(\\mathbf{x} \\in \\mathbb{R}^D\\) to outputs \\(f(\\mathbf{x}) \\in \\mathbb{R}\\). We are given training data \\(\\{(\\mathbf{\\mathbf{x}}_n, y_n)\\}_{n=1}^N\\), where each observation is modeled as: \\[ y_n = f(\\mathbf{x}_n) + \\epsilon_n. \\] Here, \\(\\epsilon_n\\) represents independent and identically distributed (i.i.d.) Gaussian noise with zero mean. With linear regression, we have two basic goals: Modeling the data: Estimate the function \\(f\\) that explains the observed data. Generalization: Ensure \\(f\\) predicts well for unseen inputs, not just the training data. When using linear regression, we have to make some decisions apriori. Model Choice: Decide on the type and parametrization of the regression function (e.g., polynomial degree). Model selection (see Section 8.6) helps identify the simplest model that explains the data effectively. Parameter Estimation: Determine the optimal model parameters using appropriate loss functions and optimization algorithms. Overfitting and Model Selection: Overfitting occurs when the model fits training data too closely, failing to generalize. This typically arises from overly flexible or complex models. Connection Between Loss Functions and Priors: Many optimization objectives can be derived from probabilistic assumptions about the data. Understanding this relationship clarifies how prior beliefs influence model behavior. Uncertainty Modeling: Since training data are finite, predictions carry uncertainty. Modeling uncertainty provides confidence bounds for predictions, which are especially important with limited data. "],["problem-formulation.html", "5.1 Problem Formulation", " 5.1 Problem Formulation In regression, we model the relationship between inputs \\(\\mathbf{x} \\in \\mathbb{R}^D\\) and outputs \\(y \\in \\mathbb{R}\\) in the presence of observation noise. We assume that each observation follows a probabilistic model: \\[ p(y | \\mathbf{x}) = \\mathcal{N}(y \\mid f(\\mathbf{\\mathbf{x}}), \\sigma^2). \\] This means the data are generated by: \\[ y = f(\\mathbf{x}) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2), \\] where \\(\\epsilon\\) is i.i.d. Gaussian noise with zero mean and variance \\(\\sigma^2\\). The goal is to find a function \\(f(\\mathbf{x})\\) that: Approimates the true (unknown) data-generating function, Generalizes well to unseen data. 5.1.1 Parametric Models We restrict our attention to parametric models, where the function depends on a set of parameters \\(\\theta\\). In linear regression the model is linear in the parameters: \\[ p(y | \\mathbf{x}, \\theta) = \\mathcal{N}(y \\mid \\mathbf{x}^T \\theta, \\sigma^2), \\] or equivalently, \\[ y = \\mathbf{x}^T \\theta + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2) \\] Here: \\(\\theta \\in \\mathbb{R}^D\\) represents the model parameters, The likelihood epresses how probable a given \\(y\\) is for known \\(\\mathbf{x}\\) and \\(\\theta\\), Without noise (\\(\\sigma^2 \\to 0\\)), the model becomes deterministic (a Dirac delta). Example 5.1 Linear Regression Model For \\(\\mathbf{x}, \\theta \\in \\mathbb{R}\\): The model \\(y = \\theta \\mathbf{x}\\) describes a straight line through the origin. The slope of the line is given by \\(\\theta\\). Different values of \\(\\theta\\) yield different linear functions. Although this model is linear in both \\(\\mathbf{x}\\) and \\(\\theta\\), we can generalize it by introducing nonlinear feature transformations: \\[ y = \\mathbf{\\phi}(\\mathbf{x})^T \\theta. \\] Here, \\(\\mathbf{\\phi}(\\mathbf{x})\\) represents a feature mapping of the input \\(\\mathbf{x}\\). The model remains linear in the parameters \\(\\theta\\), even if \\(\\mathbf{\\phi}(\\mathbf{x})\\) is nonlinear in \\(\\mathbf{x}\\). For now, we assume that the noise variance \\(\\sigma^2\\) is known and focus on learning the optimal parameters \\(\\theta\\). Exercises Exercise 5.1 Finding a regression function requires solving a variety of problems. List and discuss them. Solution Exercise 5.2 Consider the following data: \\[(1,3), (2,4), (3,8), (4,9).\\] Find the estimated regression line \\[y = \\beta_0 + \\beta_1 x,\\] based on the data. For each \\(x_i\\), compute both the estimated value of \\(y\\) and the residuals. Solution Exercise 5.3 A simple linear regression model is fit, relating plant growth over 1 year (y) to amount of fertilizer provided (x). Twenty five plants are selected, 5 each assigned to each of the fertilizer levels (12, 15, 18, 21, 24). The results of the model fit are given below: Regression Coefficients Model B Std. Error t Sig Constant 8.624 1.81 4.764 0 \\(x\\) 0.527 0.098 5.386 0 Can we conclude that there is an association between fertilizer and plant growth at the 0.05 significance level? Solution Exercise 5.4 A multiple regression model is fit, relating salary (Y) to the following predictor variables: experience (\\(X_1\\), in years), accounts in charge of (\\(X_2\\)) and gender (\\(X_3\\) is 1 if female, 0 if male). The following ANOVA table and output gives the results for fitting the model. Conduct all tests at the 0.05 significance level: ANOVA Table df SS MS F P-value Regression 3 2470.4 823.5 76.9 0 Residual 21 224.7 10.7 Total 24 2695.1 Regression Coefficients Model B Std. Error t Sig Constant 39.58 1.89 21.00 0 Experience 3.61 0.36 10.04 0 Accounts -0.28 0.36 -0.79 0.4389 Gender -3.92 1.48 -2.65 0.0149 Test whether salary is associated with any of the predictor variables. Solution "],["parameter-estimation-1.html", "5.2 Parameter Estimation", " 5.2 Parameter Estimation In linear regression, we are given a training dataset \\[ \\mathcal{D} = \\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_N, y_N)\\}, \\] where \\(\\mathbf{x}_n \\in \\mathbb{R}^D\\) are inputs and \\(y_n \\in \\mathbb{R}\\) are corresponding observations. Let \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) be the sets of training inputs and traning targets respectively. We assume each target is generated according to a probabilistic model: \\[ p(y_n | \\mathbf{x}_n, \\theta) = \\mathcal{N}(y_n| \\mathbf{x}_n^\\top \\theta, \\sigma^2), \\] where \\(\\theta\\) are model parameters and \\(\\sigma^2\\) is the noise variance. Because each data point is conditionally independent given the parameters, the likelihood factorizes as: \\[ p(\\mathcal{Y} | \\mathcal{X}, \\theta) = \\prod_{n=1}^N p(y_n | \\mathbf{x}_n, \\theta) = \\prod_{n=1}^N \\mathcal{N}(y_n| \\mathbf{x}_n^\\top \\theta, \\sigma^2). \\] 5.2.1 Maximum Likelihood Estimation (MLE) The Maximum Likelihood Estimation approach finds parameters that maximize the likelihood: \\[ \\theta_{\\text{ML}} = \\arg \\max_\\theta p(\\mathcal{Y} | \\mathcal{X}, \\theta). \\] Since taking the logarithm does not change the location of the optimum, we minimize the negative log-likelihood instead: \\[ L(\\theta) = -\\log p(\\mathcal{Y} | \\mathcal{X}, \\theta) = \\frac{1}{2\\sigma^2} \\| y - X\\theta \\|^2 + const. \\] This expression is quadratic in \\(\\theta\\), so setting its gradient to zero yields the closed-form solution: \\[ \\theta_{\\text{ML}} = (X^\\top X)^{-1} X^\\top y. \\] Here: \\(X \\in \\mathbb{R}^{N \\times D}\\) is the design matrix, where each row is \\(x_n^\\top\\). The matrix \\(X^\\top X\\) must be invertible (i.e., full column rank). This solution minimizes the sum of squared errors between the predictions \\(X\\theta\\) and the observed targets \\(y\\). 5.2.2 MLE with Nonlinear Features Although the term linear regression refers to models that are linear in the parameters, the inputs can undergo nonlinear transformations through a feature mapping: \\[ f(x) = \\mathbf{\\phi}(x)^\\top \\theta, \\] where \\(\\mathbf{\\phi}(x)\\) is a feature vector (e.g., polynomial terms, basis functions). We define the feature matrix as: \\[ \\mathbf{\\Phi} = \\begin{bmatrix} \\mathbf{\\phi}(x_1)^\\top \\\\ \\mathbf{\\phi}(x_2)^\\top \\\\ \\vdots \\\\ \\mathbf{\\phi}(x_N)^\\top \\end{bmatrix} \\in \\mathbb{R}^{N \\times K}. \\] For example, with second-order polynomial features: \\[ \\mathbf{\\phi}(x) = [1, x, x^2]^\\top, \\quad \\mathbf{\\Phi} = \\begin{bmatrix} 1 &amp; x_1 &amp; x_1^2 \\\\ 1 &amp; x_2 &amp; x_2^2 \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_N &amp; x_N^2 \\end{bmatrix}. \\] The corresponding MLE solution becomes: \\[ \\theta_{\\text{ML}} = (\\mathbf{\\Phi}^\\top \\mathbf{\\Phi})^{-1} \\mathbf{\\Phi}^\\top y. \\] The estimated noise variance can also be obtained as: \\[ \\sigma^2_{\\text{ML}} = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\mathbf{\\phi}(x_n)^\\top \\theta_{\\text{ML}})^2. \\] 5.2.3 Overfitting in Linear Regression Overfitting occurs when a model fits the training data too closely, capturing noise rather than the underlying trend. To assess performance, we often compute the Root Mean Square Error (RMSE): \\[ \\text{RMSE} = \\sqrt{\\frac{1}{N} \\| \\mathbf{y} - \\mathbf{\\mathbf{\\Phi}}\\mathbf{\\theta} \\|^2}. \\] Training error tends to decrease as model complexity (e.g., polynomial degree \\(M\\)) increases. Test error initially decreases but then increases beyond an optimal model complexity, indicating overfitting. For polynomial regression: Low-degree polynomials underfit (poor fit to data). Moderate-degree polynomials (e.g., \\(M = 4\\)) generalize well. High-degree polynomials (\\(M \\geq N - 1\\)) fit all training points but fail to generalize. 5.2.4 Maximum A Posteriori (MAP) Estimation Maximum likelihood estimation (MLE) can lead to overfitting, often resulting in very large parameter values. To mitigate this, we introduce a prior distribution \\(p(\\theta)\\) on the parameters, which encodes our beliefs about plausible parameter values before observing any data. For example: \\[ p(\\theta) = \\mathcal{N}(0, 1) \\] suggests that \\(\\theta\\) is likely to lie within approximately \\([-2, 2]\\). Once data \\((\\mathcal{X}, \\mathcal{Y})\\) are available, we seek parameters that maximize the posterior distribution: \\[ p(\\mathbf{\\theta} | \\mathcal{X}, \\mathcal{Y}) = \\frac{p(\\mathcal{Y} | \\mathcal{X}, \\mathbf{\\theta}) \\, p(\\mathbf{\\theta})}{p(\\mathcal{Y} | \\mathcal{X})} \\] The MAP estimate is the maximizer of this posterior: \\[ \\mathbf{\\theta}_{\\text{MAP}} = \\arg \\max_{\\mathbf{\\theta}} \\, p(\\mathbf{\\theta} | \\mathcal{X}, \\mathcal{Y}). \\] Taking the logarithm, we get: \\[ \\log p(\\mathbf{\\theta} | \\mathcal{X}, \\mathcal{Y}) = \\log p(\\mathcal{Y} | \\mathcal{X}, \\mathbf{\\theta}) + \\log p(\\mathbf{\\theta}) + \\text{const.} \\] This shows that the MAP estimate balances: The log-likelihood term (fit to data), and The log-prior term (penalizing unlikely parameters). Hence, MAP can be viewed as a compromise between fitting the data and respecting prior beliefs. 5.2.5 Optimization MAP estimation is equivalent to minimizing the negative log-posterior: \\[ \\mathbf{\\theta}_{\\text{MAP}} = \\arg \\min_{\\mathbf{\\theta}} \\{-\\log p(\\mathcal{Y} | \\mathcal{X}, \\mathbf{\\theta}) - \\log p(\\mathbf{\\theta})\\}. \\] If we assume a Gaussian prior \\(p(\\mathbf{\\theta}) = \\mathcal{N}(\\mathbf{0}, b^2 \\mathbf{I})\\), the negative log-posterior becomes: \\[ - \\log p(\\mathbf{\\theta} | \\mathcal{X}, \\mathcal{Y}) = \\frac{1}{2\\sigma^2}(y - \\mathbf{\\Phi} \\mathbf{\\theta})^T (y - \\mathbf{\\Phi} \\mathbf{\\theta}) + \\frac{1}{2b^2}\\mathbf{\\theta}^T \\mathbf{\\theta} + \\text{const.} \\] Solving for \\(\\mathbf{\\theta}\\) yields the MAP estimate: \\[ \\mathbf{\\theta}_{\\text{MAP}} = \\left(\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\frac{\\sigma^2}{b^2} \\mathbf{I} \\right)^{-1} \\mathbf{\\Phi}^T y. \\] 5.2.6 Comparison to MLE Compared to the MLE solution: \\[ \\mathbf{\\theta}_{\\text{MLE}} = (\\mathbf{\\Phi}^T \\mathbf{\\Phi})^{-1} \\mathbf{\\Phi}^T y, \\] MAP adds the regularization term \\(\\frac{\\sigma^2}{b^2} \\mathbf{I}\\), which: Ensures invertibility of the matrix, Reduces overfitting, Shrinks parameter values toward zero. Example 5.2 In polynomial regression: Using a Gaussian prior \\(p(\\mathbf{\\theta}) = \\mathcal{N}(0, \\mathbf{I})\\), The MAP estimate smooths high-degree polynomials (reducing overfitting), The effect is minimal for low-degree models. However, while MAP helps, it is not a complete solution to overfitting. 5.2.7 MAP Estimation as Regularization Instead of using a prior, we can add a regularization term directly to the loss function: \\[ \\min_{\\mathbf{\\theta}} \\|y - \\mathbf{\\Phi} \\mathbf{\\theta}\\|^2 + \\lambda \\|\\mathbf{\\theta}\\|_2^2. \\] Here: The first term is the data-fit (misfit) term, The second term is the regularizer, which penalizes large parameter values, \\(\\lambda \\ge 0\\) controls the strength of regularization. The regularization term corresponds to the negative log of a Gaussian prior: \\[ -\\log p(\\mathbf{\\theta}) = \\frac{1}{2b^2}\\|\\mathbf{\\theta}\\|_2^2 + \\text{const.} \\] If we set \\(\\lambda = \\frac{\\sigma^2}{b^2}\\), the regularized least squares and MAP estimation solutions are equivalent: \\[ \\mathbf{\\theta}_{\\text{RLS}} = (\\mathbf{\\Phi}^T \\mathbf{\\Phi} + \\lambda I)^{-1} \\mathbf{\\Phi}^T y. \\] Example 5.3 Linear Regression Example (46 data points) We consider a small dataset with 5 observations. Let the input be a scalar \\(x\\) and the response be \\(y\\): \\(x\\) 0 1 2 3 4 \\(y\\) 1 2 2 3 5 We assume Gaussian noise throughout. Model with Nonlinear Features Instead of a purely linear model, we use nonlinear features: \\[ \\phi(x) = \\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\end{bmatrix} \\] The regression model is: \\[ y = \\mathbf{w}^\\top \\phi(x) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2) \\] Let: \\[ \\mathbf{w} = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\end{bmatrix} \\] Design Matrix The design matrix is: \\[ \\mathbf{X} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 4 \\\\ 1 &amp; 3 &amp; 9 \\\\ 1 &amp; 4 &amp; 16 \\end{bmatrix}, \\quad \\mathbf{y} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\\\ 3 \\\\ 5 \\end{bmatrix} \\] Maximum Likelihood Estimation (MLE) The likelihood is: \\[ p(\\mathbf{y} \\mid \\mathbf{X}, \\mathbf{w}) = \\mathcal{N}(\\mathbf{X}\\mathbf{w}, \\sigma^2 \\mathbf{I}) \\] Maximizing the likelihood is equivalent to minimizing: \\[ \\mathcal{L}(\\mathbf{w}) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2 \\] The MLE solution is: \\[ \\hat{\\mathbf{w}}_{\\text{MLE}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y} \\] This shows: MLE = least squares Nonlinear features handled via \\(\\mathbf{X}\\) Regularization (Ridge Regression) To prevent overfitting, add an \\(\\ell_2\\) penalty: \\[ \\mathcal{L}_{\\text{reg}}(\\mathbf{w}) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2 + \\lambda \\|\\mathbf{w}\\|^2 \\] The solution becomes: \\[ \\hat{\\mathbf{w}}_{\\text{ridge}} = (\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\top \\mathbf{y} \\] This is regularized MLE. MAP Estimation Assume a Gaussian prior on the parameters: \\[ p(\\mathbf{w}) = \\mathcal{N}(\\mathbf{0}, \\tau^2 \\mathbf{I}) \\] The posterior is: \\[ p(\\mathbf{w} \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\mathbf{w}) p(\\mathbf{w}) \\] Maximizing the posterior is equivalent to minimizing: \\[ \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2 + \\frac{\\sigma^2}{\\tau^2}\\|\\mathbf{w}\\|^2 \\] Thus the MAP estimator is: \\[ \\hat{\\mathbf{w}}_{\\text{MAP}} = (\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\top \\mathbf{y} \\quad \\text{where } \\lambda = \\frac{\\sigma^2}{\\tau^2} \\] Exercises Exercise 5.5 Why should we maximize the log-likelihood rather than the likelihood? Solution Exercise 5.6 Show \\(p(Y|X, \\mathbf{\\theta}) = \\prod_{n=1}^N N(y_n|\\mathbf{x}_n^T\\mathbf{\\theta}, \\sigma^2)\\). Solution Exercise 5.7 We know that \\[-\\log p(Y|X, \\mathbf{\\theta}) = -\\sum_{n=1}^N \\log p(y_n|\\mathbf{x}_n, \\mathbf{\\theta}).\\] Solution Exercise 5.8 State the formula for \\(p(y_n|\\mathbf{x}_n, \\mathbf{\\theta})\\). Assume that \\(\\sigma^2\\) is known. Use it to show \\[\\log p(y_n|\\mathbf{x}_n, \\mathbf{\\theta}) = -\\dfrac{1}{2\\sigma^2}(y_n - \\mathbf{x}_n^T \\mathbf{\\theta})^2 + const.\\] What is the constant? What does it matter that it has no \\(\\theta\\) in it? Solution Exercise 5.9 Suppose \\(y = 1 + 2x_1 + 3x_2\\), \\(\\mathbf{X} = \\begin{bmatrix}1 &amp; 4 &amp; -3 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}\\), and \\(\\mathbf{y} = \\begin{bmatrix}0\\\\5 \\end{bmatrix}\\). Show that \\[\\dfrac{1}{2\\sigma^2} \\sum_{n=1}^N (y_n - \\mathbf{x}_n^T \\mathbf{\\theta})^2 = \\dfrac{1}{2\\sigma^2} ||\\mathbf{y} - \\mathbf{X}\\mathbf{\\theta}||^2. \\] Solution Exercise 5.10 Define \\(L(\\theta)\\) as the negative log-likelihood function. Show that \\[\\begin{align*} L(\\mathbf{\\theta}) &amp;= \\dfrac{1}{2\\sigma^2} \\sum_{n=1}^N (y_n - \\mathbf{x}_n^T \\mathbf{\\theta})^2 \\\\ &amp;= \\dfrac{1}{2\\sigma^2} ||\\mathbf{y} - \\mathbf{X}\\mathbf{\\theta}||^2. \\end{align*}\\] Solution Exercise 5.11 $ = (- + ^T ^T ) ^{1 x} $. Justify each step. Solution Exercise 5.12 Solve \\(\\dfrac{dL}{d\\mathbf{\\theta}} = 0\\) to find \\(\\mathbf{\\theta}_{ML}\\). Solution Exercise 5.13 To find the MAP estimate, begin with \\[p(\\mathbf{\\theta}|X,Y) = \\dfrac{p(Y|X, \\mathbf{\\theta})p(\\mathbf{\\theta})}{p(Y|X)}.\\] Find the maximum log-likelihood. Solution "],["bayesian-linear-regression.html", "5.3 Bayesian Linear Regression", " 5.3 Bayesian Linear Regression Bayesian linear regression extends the concept of linear regression by treating parameters as random variables instead of estimating a single best set of parameters (as in MLE or MAP). Rather than finding a point estimate for parameters \\(\\mathbf{\\theta}\\), it computes the full posterior distribution over them and uses this distribution to make predictions. This approach: Incorporates prior beliefs about parameters, Naturally accounts for uncertainty, Prevents overfitting, especially with limited data. 5.3.1 The Model We define the model as: \\[ p(\\mathbf{\\theta}) = \\mathcal{N}(\\mathbf{m}_0, \\mathbf{S}_0) \\quad \\text{(prior on parameters)} \\] \\[ p(y | \\mathbf{x}, \\mathbf{\\theta}) = \\mathcal{N}(y | \\phi(\\mathbf{x})^\\top \\mathbf{\\theta}, \\sigma^2) \\quad \\text{(likelihood)}. \\] The joint distribution is: \\[ p(y, \\mathbf{\\theta} | \\mathbf{x}) = p(y | \\mathbf{x}, \\mathbf{\\theta}) \\, p(\\mathbf{\\theta}). \\] Here: \\(\\mathbf{\\theta}\\) is now a random variable, \\(\\mathbf{m}_0\\) and \\(\\mathbf{S}_0\\) are the prior mean and covariance. Predictions are obtained by integrating out the parameter uncertainty: \\[ p(y_* | \\mathbf{x}_*) = \\int p(y_* | \\mathbf{x}_*, \\mathbf{\\theta}) p(\\mathbf{\\theta}) d\\mathbf{\\theta}. \\] Since both likelihood and prior are Gaussian, the predictive distribution is also Gaussian: \\[ p(y_* | \\mathbf{x}_*) = \\mathcal{N}(\\phi(\\mathbf{x}_*)^\\top \\mathbf{m}_0, \\phi(\\mathbf{x}_*)^\\top \\mathbf{S}_0 \\phi(\\mathbf{x}_*) + \\sigma^2) \\] The term \\(\\phi(\\mathbf{x}_*)^\\top \\mathbf{S}_0 \\phi(\\mathbf{x}_*)\\) reflects uncertainty due to parameter variability. The term \\(\\sigma^2\\) reflects observation noise. For noise-free function values \\(f(\\mathbf{x}_*) = \\phi(\\mathbf{x}_*)^\\top \\mathbf{\\theta}\\): \\[ p(f(\\mathbf{x}_*)) = \\mathcal{N}(\\phi(\\mathbf{x}_*)^\\top \\mathbf{m}_0, \\phi(\\mathbf{x}_*)^\\top \\mathbf{S}_0 \\phi(\\mathbf{x}_*)) \\] The parameter prior \\(p(\\mathbf{\\theta})\\) induces a distribution over functions: Each sampled parameter vector \\(\\mathbf{\\theta}_i \\sim p(\\mathbf{\\theta})\\) defines a function \\(f_i(\\mathbf{x}) = \\phi(\\mathbf{x})^\\top \\mathbf{\\theta}_i\\). The collection of these defines \\(p(f(\\cdot))\\), a distribution over possible functions. Example 5.4 Bayesian Linear Regression with a Single Feature We model a simple linear relationship \\[ y = \\theta_0 + \\theta_1 x + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2). \\] Define the feature map \\[ \\phi(x) = \\begin{bmatrix} 1 \\\\ x \\end{bmatrix}, \\quad \\mathbf{\\theta} = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\end{bmatrix}. \\] Then \\[ p(y \\mid x, \\mathbf{\\theta}) = \\mathcal{N}(y \\mid \\phi(x)^\\top \\mathbf{\\theta}, \\sigma^2). \\] Assume a Gaussian prior: \\[ p(\\mathbf{\\theta}) = \\mathcal{N}(\\mathbf{m}_0, \\mathbf{S}_0), \\quad \\mathbf{m}_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{S}_0 = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}. \\] Interpretation: Before seeing data, we believe slopes and intercepts are near 0. Large uncertainty allows many possible linear functions. Each draw \\(\\mathbf{\\theta}_i \\sim p(\\mathbf{\\theta})\\) defines a function \\[ f_i(x) = \\theta_{0,i} + \\theta_{1,i} x. \\] Thus the prior defines a distribution over lines. For a new input \\(x_*\\), \\[ p(y_* \\mid x_*) = \\mathcal{N} \\left( \\phi(x_*)^\\top \\mathbf{m}_0,\\; \\phi(x_*)^\\top \\mathbf{S}_0 \\phi(x_*) + \\sigma^2 \\right). \\] Because \\(\\mathbf{m}_0 = \\mathbf{0}\\): \\[ \\mathbb{E}[y_*] = 0. \\] The variance \\[ \\phi(x_*)^\\top \\mathbf{S}_0 \\phi(x_*) = 1 + x_*^2 \\] grows with distance from the origin  uncertainty increases away from data. 5.3.2 Posterior Distribution After observing training data \\((\\mathcal{X}, \\mathcal{Y})\\), we compute the posterior over parameters: \\[ p(\\mathbf{\\theta} | \\mathcal{X}, \\mathcal{Y}) = \\frac{p(\\mathcal{Y} | \\mathcal{X}, \\mathbf{\\theta}) p(\\mathbf{\\theta})}{p(\\mathcal{Y} | \\mathcal{X})} \\] Where: \\(p(\\mathcal{Y} | \\mathcal{X}, \\mathbf{\\theta}) = \\mathcal{N}(\\mathcal{Y} | \\mathbf{\\Phi} \\mathbf{\\theta}, \\sigma^2 I)\\) \\(p(\\mathcal{Y} | \\mathcal{X}) = \\int p(\\mathcal{Y} | \\mathcal{X}, \\mathbf{\\theta}) p(\\mathbf{\\theta}) d\\mathbf{\\theta}\\) is the marginal likelihood (normalizing constant). Because both prior and likelihood are Gaussian, the posterior is also Gaussian: \\[ p(\\mathbf{\\theta} | X, Y) = \\mathcal{N}(\\mathbf{\\theta} | \\mathbf{m}_N, \\mathbf{S}_N), \\] with \\[ \\mathbf{S}_N = (\\mathbf{S}_0^{-1} + \\sigma^{-2} \\mathbf{\\Phi}^\\top \\mathbf{\\Phi})^{-1}, \\quad \\mathbf{m}_N = \\mathbf{S}_N (\\mathbf{S}_0^{-1} \\mathbf{m}_0 + \\sigma^{-2} \\mathbf{\\Phi}^\\top Y). \\] This is derived by completing the square in the exponent of the unnormalized posterior. Example 5.5 Posterior After Observing Data Suppose we observe: \\[ \\mathcal{X} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathcal{Y} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix}. \\] The design matrix is \\[ \\mathbf{\\Phi} = \\begin{bmatrix} 1 &amp; 0 \\\\ 1 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix}. \\] Because the prior and likelihood are Gaussian, the posterior is: \\[ p(\\mathbf{\\theta} \\mid \\mathcal{X}, \\mathcal{Y}) = \\mathcal{N}(\\mathbf{m}_N, \\mathbf{S}_N), \\] with \\[ \\mathbf{S}_N = (\\mathbf{S}_0^{-1} + \\sigma^{-2} \\mathbf{\\Phi}^\\top \\mathbf{\\Phi})^{-1}, \\quad \\mathbf{m}_N = \\mathbf{S}_N (\\mathbf{S}_0^{-1} \\mathbf{m}_0 + \\sigma^{-2} \\mathbf{\\Phi}^\\top \\mathcal{Y}). \\] Interpretation: Posterior mean \\(\\mathbf{m}_N\\): best estimate of parameters (MAP). Posterior covariance \\(\\mathbf{S}_N\\): remaining uncertainty after seeing data. Uncertainty shrinks in directions well-supported by data. 5.3.3 Posterior Predictions To predict at a new point \\(\\mathbf{x}_*\\), we again integrate over \\(\\mathbf{\\theta}\\), but now using the posterior instead of the prior: \\[ p(y_* | \\mathcal{X}, \\mathcal{Y}, \\mathbf{x}_*) = \\int p(y_* | \\mathbf{x}_*, \\mathbf{\\theta}) p(\\mathbf{\\theta} | \\mathcal{X}, \\mathcal{Y}) d\\mathbf{\\theta}. \\] This yields: \\[ p(y_* | \\mathcal{X}, \\mathcal{Y}, \\mathbf{x}_*) = \\mathcal{N}(\\phi(\\mathbf{x}_*)^\\top \\mathbf{m}_N, \\phi(\\mathbf{x}_*)^\\top \\mathbf{S}_N \\phi(\\mathbf{x}_*) + \\sigma^2). \\] The predictive mean \\(\\phi(\\mathbf{x}_*)^\\top \\mathbf{m}_N\\) equals the MAP estimate prediction. The predictive variance accounts for both model and observation uncertainty. For \\(f(\\mathbf{x}_*) = \\phi(\\mathbf{x}_*)^\\top \\mathbf{\\theta}\\): \\[ \\mathbb{E}[f(\\mathbf{x}_*) | \\mathcal{X}, \\mathcal{Y}] = \\phi(\\mathbf{x}_*)^\\top \\mathbf{m}_N, \\quad \\text{Var}[f(\\mathbf{x}_*) | \\mathcal{X}, \\mathcal{Y}] = \\phi(\\mathbf{x}_*)^\\top \\mathbf{S}_N \\phi(\\mathbf{x}_*) \\] Sampling \\(\\mathbf{\\theta}_i \\sim p(\\mathbf{\\theta} | \\mathcal{X}, \\mathcal{Y})\\) gives functions \\(f_i(\\mathbf{x}) = \\phi(\\mathbf{x})^\\top \\mathbf{\\theta}_i\\). These represent the posterior distribution over functions, with: Mean function: \\(\\mathbf{m}_N^\\top \\phi(\\mathbf{x})\\), Variance: \\(\\phi(\\mathbf{x})^\\top \\mathbf{S}_N \\phi(\\mathbf{x})\\). Interpretation: Higher uncertainty (larger variance) reflects regions with sparse or no data. Example 5.6 Posterior Predictive Distribution For a new input \\(x_* = 1.5\\): \\[ p(y_* \\mid \\mathcal{X}, \\mathcal{Y}, x_*) = \\mathcal{N} \\left( \\phi(x_*)^\\top \\mathbf{m}_N,\\; \\phi(x_*)^\\top \\mathbf{S}_N \\phi(x_*) + \\sigma^2 \\right). \\] This variance has two components: \\(\\phi(x_*)^\\top \\mathbf{S}_N \\phi(x_*)\\): parameter uncertainty \\(\\sigma^2\\): observation noise Even with infinite data, noise remains. Example 5.7 Noise-Free Function Values For the latent function \\[ f(x_*) = \\phi(x_*)^\\top \\mathbf{\\theta}, \\] we have \\[ p(f(x_*) \\mid \\mathcal{X}, \\mathcal{Y}) = \\mathcal{N} \\left( \\phi(x_*)^\\top \\mathbf{m}_N,\\; \\phi(x_*)^\\top \\mathbf{S}_N \\phi(x_*) \\right). \\] This is a distribution over functions, not just numbers. 5.3.4 Marginal Likelihood The marginal likelihood (or evidence) measures how well the model explains the observed data: \\[ p(\\mathcal{Y} | \\mathcal{X}) = \\int p(\\mathcal{Y} |\\mathcal{X}, \\mathbf{\\theta}) p(\\mathbf{\\theta}) d\\mathbf{\\theta}. \\] Using Gaussian conjugacy, the result is: \\[ p(\\mathcal{Y} | \\mathcal{X}) = \\mathcal{N}(\\mathbf{y} | \\mathbf{X} \\mathbf{m}_0, \\mathbf{X} \\mathbf{S}_0 \\mathbf{X}^\\top + \\sigma^2 \\mathbf{I}), \\] with: \\[ \\mathbb{E}[\\mathcal{Y} | \\mathcal{X}] = \\mathbf{X} \\mathbf{m}_0, \\quad \\text{Cov}[\\mathcal{Y} | \\mathcal{X}] = \\mathbf{X} \\mathbf{S}_0 X^\\top + \\sigma^2 I \\] The marginal likelihood is crucial for model comparison and selection, as it measures the models fit after integrating over parameter uncertainty. Example 5.8 Marginal Likelihood (Evidence) The marginal likelihood integrates out parameters: \\[ p(\\mathcal{Y} \\mid \\mathcal{X}) = \\int p(\\mathcal{Y} \\mid \\mathcal{X}, \\mathbf{\\theta}) p(\\mathbf{\\theta}) d\\mathbf{\\theta}. \\] Result: \\[ p(\\mathcal{Y} \\mid \\mathcal{X}) = \\mathcal{N} (\\mathcal{Y} \\mid \\mathbf{\\Phi} \\mathbf{m}_0, \\mathbf{\\Phi} \\mathbf{S}_0 \\mathbf{\\Phi}^\\top + \\sigma^2 \\mathbf{I}). \\] Interpretation: Measures how well the model explains data on average over parameters Automatically penalizes overly flexible models Central to Bayesian model selection 5.3.4.1 Key Insights Concept Description Parameter Treatment Bayesian regression treats parameters \\(\\mathbf{\\theta}\\) as random variables. Prior  Posterior Prior \\(p(\\mathbf{\\theta})\\) is updated using data to yield posterior \\(p(\\mathbf{\\theta} | \\mathcal{X}, \\mathcal{Y})\\). Predictions Are averages over all plausible parameter values (not point estimates). Uncertainty Captured via predictive variance combining model and observation noise. Marginal Likelihood Used for model evidence and Bayesian model selection. 5.3.4.2 Summary of Distributions Distribution Epression Description Prior \\(p(\\mathbf{\\theta}) = \\mathcal{N}(\\mathbf{m}_0, \\mathbf{S}_0)\\) Beliefs before seeing data Likelihood \\(p(\\mathcal{Y} | \\mathcal{X}, \\mathbf{\\theta}) = \\mathcal{N}(Y | \\mathbf{\\Phi} \\mathbf{\\theta}, \\sigma^2 I)\\) Data model Posterior \\(p(\\mathbf{\\theta} | \\mathcal{X}, \\mathcal{Y}) = \\mathcal{N}(\\mathbf{\\theta} | \\mathbf{m}_N, \\mathbf{S}_N)\\) Updated parameter beliefs Predictive \\(p(y_* | \\mathcal{X}, \\mathcal{Y}, \\mathbf{x}_*) = \\mathcal{N}(\\phi(\\mathbf{x}_*)^\\top \\mathbf{m}_N, \\phi(\\mathbf{x}_*)^\\top \\mathbf{S}_N \\phi(\\mathbf{x}_*) + \\sigma^2)\\) Predictions at new points Marginal Likelihood \\(p(\\mathcal{Y} | \\mathcal{X}) = \\mathcal{N}(\\mathcal{Y} | \\mathbf{X} \\mathbf{m}_0, \\mathbf{X} \\mathbf{S}_0 \\mathbf{X} ^\\top + \\sigma^2 \\mathbf{I} )\\) Model evidence Exercises Put some exercises here. "],["maximum-likelihood-as-orthogonal-projection.html", "5.4 Maximum Likelihood as Orthogonal Projection", " 5.4 Maximum Likelihood as Orthogonal Projection Maximum Likelihood Estimation (MLE) in linear regression has a clear geometric interpretation. It corresponds to the orthogonal projection of the target vector \\(\\mathbf{y}\\) onto the subspace spanned by the input data. 5.4.1 Simple Linear Regression Case Consider the model: \\[ y = \\mathbf{x}\\mathbf{\\theta} + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2). \\] Given training data \\(\\{(x_1, y_1), \\ldots, (x_N, y_N)\\}\\), the MLE for \\(\\theta\\) is: \\[ \\theta_{ML} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y} = \\frac{\\mathbf{X}^\\top \\mathbf{y}}{\\mathbf{X}^\\top \\mathbf{X}} \\] where: \\(\\mathbf{X} = [x_1, \\ldots, x_N]^\\top \\in \\mathbb{R}^N\\), \\(\\mathbf{y} = [y_1, \\ldots, y_N]^\\top \\in \\mathbb{R}^N\\). This gives the fitted (reconstructed) outputs: \\[ \\mathbf{X} \\theta_{ML} = \\frac{\\mathbf{X}\\mathbf{X}^\\top}{\\mathbf{X}^\\top \\mathbf{X}} \\mathbf{y} \\] Geometrically, we can view the matrix \\[ P = \\frac{\\mathbf{X}\\mathbf{X}^\\top}{\\mathbf{X}^\\top \\mathbf{X}}. \\] as the projection matrix onto the one-dimensional subspace spanned by \\(\\mathbf{X}\\). The MLE solution \\(\\mathbf{X}\\theta_{ML}\\) is thus the orthogonal projection of \\(\\mathbf{y}\\) onto this subspace. This projection minimizes the squared distance between \\(\\mathbf{y}\\) and the model predictions \\(\\mathbf{X}\\theta\\): \\[ \\min_{\\theta} \\|\\mathbf{y} - \\mathbf{X}\\theta\\|^2. \\] Hence, MLE not only gives the best statistical fit (in the least-squares sense) but also the geometrically optimal fit  the closest point to \\(\\mathbf{y}\\) within the space of linear combinations of \\(\\mathbf{X}\\). 5.4.2 General Linear Regression Case For the more general model: \\[ y = \\phi(\\mathbf{x})^\\top \\theta + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2), \\] where \\(\\phi(\\mathbf{x}) \\in \\mathbb{R}^K\\) is a vector of feature functions, the results extend naturally. The MLE estimate is: \\[ \\theta_{ML} = (\\Phi^\\top \\Phi)^{-1} \\Phi^\\top y, \\] and the fitted outputs are: \\[ \\mathbf{y} \\approx \\Phi \\theta_{ML}. \\] Here: \\(\\Phi \\in \\mathbb{R}^{N \\times K}\\) is the feature matrix, The column space of \\(\\Phi\\) defines a K-dimensional subspace of \\(\\mathbb{R}^N\\), The projection matrix is: \\[ P = \\Phi (\\Phi^\\top \\Phi)^{-1} \\Phi^\\top. \\] Thus, MLE corresponds to orthogonal projection of \\(\\mathbf{y}\\) onto the subspace spanned by the columns of \\(\\Phi\\). 5.4.3 Special Case: Orthonormal Basis If the feature vectors \\(\\phi_k\\) form an orthonormal basis, then: \\[ \\Phi^\\top \\Phi = I \\] and the projection simplifies to: \\[ P = \\Phi \\Phi^\\top = \\sum_{k=1}^K \\phi_k \\phi_k^\\top. \\] In this case: The projection of \\(\\mathbf{y}\\) is simply the sum of individual projections onto each basis vector. The coupling between features disappears because of orthogonality. Example 5.9 Fourier bases and wavelets are examples of orthogonal bases commonly used in signal processing. Exercises Put some exercises here. "],["dimensionality-reduction-with-principal-component-analysis-pca.html", "Chapter 6 Dimensionality Reduction with Principal Component Analysis (PCA)", " Chapter 6 Dimensionality Reduction with Principal Component Analysis (PCA) High-dimensional data, such as images, often pose challenges for analysis, visualization, and storage. For instance, a 640Ã—480 color image represents a point in a million-dimensional space (three dimensions per pixel). Despite this complexity, high-dimensional data usually contains redundancy and correlation, allowing it to be represented in a lower-dimensional space with minimal loss of information. Dimensionality reduction leverages these redundancies to produce a more compact representation of the datasimilar to compression techniques like JPEG (for images) or MP3 (for audio). "],["principal-component-analysis-pca.html", "6.1 Principal Component Analysis (PCA)", " 6.1 Principal Component Analysis (PCA) Principal Component Analysis (PCA) is a method for linear dimensionality reduction. The idea was originally proposed by Pearson (1901) and Hotelling (1933). In signal processing, PCA is also known as the KarhunenLoÃ¨ve Transform. PCA is used for data compression, visualization, and identifying patterns or latent factors in high-dimensional data. To begin, PCA assumes that high-dimensional data lies approximately on a low-dimensional subspace. By projecting data onto this subspace, PCA reduces dimensionality while preserving as much variance (information) as possible. Let: \\(\\mathcal{X} = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}\\), with \\(\\mathbf{x}_n \\in \\mathbb{R}^D\\) (mean-centered data) The covariance matrix be defined as: \\[ \\mathbf{S} = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_n \\mathbf{x}_n^\\top \\] A low-dimensional representation (code): \\[ \\mathbf{z}_n = \\mathbf{B}^\\top \\mathbf{x}_n \\in \\mathbb{R}^M, \\quad M &lt; D \\] The projection matrix: \\[ \\mathbf{B} = [\\mathbf{b}_1, \\dots, \\mathbf{b}_M] \\in \\mathbb{R}^{D \\times M} \\] where the columns \\(\\mathbf{b}_i\\) are orthonormal: \\(\\mathbf{b}_i^\\top \\mathbf{b}_j = 0\\) if \\(i \\neq j\\), and \\(\\mathbf{b}_i^\\top \\mathbf{b}_i = 1\\). The projected data is given by: \\[ \\tilde{\\mathbf{x}}_n = \\mathbf{B}\\mathbf{B}^\\top \\mathbf{x}_n \\in \\mathbb{R}^D \\] The goal of PCA is to find \\(\\mathbf{B}\\) and \\(\\mathbf{z}_n\\) that minimize information loss between \\(\\mathbf{x}_n\\) and \\(\\tilde{\\mathbf{x}}_n\\). Example 6.1 In \\(\\mathbb{R}^2\\), the canonical basis is: \\[ \\mathbf{e}_1 = [1, 0]^\\top, \\quad \\mathbf{e}_2 = [0, 1]^\\top \\] A point \\(\\mathbf{x} = [5, 3]^\\top = 5\\mathbf{e}_1 + 3\\mathbf{e}_2\\) can be represented in this basis. If we project onto the subspace spanned by \\(\\mathbf{e}_2\\): \\[ \\tilde{\\mathbf{x}} = [0, z]^\\top = z\\mathbf{e}_2 \\] then only the coordinate \\(z\\) needs to be storedrepresenting a 1D subspace of \\(\\mathbb{R}^2\\). This illustrates dimensionality reduction: the original 2D vector is represented by a single coordinate in a 1D subspace. 6.1.1 Compression Interpretation PCA can be viewed as a data compression system: Encoder: \\(\\mathbf{z} = \\mathbf{B}^\\top \\mathbf{x}\\) (projects data to lower-dimensional space) Decoder: \\(\\tilde{\\mathbf{x}} = \\mathbf{B}\\mathbf{z}\\) (reconstructs data from its low-dimensional code) The matrix \\(\\mathbf{B}\\) defines both the encoding and decoding transformations. Example 6.2 MNIST Digits PCA is often demonstrated using the MNIST dataset, which contains 60,000 grayscale images of handwritten digits (09). Each image has 28Ã—28 pixels = 784 dimensions. Thus, each image can be represented as a vector \\(\\mathbf{x} \\in \\mathbb{R}^{784}\\). By applying PCA, we can compress and visualize the intrinsic lower-dimensional structure of this dataset. Exercises Put some exercises here. "],["maximum-variance-perspective.html", "6.2 Maximum Variance Perspective", " 6.2 Maximum Variance Perspective Principal Component Analysis (PCA) can be derived from the idea that the most informative directions in data are those with the highest variance. When reducing dimensionality, we aim to retain as much of this variance (and hence information) as possible. In essence, PCA finds a low-dimensional subspace that maximizes the variance of the projected data. For simplicity, we assume that the dataset is centered (mean = 0). This assumption can be made without loss of generality because shifting the data by its mean does not affect its variance: \\[ V_z[\\mathbf{z}] = V_x[\\mathbf{B}^\\top(\\mathbf{x} - \\boldsymbol{\\mu})] = V_x[\\mathbf{B}^\\top \\mathbf{x}]. \\] Therefore, we can safely assume that both the data \\(\\mathbf{x}\\) and the low-dimensional code \\(\\mathbf{z} = \\mathbf{B}^\\top \\mathbf{x}\\) have zero mean. 6.2.1 Direction with Maximal Variance We start by finding a single direction \\(\\mathbf{b}_1 \\in \\mathbb{R}^D\\) along which the variance of the projected data is maximized. For centered data: \\[ z_{1n} = \\mathbf{b}_1^\\top \\mathbf{x}_n \\] and the variance of the projection is: \\[ V_1 = \\frac{1}{N} \\sum_{n=1}^{N} (\\mathbf{b}_1^\\top \\mathbf{x}_n)^2 = \\mathbf{b}_1^\\top \\mathbf{S} \\mathbf{b}_1, \\] where \\(\\mathbf{S}\\) is the data covariance matrix. To prevent arbitrary scaling of \\(\\mathbf{b}_1\\), we impose the constraint: \\[ \\|\\mathbf{b}_1\\|^2 = 1. \\] This leads to the constrained optimization problem: \\[ \\max_{\\mathbf{b}_1} \\; \\mathbf{b}_1^\\top \\mathbf{S} \\mathbf{b}_1 \\quad \\text{subject to } \\|\\mathbf{b}_1\\|^2 = 1. \\] Using a Lagrangian formulation: \\[ L(\\mathbf{b}_1, \\lambda_1) = \\mathbf{b}_1^\\top \\mathbf{S} \\mathbf{b}_1 + \\lambda_1(1 - \\mathbf{b}_1^\\top \\mathbf{b}_1). \\] Setting derivatives to zero gives: \\[ \\mathbf{S} \\mathbf{b}_1 = \\lambda_1 \\mathbf{b}_1. \\] Thus, \\(\\mathbf{b}_1\\) is an eigenvector of \\(\\mathbf{S}\\), and \\(\\lambda_1\\) is the corresponding eigenvalue. The variance of the projection onto \\(\\mathbf{b}_1\\) is: \\[ V_1 = \\lambda_1. \\] Hence, the first principal component is the eigenvector corresponding to the largest eigenvalue of \\(\\mathbf{S}.\\) The projected data point can be reconstructed as: \\[ \\tilde{\\mathbf{x}}_n = \\mathbf{b}_1 \\mathbf{b}_1^\\top \\mathbf{x}_n. \\] 6.2.2 M-Dimensional Subspace with Maximal Variance To generalize, we seek an M-dimensional subspace spanned by orthonormal vectors: \\[ \\mathbf{B} = [\\mathbf{b}_1, \\mathbf{b}_2, \\dots, \\mathbf{b}_M], \\] where each \\(\\mathbf{b}_m\\) is an eigenvector of \\(\\mathbf{S}\\) associated with one of its largest eigenvalues. After finding the first \\(m-1\\) principal components, we can define the remaining (unexplained) data as: \\[ \\hat{\\mathbf{X}} = \\mathbf{X} - \\sum_{i=1}^{m-1} \\mathbf{b}_i \\mathbf{b}_i^\\top \\mathbf{X} = \\mathbf{X} - \\mathbf{B}_{m-1} \\mathbf{X}, \\] and seek the next direction \\(\\mathbf{b}_m\\) that maximizes the variance of the residual data. This again leads to: \\[ \\mathbf{S} \\mathbf{b}_m = \\lambda_m \\mathbf{b}_m, \\] where \\(\\lambda_m\\) is the m-th largest eigenvalue of \\(\\mathbf{S}\\). Thus, each principal component corresponds to an eigenvector of the covariance matrix. The variance captured by the m-th component is: \\[ V_m = \\mathbf{b}_m^\\top \\mathbf{S} \\mathbf{b}_m = \\lambda_m. \\] Theorem 6.1 The total variance captured by the first \\(M\\) principal components is: \\[ V_M = \\sum_{m=1}^{M} \\lambda_m. \\] The variance lost (due to compression) is: \\[ J_M = \\sum_{j=M+1}^{D} \\lambda_j = V_D - V_M. \\] Alternatively, the relative variance captured is: \\[ \\frac{V_M}{V_D}, \\] and the relative variance lost is: \\[ 1 - \\frac{V_M}{V_D}. \\] Example 6.3 If we project the point \\(\\mathbf{x} = [5, 3]^\\top = 5\\mathbf{e}_1 + 3\\mathbf{e}_2\\) onto the subspace spanned by \\(\\mathbf{e}_2\\): \\[ \\tilde{\\mathbf{x}} = [0, z]^\\top = z\\mathbf{e}_2 = [0, 3]^\\top \\] We can calculate the error as \\[ \\mathbf{x} - \\tilde{\\mathbf{x}} = [5, 3]^\\top [0, 3]^\\top = [5, 0]^\\top.\\] If this projection came from the covariance matrix of a large data set with 2 eigenvalues 6 and 4, then this projection represents 60% (\\(6/(6+4)\\)) of the variance in the projection while the lost variance is 40%. In essence, 60% of the variability of the data associated with a projection of this type would be a result of the second component. Example 6.4 When PCA is applied to the MNIST images of the digit 8: The eigenvalues of the data covariance matrix decrease rapidly. Most variance is captured by only a small number of principal components. This demonstrates that the dataset has an intrinsic low-dimensional structure. Exercises Exercise 6.1 Show that the variance of low dimensional code does not depend on the mean of the data. This allows us to assume our data has mean 0. Solution Exercise 6.2 Consider the constrained optimization problem \\[\\begin{align*} &amp;\\max_{\\mathbf{b}_1} \\mathbf{b}_1^T\\mathbf{S}\\mathbf{b}_1\\\\&amp; \\text{subject to} ||\\mathbf{b}_1||^2 = 1. \\end{align*}\\] Derive the necessary Lagrangian conditions for this optimization problem. Solution Exercise 6.3 Explain why we maximize the variance when we select the basis vector that is associated with the largest eigenvalue. Solution "],["projection-perspective.html", "6.3 Projection Perspective", " 6.3 Projection Perspective Principal Component Analysis (PCA) can also be viewed as finding the best low-dimensional linear projection of the data that minimizes the average reconstruction error between the original data and its projection. This interpretation connects PCA with optimal linear autoencoders. 6.3.1 Setting and Objective Any data vector \\(\\mathbf{x} \\in \\mathbb{R}^D\\) can be expressed as a linear combination of orthonormal basis (ONB) vectors \\((\\mathbf{b}_1, \\dots, \\mathbf{b}_D)\\): \\[ \\mathbf{x} = \\sum_{d=1}^{D} \\zeta_d \\mathbf{b}_d. \\] We seek an approximation \\(\\tilde{\\mathbf{x}}\\) in a lower-dimensional subspace \\(U \\subseteq \\mathbb{R}^D\\) of dimension \\(M &lt; D\\): \\[ \\tilde{\\mathbf{x}} = \\sum_{m=1}^{M} z_m \\mathbf{b}_m. \\] The goal is to make \\(\\mathbf{x}\\) and \\(\\tilde{\\mathbf{x}}\\) as close as possible by minimizing the average squared Euclidean distance: \\[ J_M = \\frac{1}{N} \\sum_{n=1}^{N} \\| \\mathbf{x}_n - \\tilde{\\mathbf{x}}_n \\|^2. \\] This quantity is known as the reconstruction error. 6.3.2 Finding Optimal Coordinates For a fixed basis \\((\\mathbf{b}_1, \\dots, \\mathbf{b}_M)\\), the optimal coordinates of the projection are given by the orthogonal projection: \\[ z_{in} = \\mathbf{b}_i^\\top \\mathbf{x}_n. \\] Hence, the optimal projection of \\(\\mathbf{x}_n\\) is: \\[ \\tilde{\\mathbf{x}}_n = \\mathbf{B} \\mathbf{B}^\\top \\mathbf{x}_n. \\] where \\(\\mathbf{B} = [\\mathbf{b}_1, \\dots, \\mathbf{b}_M]\\). Interpretation: PCAs projection step is an orthogonal projection of the data onto the principal subspace spanned by the top \\(M\\) basis vectors. 6.3.3 Finding the Basis of the Principal Subspace Rewriting the reconstruction error using projection matrices: \\[ J_M = \\frac{1}{N} \\sum_{n=1}^{N} \\| \\mathbf{x}_n - \\tilde{\\mathbf{x}}_n \\|^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\| ( \\mathbf{I} - \\mathbf{B}\\mathbf{B}^\\top ) \\mathbf{x}_n \\|^2. \\] \\(\\mathbf{B}\\mathbf{B}^\\top\\) is a rank-\\(M\\) symmetric matrix  the best rank-\\(M\\) approximation of the identity matrix \\(\\mathbf{I}\\). The reconstruction error can also be expressed as: \\[ J_M = \\sum_{j=M+1}^{D} \\mathbf{b}_j^\\top \\mathbf{S} \\mathbf{b}_j = \\sum_{j=M+1}^{D} \\lambda_j, \\] where \\(\\mathbf{S}\\) is the data covariance matrix and \\(\\lambda_j\\) are its eigenvalues. Example 6.5 Applying PCA to the MNIST digits 0 and 1: Data embedded in 2D (using top two principal components) shows clear class separation. Digits 0 show greater internal variation than digits 1. Exercises Put some exercises here. "],["eigenvector-computation-and-low-rank-approximations.html", "6.4 Eigenvector Computation and Low-Rank Approximations", " 6.4 Eigenvector Computation and Low-Rank Approximations 6.4.1 Computing Eigenvectors via Covariance and SVD The principal subspace basis of PCA is obtained from the eigenvectors corresponding to the largest eigenvalues of the data covariance matrix: \\[ \\mathbf{S} = \\frac{1}{N} \\sum_{n=1}^N \\mathbf{x}_n \\mathbf{x}_n^\\top = \\frac{1}{N} \\mathbf{X} \\mathbf{X}^\\top, \\quad \\mathbf{X} \\in \\mathbb{R}^{D \\times N}. \\] Two equivalent approaches exist: Eigen-decomposition of \\(\\mathbf{S}\\) - Compute eigenvalues and eigenvectors directly from \\(\\mathbf{S}\\). Singular Value Decomposition (SVD) of \\(\\mathbf{X}\\) \\[ \\mathbf{X} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^\\top. \\] \\(\\mathbf{U} \\in \\mathbb{R}^{D \\times D}\\) and \\(\\mathbf{V} \\in \\mathbb{R}^{N \\times N}\\) are orthogonal. \\(\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{D \\times N}\\) holds the singular values \\(\\sigma_i \\ge 0\\). From this decomposition: \\[ \\mathbf{S} = \\frac{1}{N} \\mathbf{U} \\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma}^\\top \\mathbf{U}^\\top. \\] Hence: Columns of \\(\\mathbf{U}\\) are eigenvectors of \\(\\mathbf{S}\\). Eigenvalues of \\(\\mathbf{S}\\) are related to singular values by: \\[ \\lambda_d = \\frac{\\sigma_d^2}{N}. \\] This connects the maximum variance view of PCA with the SVD perspective. 6.4.2 PCA via Low-Rank Matrix Approximations PCA can be viewed as finding the best rank-\\(M\\) approximation to \\(\\mathbf{X}\\) that minimizes reconstruction error. Theorem 6.2 EckartYoung theorem: \\[ \\tilde{\\mathbf{X}}_M = \\arg\\min_{\\text{rank}(A) \\le M} \\| \\mathbf{X} - A \\|_2. \\] The optimal low-rank approximation is obtained by truncating the SVD: \\[ \\tilde{\\mathbf{X}}_M = \\mathbf{U}_M \\boldsymbol{\\Sigma}_M \\mathbf{V}_M^\\top, \\] where: \\(\\mathbf{U}_M = [\\mathbf{u}_1, \\dots, \\mathbf{u}_M] \\in \\mathbb{R}^{D \\times M}\\) \\(\\mathbf{V}_M = [\\mathbf{v}_1, \\dots, \\mathbf{v}_M] \\in \\mathbb{R}^{N \\times M}\\) \\(\\boldsymbol{\\Sigma}_M \\in \\mathbb{R}^{M \\times M}\\) contains the top \\(M\\) singular values. This provides the optimal projection matrix for PCA and directly yields the principal components. 6.4.3 Practical Aspects of Eigenvalue Computation While eigenvalues can theoretically be found by solving the characteristic polynomial, the AbelRuffini theorem states that general algebraic solutions for polynomials of degree  5 do not exist. In practice: Numerical methods (e.g., np.linalg.eigh, np.linalg.svd) compute eigenvalues iteratively. When only a few leading eigenvectors are needed, iterative methods are more efficient than full decompositions. A simple iterative algorithm to compute the dominant eigenvector: \\[ \\mathbf{x}_{k+1} = \\frac{\\mathbf{S} \\mathbf{x}_k}{\\| \\mathbf{S} \\mathbf{x}_k \\|}. \\] This converges to the eigenvector corresponding to the largest eigenvalue. The original Google PageRank algorithm used a similar approach. Exercises Put some exercises here. "],["pca-in-high-dimensions.html", "6.5 PCA in High Dimensions", " 6.5 PCA in High Dimensions For high-dimensional data, computing the \\(D \\times D\\) covariance matrix and its eigen-decomposition is computationally expensive, scaling cubically with \\(D\\). When the number of samples is much smaller than the number of features (\\(N \\ll D\\)), most eigenvalues are zero, and the covariance matrix \\(\\mathbf{S}\\) has rank \\(N\\). 6.5.1 Dimensionality Reduction Trick for \\(N \\ll D\\) Given centered data \\(\\mathbf{X} = [\\mathbf{x}_1, \\dots, \\mathbf{x}_N] \\in \\mathbb{R}^{D \\times N}\\), the covariance matrix is: \\[ \\mathbf{S} = \\frac{1}{N} \\mathbf{X} \\mathbf{X}^\\top. \\] The eigenvector equation is: \\[ \\mathbf{S} \\mathbf{b}_m = \\lambda_m \\mathbf{b}_m. \\] Multiplying by \\(\\mathbf{X}^\\top\\) gives: \\[ \\frac{1}{N} \\mathbf{X}^\\top \\mathbf{X} \\mathbf{c}_m = \\lambda_m \\mathbf{c}_m, \\quad \\text{where } \\mathbf{c}_m = \\mathbf{X}^\\top \\mathbf{b}_m. \\] Thus: \\(\\mathbf{X}^\\top \\mathbf{X} / N\\) (size \\(N \\times N\\)) shares the same nonzero eigenvalues as \\(\\mathbf{S}\\). We can compute eigenvectors more efficiently using this smaller matrix. To recover the original eigenvectors of \\(\\mathbf{S}\\): \\[ \\mathbf{b}_m = \\mathbf{X} \\mathbf{c}_m. \\] Finally, normalize \\(\\mathbf{b}_m\\) to have unit norm for use in PCA. Exercises Put some exercises here. "],["key-steps-of-pca-in-practice.html", "6.6 Key Steps of PCA in Practice", " 6.6 Key Steps of PCA in Practice PCA consists of several main computational steps to identify and project data onto a lower-dimensional subspace. 1. Mean Subtraction Center the data by subtracting the mean vector: \\[ \\boldsymbol{\\mu} = \\frac{1}{N} \\sum_{n=1}^N \\mathbf{x}_n, \\quad \\mathbf{x}_n \\leftarrow \\mathbf{x}_n - \\boldsymbol{\\mu}. \\] This ensures the dataset has zero mean, which improves numerical stability. 2. Standardization Divide each dimension by its standard deviation \\(\\sigma_d\\): \\[ x_{n}^{(d)} \\leftarrow \\frac{x_{n}^{(d)} - \\mu_d}{\\sigma_d}, \\quad d = 1, \\dots, D. \\] This step removes scale dependence and gives each feature unit variance. 3. Eigendecomposition of the Covariance Matrix Compute the covariance matrix: \\[ \\mathbf{S} = \\frac{1}{N} \\mathbf{X} \\mathbf{X}^\\top. \\] Then find its eigenvalues and eigenvectors: \\[ \\mathbf{S} \\mathbf{u}_i = \\lambda_i \\mathbf{u}_i. \\] The eigenvectors form an orthonormal basis (ONB), and the one with the largest eigenvalue defines the principal subspace. 4. Projection To project a new data point \\(\\mathbf{x}_\\ast\\) onto the principal subspace: Standardize using the training mean and standard deviation: \\[ x_\\ast^{(d)} \\leftarrow \\frac{x_\\ast^{(d)} - \\mu_d}{\\sigma_d}. \\] Project: \\[ \\tilde{\\mathbf{x}}_\\ast = \\mathbf{B}\\mathbf{B}^\\top \\mathbf{x}_\\ast. \\] where \\(\\mathbf{B}\\) contains the top eigenvectors as columns. Obtain low-dimensional coordinates: \\[ \\mathbf{z}_\\ast = \\mathbf{B}^\\top \\mathbf{x}_\\ast. \\] To recover the projection in the original data space: \\[ \\tilde{x}_\\ast^{(d)} \\leftarrow \\tilde{x}_\\ast^{(d)} \\sigma_d + \\mu_d. \\] Example 6.6 PCA applied to MNIST digits (e.g., digit 8) shows that: With few components (e.g., 110 PCs), reconstructions are blurry. Increasing PCs (up to ~500) yields near-perfect reconstructions. The reconstruction error: \\[ \\frac{1}{N}\\sum_{n=1}^N \\| \\mathbf{x}_n - \\tilde{\\mathbf{x}}_n \\|^2 = \\sum_{i=M+1}^{D} \\lambda_i \\] decreases sharply with \\(M\\) until most variance is captured by a small number of components. Example 6.7 A Simple Centered Dataset Consider five observations in \\(\\mathbb{R}^2\\): \\[ \\mathbf{x}_1 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{x}_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad \\mathbf{x}_3 = \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{x}_4 = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}, \\quad \\mathbf{x}_5 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}. \\] The sample mean is \\[ \\frac{1}{5}\\sum_{n=1}^5 \\mathbf{x}_n = \\mathbf{0}, \\] so the data are already mean-centered. Geometrically, the data exhibit greater spread along the horizontal axis than the vertical axis. Compute the Covariance Matrix For centered data, the empirical covariance matrix is \\[ \\mathbf{S} = \\frac{1}{N} \\sum_{n=1}^N \\mathbf{x}_n \\mathbf{x}_n^\\top. \\] Summing the outer products gives \\[ \\sum_{n=1}^5 \\mathbf{x}_n \\mathbf{x}_n^\\top = \\begin{pmatrix} 8 &amp; 0 \\\\ 0 &amp; 2 \\end{pmatrix}. \\] Therefore, \\[ \\mathbf{S} = \\frac{1}{5} \\begin{pmatrix} 8 &amp; 0 \\\\ 0 &amp; 2 \\end{pmatrix} = \\begin{pmatrix} 1.6 &amp; 0 \\\\ 0 &amp; 0.4 \\end{pmatrix}. \\] Variance Along a Direction Let \\[ \\mathbf{b} = \\begin{pmatrix} b_1 \\\\ b_2 \\end{pmatrix}, \\quad \\text{with } \\|\\mathbf{b}\\|^2 = b_1^2 + b_2^2 = 1. \\] The variance of the projection of the data onto \\(\\mathbf{b}\\) is \\[ V(\\mathbf{b}) = \\mathbf{b}^\\top \\mathbf{S} \\mathbf{b} = 1.6 b_1^2 + 0.4 b_2^2. \\] The PCA optimization problem is \\[ \\max_{\\mathbf{b}} \\; \\mathbf{b}^\\top \\mathbf{S} \\mathbf{b} \\quad \\text{subject to } \\|\\mathbf{b}\\|^2 = 1. \\] Lagrangian Formulation Introduce a Lagrange multiplier \\(\\lambda\\): \\[ L(\\mathbf{b}, \\lambda) = \\mathbf{b}^\\top \\mathbf{S} \\mathbf{b} + \\lambda (1 - \\mathbf{b}^\\top \\mathbf{b}). \\] Taking derivatives with respect to \\(\\mathbf{b}\\) and setting them to zero yields \\[ \\mathbf{S} \\mathbf{b} = \\lambda \\mathbf{b}. \\] Thus, the optimal direction \\(\\mathbf{b}\\) must be an eigenvector of the covariance matrix \\(\\mathbf{S}\\). Eigenvalues and Eigenvectors Since \\(\\mathbf{S}\\) is diagonal, \\[ \\mathbf{S} = \\begin{pmatrix} 1.6 &amp; 0 \\\\ 0 &amp; 0.4 \\end{pmatrix}, \\] its eigenvalues and eigenvectors are: Eigenvalue Eigenvector \\(\\lambda_1 = 1.6\\) \\(\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\) \\(\\lambda_2 = 0.4\\) \\(\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\) First Principal Component The largest eigenvalue is \\(\\lambda_1 = 1.6\\), with eigenvector \\[ \\mathbf{b}_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}. \\] This vector defines the first principal component. The variance of the data projected onto this direction is \\[ V_1 = \\lambda_1 = 1.6. \\] Projection and Reconstruction For any data point \\(\\mathbf{x}_n\\), the one-dimensional projection is \\[ z_{1n} = \\mathbf{b}_1^\\top \\mathbf{x}_n. \\] The rank-1 reconstruction using the first principal component is \\[ \\tilde{\\mathbf{x}}_n = \\mathbf{b}_1 \\mathbf{b}_1^\\top \\mathbf{x}_n = \\begin{pmatrix} x_{n1} \\\\ 0 \\end{pmatrix}. \\] Thus, PCA retains the horizontal coordinate and discards the vertical one. Exercises Exercise 6.4 What are the steps of PCA? Solution Exercise 6.5 Consider the following data. What is the first principal component? x y 1 -1 0 1 -1 0 Solution Exercise 6.6 Solution Exercise 6.7 Consider the following data. What is the first principal component? x y 126 78 130 82 128 82 128 78 Solution Exercise 6.8 Consider the following data. What is the first principal component? x y 4 11 8 4 13 5 7 14 Solution Exercise 6.9 Consider the following data. What are the first two principal components? Math Science Arts 90 90 80 90 80 90 70 70 60 70 60 70 50 50 50 50 40 50 Solution Exercise 6.10 Consider the following data. What is the first principal component? Large Apples Rotten Apples Damaged Apples Small Apples 1 5 3 1 4 2 6 3 1 4 3 2 4 4 1 1 5 5 2 3 Solution Exercise 6.11 Consider the following data. What is the first principal component? f1 f2 f3 f4 1 2 3 4 5 5 6 7 1 4 2 3 5 3 2 1 8 1 2 2 Solution "],["latent-variable-perspective.html", "6.7 Latent Variable Perspective", " 6.7 Latent Variable Perspective While PCA can be derived geometrically or algebraically, it can also be viewed probabilistically using a latent variable model. 6.7.1 Probabilistic PCA (PPCA) PPCA (Tipping &amp; Bishop, 1999) introduces a latent variable \\(\\mathbf{z} \\in \\mathbb{R}^M\\) with: \\[ p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z} | 0, \\mathbf{I}), \\] and a linear mapping to the observed data: \\[ \\mathbf{x} = \\mathbf{Bz} + \\boldsymbol{\\mu} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2 \\mathbf{I}). \\] Thus: \\[ p(\\mathbf{x} | \\mathbf{z}, \\mathbf{B}, \\boldsymbol{\\mu}, \\sigma^2) = \\mathcal{N}(\\mathbf{x} | \\mathbf{Bz} + \\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}). \\] The joint distribution is: \\[ p(\\mathbf{x}, \\mathbf{z} | \\mathbf{B}, \\boldsymbol{\\mu}, \\sigma^2) = p(\\mathbf{x} | \\mathbf{z}, \\mathbf{B}, \\boldsymbol{\\mu}, \\sigma^2)p(\\mathbf{z}). \\] This defines the generative process: Sample \\(\\mathbf{z} \\sim \\mathcal{N}(0, \\mathbf{I})\\) Sample \\(\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{Bz} + \\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I})\\) 6.7.2 Likelihood and Covariance Structure By integrating out \\(\\mathbf{z}\\): \\[ p(\\mathbf{x} | \\mathbf{B}, \\boldsymbol{\\mu}, \\sigma^2) = \\int p(\\mathbf{x} | \\mathbf{z}, \\mathbf{B}, \\boldsymbol{\\mu}, \\sigma^2)p(\\mathbf{z})\\, d\\mathbf{z} = \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}, \\mathbf{B}\\mathbf{B}^\\top + \\sigma^2 \\mathbf{I}). \\] Mean: \\(\\mathbb{E}[\\mathbf{x}] = \\boldsymbol{\\mu}\\) Covariance: \\(\\text{Var}[\\mathbf{x}] = \\mathbf{B}\\mathbf{B}^\\top + \\sigma^2 \\mathbf{I}\\) Hence, the observed data has covariance derived from both the latent structure and the noise. 6.7.3 Posterior Distribution Given an observation \\(\\mathbf{x}\\), the posterior over latent variables is: \\[ p(\\mathbf{z} | \\mathbf{x}) = \\mathcal{N}(\\mathbf{z} | \\mathbf{m}, \\mathbf{C}), \\] with: \\[ \\mathbf{m} = \\mathbf{B}^\\top (\\mathbf{B}\\mathbf{B}^\\top + \\sigma^2 \\mathbf{I})^{-1}(\\mathbf{x} - \\boldsymbol{\\mu}), \\] \\[ \\mathbf{C} = \\mathbf{I} - \\mathbf{B}^\\top (\\mathbf{B}\\mathbf{B}^\\top + \\sigma^2 \\mathbf{I})^{-1} \\mathbf{B}. \\] The covariance \\(\\mathbf{C}\\) represents the uncertainty of the latent embedding: Small determinant  confident embedding Large determinant  uncertain or outlier point To visualize or reconstruct data: Sample \\(\\mathbf{z}_\\ast \\sim p(\\mathbf{z} | \\mathbf{x}_\\ast)\\) Generate \\(\\tilde{\\mathbf{x}}_\\ast \\sim p(\\mathbf{x} | \\mathbf{z}_\\ast, \\mathbf{B}, \\boldsymbol{\\mu}, \\sigma^2)\\). This process allows data generation and exploration of latent structure, forming a bridge between classical PCA and modern generative models. Exercises Put some exercises here. "],["density-estimation-with-gaussian-mixture-models.html", "Chapter 7 Density Estimation with Gaussian Mixture Models", " Chapter 7 Density Estimation with Gaussian Mixture Models In addition to regression and dimensionality reduction, density estimation is a core pillar of machine learning. Its goal is to represent data compactly by estimating the underlying probability density function that generated it. Instead of storing all data points, density estimation models the data using a parametric family of distributions, such as a Gaussian. For example, we can represent a dataset using its mean and variance, found via maximum likelihood (MLE) or maximum a posteriori (MAP) estimation. However, a single Gaussian distribution often provides a poor fit for complex or multimodal data (data with multiple clusters). To handle this, we use mixture models. Definition 7.1 A mixture model represents a probability density as a convex combination of simpler component distributions: \\[ p(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k p_k(\\mathbf{x}), \\] subject to: \\[ 0 \\leq \\pi_k \\leq 1, \\quad \\sum_{k=1}^{K} \\pi_k = 1, \\] where \\(p_k(\\mathbf{x})\\) are individual component distributions (e.g., Gaussian, Bernoulli), and \\(\\pi_k\\): mixture weights representing the contribution of each component Mixture models can capture multimodal structures in data and are thus more expressive than single distributions. "],["gaussian-mixture-models-gmms.html", "7.1 Gaussian Mixture Models (GMMs)", " 7.1 Gaussian Mixture Models (GMMs) Definition 7.2 A Gaussian Mixture Model (GMM) combines multiple Gaussian distributions: \\[ p(\\mathbf{x} | \\boldsymbol{\\boldsymbol{\\theta}}) = \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\boldsymbol{\\mu}}_k, \\boldsymbol{\\boldsymbol{\\Sigma}}_k), \\] where: \\(\\boldsymbol{\\boldsymbol{\\theta}} = \\{\\pi_k, \\boldsymbol{\\boldsymbol{\\mu}}_k, \\boldsymbol{\\boldsymbol{\\Sigma}}_k : k = 1, \\dots, K\\}\\), each \\(\\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\boldsymbol{\\mu}}_k, \\boldsymbol{\\boldsymbol{\\Sigma}}_k)\\) is a Gaussian component, and the mixture weights \\(\\pi_k\\) satisfy \\(\\sum_{k=1}^{K} \\pi_k = 1\\). This convex combination of Gaussians provides far greater flexibility for modeling complex or clustered data. Example 7.1 A GMM represents a probability distribution as a weighted sum of Gaussian (normal) distributions. Let \\(x \\in \\mathbb{R}\\) be a continuous random variable. A 2-component GMM is defined as: \\[ p(x) = \\pi_1 \\, \\mathcal{N}(x \\mid \\mu_1, \\sigma_1^2) + \\pi_2 \\, \\mathcal{N}(x \\mid \\mu_2, \\sigma_2^2), \\] where: \\(\\pi_1, \\pi_2 \\ge 0\\) are mixing coefficients \\(\\pi_1 + \\pi_2 = 1\\) \\(\\mathcal{N}(x \\mid \\mu_k, \\sigma_k^2)\\) is a Gaussian pdf Let: \\[ \\pi_1 = 0.4, \\quad \\pi_2 = 0.6 \\] \\[ \\mu_1 = -2, \\quad \\sigma_1^2 = 1 \\] \\[ \\mu_2 = 3, \\quad \\sigma_2^2 = 2 \\] Then the model becomes: \\[ p(x) = 0.4 \\, \\mathcal{N}(x \\mid -2, 1) + 0.6 \\, \\mathcal{N}(x \\mid 3, 2) \\] So, With probability 0.4, a data point is generated from a Gaussian centered at 2 With probability 0.6, a data point is generated from a Gaussian centered at 3 The overall distribution is bimodal, with two peaks. Given an observed value \\(x\\), the probability that it came from component \\(k\\) is: \\[ \\gamma_k(x) = p(z = k \\mid x) = \\frac{\\pi_k \\mathcal{N}(x \\mid \\mu_k, \\sigma_k^2)} {\\sum_{j=1}^{2} \\pi_j \\mathcal{N}(x \\mid \\mu_j, \\sigma_j^2)} \\] These are called responsibilities and are used in the ExpectationMaximization (EM) algorithm. Example Calculation Suppose we observe \\(x = 0\\). Component 1 likelihood: \\[ \\mathcal{N}(0 \\mid -2, 1) \\approx \\frac{0.1353}{2.5066} \\approx 0.054. \\] Component 2 likelihood: \\[ \\mathcal{N}(0 \\mid 3, 2) \\approx \\frac{0.1054}{3.5449} \\approx 0.0297. \\] After weighting by \\(\\pi_1\\) and \\(\\pi_2\\), we compute \\(\\gamma_1(0)\\) and \\(\\gamma_2(0)\\) to determine which Gaussian most likely generated the data point. \\[\\gamma_1(0) = \\dfrac{0.4(0.054)}{0.4(0.054) + 0.6(0.0297)} = 0.548 \\quad \\gamma_2(0) = \\dfrac{0.6(0.0297)}{0.4(0.054) + 0.6(0.0297)} = 0.452. \\] Therefore, it is more likely that the point came from Gaussian 1. Unlike linear regression or PCA, GMMs do not have a closed-form MLE solution. Instead, parameter estimation is achieved iteratively, most commonly using the Expectation-Maximization (EM) algorithm. Exercises "],["parameter-learning-via-maximum-likelihood.html", "7.2 Parameter Learning via Maximum Likelihood", " 7.2 Parameter Learning via Maximum Likelihood We are given a dataset \\(\\mathcal{X} = \\{\\textbf{x}_1, \\dots, \\textbf{x}_N\\}\\), where each data point \\(\\textbf{x}_n\\) is drawn i.i.d. from an unknown distribution \\(p(\\textbf{x})\\). Our goal is to approximate this distribution using a Gaussian mixture model with \\(K\\) components, each defined by: \\[ \\boldsymbol{\\theta} = \\{\\pi_k, \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k : k = 1, \\dots, K\\} \\] where: \\(\\pi_k\\) is the mixture weights for the \\(k^{th}\\) distribution, \\(\\boldsymbol{\\mu}_k\\) is the mean for the \\(k^{th}\\) distribution, and \\(\\boldsymbol{\\Sigma}_k\\) is the covariance associated with the \\(k^{th}\\) distribution. 7.2.1 Likelihood and Log-Likelihood The likelihood of the dataset is: \\[ p(\\mathcal{X} | \\boldsymbol{\\theta}) = \\prod_{n=1}^{N} p(\\textbf{x}_n | \\boldsymbol{\\theta}), \\quad p(\\textbf{x}_n | \\boldsymbol{\\theta}) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\textbf{x}_n | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k). \\] The log-likelihood becomes: \\[ L(\\boldsymbol{\\theta}) = \\sum_{n=1}^{N} \\log \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\textbf{x}_n | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k). \\] Maximizing this log-likelihood yields the maximum likelihood estimate (MLE) \\(\\boldsymbol{\\theta}_{ML}\\). However, because of the log-sum term, there is no closed-form solution, so we rely on iterative optimization, leading to the Expectation-Maximization (EM) algorithm. 7.2.2 Responsibilities Definition 7.3 The responsibility \\(r_{nk}\\) as the probability that mixture component \\(k\\) generated data point \\(\\textbf{x}_n\\): \\[ r_{nk} = \\frac{\\pi_k \\mathcal{N}(\\textbf{x}_n | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(\\textbf{x}_n | \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}. \\] Each row \\(r_n = [r_{n1}, \\dots, r_{nK}]\\) forms a probability vector where \\(\\sum_k r_{nk} = 1\\). 7.2.3 Updating Parameters The EM algorithm alternates between computing responsibilities (E-step) and updating parameters (M-step): 1. Update Means \\[ \\boldsymbol{\\mu}_k^{new} = \\frac{\\sum_{n=1}^{N} r_{nk} \\textbf{x}_n}{\\sum_{n=1}^{N} r_{nk}} = \\frac{1}{N_k} \\sum_{n=1}^{N} r_{nk} \\textbf{x}_n, \\] where \\(N_k = \\sum_{n=1}^{N} r_{nk}\\) is the effective number of points assigned to component \\(k\\). 2. Update Covariances \\[ \\boldsymbol{\\Sigma}_k^{new} = \\frac{1}{N_k} \\sum_{n=1}^{N} r_{nk} (\\textbf{x}_n - \\boldsymbol{\\mu}_k)(\\textbf{x}_n - \\boldsymbol{\\mu}_k)^{\\top}. \\] This represents a responsibility-weighted covariance. 3. Update Mixture Weights \\[ \\pi_k^{new} = \\frac{N_k}{N}. \\] This ensures that the mixture weights sum to 1, reflecting the proportion of data explained by each component. 7.2.4 Interpretation At each update step, three things happen in the algorithm: It re-estimates model parameters based on the soft assignment of data points (responsibilities). It moves the Gaussian components toward regions of high data density. It increases the log-likelihood, converging to a local optimum. Together, these iterative updates form the Expectation-Maximization (EM) algorithm for GMMs, which alternates between inferring hidden assignments (E-step) and maximizing parameters (M-step) until convergence. Exercises Exercise 7.1 If we were to consider a single Gaussian as the desired density, the sum over \\(k\\) in \\[\\sum_{n=1}^N \\log \\sum_{k=1}^K \\pi_kN(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\mathbf{\\Sigma}_k)\\] vanishes. So, \\(\\log p(X|\\mathbf{\\theta})\\) equals what? Solution Exercise 7.2 Given \\(p(X|\\mathbf{\\theta}) = \\prod_{n=1}^N p(\\mathbf{x}_n|\\mathbf{\\theta})\\) and \\(p(\\mathbf{x}_n|\\mathbf{\\theta}) = \\sum_{k=1}^K \\pi_k N(\\mathbf{x}_n|\\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)\\), derive the log likelihood and the necessary conditions for a local optimum. Solution Exercise 7.3 Prove the update of the mean parameters \\(\\mathbf{\\mu}_k\\) of the GMM is given by \\[\\mathbf{\\mu}_k^{new} = \\dfrac{\\sum_{n=1}^N r_{nk}\\mathbf{x}_n}{\\sum_{n=1}^N r_{nk}},\\] where \\(r_{nk}\\) is the responsibility. Solution Exercise 7.4 Prove the update of the covariance parameters \\(\\pi_k\\) of the GMM is given by \\[\\pi_k^{new} = \\dfrac{N_k}{N},\\] where \\(N\\) is the number of data points and \\(N_k\\) the column sum of the responsibility matrix. Solution Exercise 7.5 Prove the update of the mixture weights \\(\\mathbf{\\Sigma}_k\\) of the GMM is given by \\[\\mathbf{\\Sigma}_k^{new} = \\dfrac{1}{N_k}\\sum_{n=1}^N r_{nk}(\\mathbf{x}_n-\\mathbf{\\mu}_k)(\\mathbf{x}_n - \\mathbf{\\mu}_k)^T,\\] where \\(r_{nk}\\) is the responsibility and \\(N_k\\) the column sum of the responsibility matrix. Solution "],["expectation-maximization-em-algorithm.html", "7.3 Expectation-Maximization (EM) Algorithm", " 7.3 Expectation-Maximization (EM) Algorithm The Expectation-Maximization (EM) algorithm, introduced by Dempster et al.Â (1977), is an iterative method for estimating parameters in models with latent variables, such as Gaussian mixture models. In GMMs, parameters \\(\\mu_k, \\boldsymbol{\\Sigma}_k, \\pi_k\\) cannot be solved in closed form because the responsibilities \\(r_{nk}\\) depend on them in a complex, nonlinear way. Therefore, the EM algorithm alternates between estimating responsibilities and updating parameters until convergence. 7.3.1 Algorithm Steps Initialize parameters Select initial values for each parameter we want to find: \\[ \\{\\pi_k, \\mu_k, \\boldsymbol{\\Sigma}_k\\}_{k=1}^{K}. \\] E-step (Expectation) Compute the responsibility of component \\(k\\) for data point \\(\\mathbf{x}_n\\): \\[ r_{nk} = \\frac{\\pi_k \\, \\mathcal{N}(\\mathbf{x}_n | \\mu_k, \\boldsymbol{\\Sigma}_k)} {\\sum_{j=1}^{K} \\pi_j \\, \\mathcal{N}(\\mathbf{x}_n | \\mu_j, \\boldsymbol{\\Sigma}_j)}. \\] M-step (Maximization) Update the parameters using the new responsibilities: \\[ N_k = \\sum_{n=1}^{N} r_{nk}. \\] Update means: \\[ \\mu_k = \\frac{1}{N_k} \\sum_{n=1}^{N} r_{nk} \\mathbf{x}_n. \\] Update covariances: \\[ \\boldsymbol{\\Sigma}_k = \\frac{1}{N_k} \\sum_{n=1}^{N} r_{nk}(\\mathbf{x}_n - \\mu_k)(\\mathbf{x}_n - \\mu_k)^{\\top}. \\] Update mixture weights: \\[ \\pi_k = \\frac{N_k}{N}. \\] Each EM iteration increases the log-likelihood, and convergence can be monitored by checking either the change in log-likelihood or parameter stability. 7.3.2 Example: GMM Fit After several iterations (typically few), the EM algorithm converges to a stable mixture model. For instance, a fitted GMM might be: \\[ p(\\mathbf{x}) = 0.29\\,\\mathcal{N}(\\mathbf{x}|-2.75, 0.06) + 0.28\\,\\mathcal{N}(\\mathbf{x}|-0.5, 0.25) + 0.43\\,\\mathcal{N}(\\mathbf{x}|3.64, 1.63). \\] Plots of negative log-likelihood across iterations demonstrate steady improvement until convergence. Exercises Exercise 7.6 Use Excel to analyze the GMM for 2 Gaussians and data \\((-3, -1,0, 1, 3, 4)\\) Solution Exercise 7.7 Use Excel to analyze the GMM for 3 Gaussians and data \\((-3, -1,0, 1, 3, 4)\\) Solution Exercise 7.8 Use Excel to analyze the GMM for 2 Gaussians and data \\((-4,-3, -1,0, 1, 3, 4)\\) Solution Exercise 7.9 Use Excel to analyze the GMM for 3 Gaussians and data \\((-4,-3, -1,0, 1, 3, 4)\\) Solution Exercise 7.10 Use Excel to analyze the GMM for 3 Gaussians and data \\((-4,-3, -2.5, 0, 1, 3, 4)\\) Solution Exercise 7.11 Use Excel to analyze the GMM for 3 Gaussians and data \\((-4,-3, -3, 0, 1, 3, 4)\\) Solution "],["latent-variable-perspective-1.html", "7.4 Latent-Variable Perspective", " 7.4 Latent-Variable Perspective A GMM can be interpreted as a latent-variable model where each data point \\(\\mathbf{x}_n\\) is associated with a hidden variable \\(z_n \\in \\{0,1\\}\\) indicating if the \\(n^{th}\\) mixture component generated it. 7.4.1 Generative Model Each component \\(k\\) is selected according to the mixture probabilities \\(\\pi = [\\pi_1, \\dots, \\pi_K]\\), and data is sampled as: Sample \\(z \\sim \\text{Categorical}(\\pi)\\) Sample \\(\\mathbf{x} \\sim \\mathcal{N}(\\mu_k, \\boldsymbol{\\Sigma}_k)\\) given \\(z_k = 1.\\) Thus, the joint distribution is: \\[ p(\\mathbf{x}, z_k = 1) = \\pi_k \\, \\mathcal{N}(\\mathbf{x} | \\mu_k, \\boldsymbol{\\Sigma}_k). \\] 7.4.2 Likelihood The marginal likelihood is obtained by summing over latent states: \\[ p(\\mathbf{x} | \\boldsymbol{\\theta}) = \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(\\mathbf{x} | \\mu_k, \\boldsymbol{\\Sigma}_k) \\] For a dataset \\(\\mathcal{X} = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}\\), the total likelihood is: \\[ p(\\mathcal{X} | \\boldsymbol{\\theta}) = \\prod_{n=1}^{N} \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(\\mathbf{x}_n | \\mu_k, \\boldsymbol{\\Sigma}_k). \\] This formulation is identical to the GMM likelihood derived earlier. 7.4.3 Posterior Distribution Using Bayes theorem, the posterior probability (responsibility) that component \\(k\\) generated \\(\\mathbf{x}_n\\) is: \\[ p(z_k = 1 | \\mathbf{x}_n) = \\frac{\\pi_k \\, \\mathcal{N}(\\mathbf{x}_n | \\mu_k, \\boldsymbol{\\Sigma}_k)} {\\sum_{j=1}^{K} \\pi_j \\, \\mathcal{N}(\\mathbf{x}_n | \\mu_j, \\boldsymbol{\\Sigma}_j)} = r_{nk}. \\] Thus, the responsibilities from the EM algorithm have a probabilistic interpretation as posterior probabilities. 7.4.4 Extension to Full Dataset Each data point \\(\\mathbf{x}_n\\) has its own latent variable \\(z_n\\), forming a set of hidden assignments: \\[ z_n = [z_{n1}, \\dots, z_{nK}]^{\\top}. \\] The same prior \\(\\pi\\) applies to all, and the joint conditional distribution factorizes as: \\[ p(\\mathbf{x}_1, \\dots, \\mathbf{x}_N | z_1, \\dots, z_N) = \\prod_{n=1}^{N} p(\\mathbf{x}_n | z_n). \\] Responsibilities \\(r_{nk}\\) again represent \\(p(z_{nk} = 1 | \\mathbf{x}_n)\\). 7.4.5 EM Algorithm Revisited From the latent-variable perspective, EM can be derived as maximizing the expected complete-data log-likelihood: \\[ Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) = \\mathbb{E}_{z | \\mathbf{x}, \\boldsymbol{\\theta}^{(t)}}[\\log p(\\mathbf{x}, z | \\boldsymbol{\\theta})]. \\] E-step: Compute the expected value of the log-likelihood under the posterior \\(p(z | \\mathbf{x}, \\boldsymbol{\\theta}^{(t)})\\). M-step: Maximize this expectation with respect to \\(\\boldsymbol{\\theta}\\). Each iteration increases the log-likelihood but may converge to a local maximum, depending on initialization. Exercises Put some exercises here. "],["classification-with-support-vector-machines-svms.html", "Chapter 8 Classification with Support Vector Machines (SVMs)", " Chapter 8 Classification with Support Vector Machines (SVMs) In many machine learning problems, we aim to predict discrete outcomes rather than continuous values. For example: An email classifier distinguishes between personal mail and junk mail. A telescope identifies whether an object is a galaxy, star, or planet. This type of task is known as classification, and when there are only two possible outcomes, it is called binary classification. In binary classification, the goal is to predict labels \\(y_n \\in \\{+1, -1\\}\\) for examples \\(\\mathbf{x}_n \\in \\mathbb{R}^D\\). The two labels are often referred to as the positive and negative classes, although these names do not imply any moral or semantic positiveness. Example 8.1 In a cancer detection problem, \\(y = +1\\) might indicate the presence of cancer in a patient while \\(y = -1\\) might indicate no cancer. Formally, the predictor is a function: \\[ f: \\mathbb{R}^D \\to \\{+1, -1\\}. \\] The task is to find model parameters that minimize classification error on a labeled dataset: \\[ \\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_N, y_N)\\}. \\] Similar to regression (Chapter 9), SVMs assume a linear model in some transformed feature space: \\[ f(\\mathbf{x}) = \\text{sign}(\\langle \\mathbf{w}, \\phi(\\mathbf{x}) \\rangle + b), \\] where \\(\\phi(\\mathbf{x})\\) is a possibly nonlinear transformation of the input. This approach allows SVMs to represent nonlinear decision boundaries through kernel functions. SVMs are a widely used and theoretically grounded method for binary classification. They are particularly valuable because: They provide ageometric interpretation of classification, based on concepts such as inner products, projection, and margins. The optimization problem in SVMs does not have a closed-form solution, requiring tools from convex optimization (see Chapter 7). They achieve strong theoretical guarantees and excellent empirical performance (Steinwart &amp; Christmann, 2008). Unlike probabilistic approaches such as maximum likelihood estimation (Chapter 9), SVMs are derived from geometric principle* and empirical risk minimization (Section 8.2). "],["separating-hyperplanes.html", "8.1 Separating Hyperplanes", " 8.1 Separating Hyperplanes The key idea of SVMs is to separate data points of different classes using a hyperplane in \\(\\mathbb{R}^D\\). A hyperplane divides the space into two regions, each corresponding to one of the two classes. Definition 8.1 A hyperplane in \\(\\mathbb{R}^D\\) is defined as: \\[ \\{ \\mathbf{x} \\in \\mathbb{R}^D : f(\\mathbf{x}) = 0 \\}, \\] where: \\[ f(\\mathbf{x}) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle + b. \\] Here: \\(\\mathbf{w} \\in \\mathbb{R}^D\\) is the normal vector to the hyperplane, and \\(b \\in \\mathbb{R}\\) is the bias (intercept) term. Any vector \\(\\mathbf{w}\\) orthogonal to the hyperplane satisfies: \\[ \\langle \\mathbf{w}, \\mathbf{x}_a - \\mathbf{x}_b \\rangle = 0, \\] for all points \\(\\mathbf{x}_a, \\mathbf{x}_b\\) lying on the hyperplane. 8.1.1 Classification Rule A new example \\(\\mathbf{x}_{\\text{test}}\\) is classified based on the sign of \\(f(\\mathbf{x}_{\\text{test}})\\): \\[ f(\\mathbf{x}_{\\text{test}}) = \\begin{cases} +1, &amp; \\text{if } f(\\mathbf{x}_{\\text{test}}) \\ge 0, \\\\ -1, &amp; \\text{if } f(\\mathbf{x}_{\\text{test}}) &lt; 0. \\end{cases} \\] Geometrically: Points with \\(f(\\mathbf{x}) &gt; 0\\) lie on the positive side of the hyperplane. Points with \\(f(\\mathbf{x}) &lt; 0\\) lie on the negative side. 8.1.2 Training Objective During training, we want: Positive examples (\\(y_n = +1\\)) to be on the positive side: \\[ \\langle \\mathbf{w}, \\mathbf{x}_n \\rangle + b \\ge 0, \\] Negative examples (\\(y_n = -1\\)) to be on the negative side: \\[ \\langle \\mathbf{w}, \\mathbf{x}_n \\rangle + b &lt; 0. \\] Both conditions can be combined compactly as: \\[ y_n (\\langle \\mathbf{w}, \\mathbf{x}_n \\rangle + b) \\ge 0. \\] This equation expresses that all examples are correctly classified relative to the hyperplane defined by \\((\\mathbf{w}, b)\\). 8.1.3 Geometric Interpretation The vector \\(\\mathbf{w}\\) determines the orientation of the hyperplane. The scalar \\(b\\) shifts the hyperplane along \\(\\mathbf{w}\\). Classification is performed by checking on which side of the hyperplane each example lies. The SVMs goal is to find the hyperplane that maximizes the margin  the distance between the hyperplane and the nearest data points from each class. Exercises Put some exercises here. "],["primal-support-vector-machine.html", "8.2 Primal Support Vector Machine", " 8.2 Primal Support Vector Machine Support Vector Machines (SVMs) are based on the geometric concept of finding a separating hyperplane that best divides positive and negative examples. When data is linearly separable, there are infinitely many such hyperplanes. To obtain a unique and robust solution, SVMs choose the hyperplane that maximizes the marginthe distance between the hyperplane and the nearest data points. A large-margin classifier tends to generalize well to unseen data (Steinwart &amp; Christmann, 2008). Definition 8.2 The margin is the distance between the separating hyperplane and the closest training examples. Example 8.2 For a hyperplane defined as: \\[ f(\\mathbf{x}) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle + b = 0, \\] and an example \\(\\mathbf{x}_a\\), the orthogonal distance \\(r\\) to the hyperplane is obtained by projecting \\(\\mathbf{x}_a\\) onto it: \\[ \\mathbf{x}_a = \\mathbf{x}_a&#39; + r \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}, \\] where \\(\\mathbf{x}_a&#39;\\) lies on the hyperplane and \\(\\|\\mathbf{w}\\|\\) is the Euclidean norm of \\(\\mathbf{w}\\). To ensure that all examples lie at least distance \\(r\\) from the hyperplane: \\[ y_n(\\langle \\mathbf{w}, \\mathbf{x}_n \\rangle + b) \\ge r. \\] Assuming \\(\\|\\mathbf{w}\\| = 1\\) (unit length normalization), the optimization problem becomes: \\[ \\max_{\\mathbf{w},b,r} \\quad r, \\] subject to \\[ y_n(\\langle \\mathbf{w}, \\mathbf{x}_n \\rangle + b) \\ge r, \\quad \\|\\mathbf{w}\\| = 1, \\quad r &gt; 0. \\] This formulation seeks to maximize the margin \\(r\\) while ensuring all data points are correctly classified. 8.2.1 Traditional Derivation of the Margin An equivalent approach defines the scale of the data such that the closest points satisfy: \\[ \\langle \\mathbf{w}, \\mathbf{x}_a \\rangle + b = 1. \\] By projecting onto the hyperplane, we find the margin: \\[ r = \\frac{1}{\\|\\mathbf{w}\\|}. \\] Thus, maximizing the margin is equivalent to minimizing the norm of \\(\\mathbf{w}\\). The optimization problem becomes: \\[ \\min_{\\mathbf{w},b} \\frac{1}{2}\\|\\mathbf{w}\\|^2, \\] subject to \\[ y_n(\\langle \\mathbf{w}, \\mathbf{x}_n \\rangle + b) \\ge 1, \\quad \\forall n. \\] This is known as the Hard Margin SVM, which assumes perfect separability and allows no violations of the margin condition. 8.2.2 Why We Can Set the Margin to 1? Theorem 12.1 below shows that the two formulationsmaximizing \\(r\\) under \\(\\|\\mathbf{w}\\|=1\\), and minimizing \\(\\|\\mathbf{w}\\|^2\\) under a unit marginare equivalent. Theorem 8.1 Maximizing the margin \\(r\\), where we consider normalized weights: \\[ \\begin{aligned} \\max_{w, b, r} \\quad &amp; r \\quad &amp;&amp; \\text{(margin)} \\\\ \\text{subject to} \\quad &amp; y_n(\\langle w, x_n \\rangle + b) \\ge r \\quad &amp;&amp; \\text{(data fitting)} \\\\ &amp; \\|w\\| = 1 \\quad &amp;&amp; \\text{(normalization)} \\\\ &amp; r &gt; 0 \\end{aligned} \\] is equivalent to scaling the data such that the margin is unity: \\[ \\begin{aligned} \\min_{w, b} \\quad &amp; \\frac{1}{2} \\|w\\|^2 \\quad &amp;&amp; \\text{(margin)} \\\\ \\text{subject to} \\quad &amp; y_n(\\langle w, x_n \\rangle + b) \\ge 1 \\quad &amp;&amp; \\text{(data fitting)} \\end{aligned} \\] Hence, setting the margin to 1 simplifies computation without changing the solution. This equivalence arises from a rescaling of parameters: \\[ r = \\frac{1}{\\|\\mathbf{w}\\|}. \\] 8.2.3 Soft Margin SVM: Geometric View When data is not linearly separable, we relax the hard constraints by introducing slack variables \\(\\xi_n \\ge 0\\), allowing points to lie within or beyond the margin. The resulting Soft Margin SVM optimization problem is: \\[ \\min_{\\mathbf{w},b,\\xi} \\quad \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{n=1}^N \\xi_n, \\] subject to \\[ y_n(\\langle \\mathbf{w}, \\mathbf{x}_n \\rangle + b) \\ge 1 - \\xi_n, \\quad \\xi_n \\ge 0. \\] Here: \\(C &gt; 0\\) is the regularization parameter controlling the trade-off between a large margin and small classification error. The term \\(\\|\\mathbf{w}\\|^2\\) acts as the regularizer, encouraging simpler models. Larger \\(C\\) penalizes margin violations more strongly (less regularization). 8.2.4 Soft Margin SVM: Loss Function View Using the empirical risk minimization framework, we can rewrite the SVM objective as minimizing a regularized loss: \\[ \\min_{\\mathbf{w},b} \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{n=1}^N \\ell(y_n(\\langle \\mathbf{w}, \\mathbf{x}_n \\rangle + b)), \\] where \\(\\ell(t)\\) is the hinge loss: \\[ \\ell(t) = \\max(0, 1 - t). \\] This loss penalizes points: Outside the margin (\\(t \\ge 1\\)): no penalty (\\(\\ell = 0\\)), Inside the margin (\\(0 &lt; t &lt; 1\\)): linear penalty, Misclassified (\\(t &lt; 0\\)): large penalty. Hence, the SVM objective can be written as an unconstrained optimization: \\[ \\min_{\\mathbf{w},b} \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{n=1}^N \\max(0, 1 - y_n(\\langle \\mathbf{w}, \\mathbf{x}_n \\rangle + b)). \\] The first term acts as a regularizer (encouraging a large margin), while the second term measures training error. Exercises Put some exercises here. "],["dual-support-vector-machine.html", "8.3 Dual Support Vector Machine", " 8.3 Dual Support Vector Machine The Dual Support Vector Machine (SVM) is an alternative formulation of the SVM optimization problem. While the primal SVM works with variables \\(\\mathbf{w}\\) and \\(b\\) (whose size depends on the number of features \\(D\\)), the dual SVM expresses the problem in terms of Lagrange multipliers, where the number of parameters depends instead on the number of training examples. This formulation is especially useful when: The number of features is much larger than the number of examples. Kernel functions are used, as they can be incorporated easily in the dual form. We leverage convex duality, discussed earlier in the text. 8.3.1 Convex Duality via Lagrange Multipliers The dual formulation is derived from the primal soft-margin SVM by introducing Lagrange multipliers: \\(\\alpha_n \\ge 0\\) for the classification constraints. \\(\\gamma_n \\ge 0\\) for the non-negativity of the slack variables \\(\\xi_n\\). The Lagrangian is defined as: \\[ L(\\mathbf{w}, b, \\xi, \\alpha, \\gamma) = \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C\\sum_{n=1}^N \\xi_n - \\sum_{n=1}^N \\alpha_n[y_n(\\langle \\mathbf{w}, \\mathbf{x}_n \\rangle + b) - 1 + \\xi_n] - \\sum_{n=1}^N \\gamma_n \\xi_n. \\] Setting derivatives to zero gives: \\[ \\frac{\\partial L}{\\partial \\mathbf{w}} = 0 \\Rightarrow \\mathbf{w} = \\sum_{n=1}^N \\alpha_n y_n \\mathbf{x}_n. \\] This equation illustrates the representer theorem, showing that \\(\\mathbf{w}\\) is a linear combination of the training example. Only points with \\(\\alpha_n &gt; 0\\) contribute to \\(\\mathbf{w}\\); these are the support vectors. 8.3.2 Dual Optimization Problem Substituting \\(\\mathbf{w}\\) back into the Lagrangian yields the dual problem: \\[ \\begin{aligned} \\min_{\\alpha} \\quad &amp; \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N y_i y_j \\alpha_i \\alpha_j \\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle - \\sum_{i=1}^N \\alpha_i \\\\ \\text{subject to} \\quad &amp; \\sum_{i=1}^N y_i \\alpha_i = 0, \\\\ &amp; 0 \\le \\alpha_i \\le C, \\; \\forall i = 1, \\ldots, N. \\end{aligned} \\] These box constraints restrict each \\(\\alpha_i\\) between 0 and \\(C\\). Support vectors with \\(0 &lt; \\alpha_i &lt; C\\) lie exactly on the margin boundary. The primal parameters can be recovered as: \\[ \\mathbf{w}^* = \\sum_{n=1}^N \\alpha_n y_n \\mathbf{x}_n, \\quad b^* = y_n - \\langle \\mathbf{w}^*, \\mathbf{x}_n \\rangle \\] (for any example on the margin). 8.3.3 Dual SVM: Convex Hull View An alternative geometric view of the dual SVM comes from convex hulls: Each class (positive and negative) forms a convex hull from its examples. The SVM finds the closest points \\(c\\) and \\(d\\) between these two convex hulls. The difference vector \\(\\mathbf{w} = \\mathbf{c} - \\mathbf{d}\\) defines the maximum-margin hyperplane. Mathematically: \\[ \\text{conv}(X) = \\left\\{ \\sum_{n=1}^N \\alpha_n \\mathbf{x}_n \\; \\Big| \\; \\sum_{n=1}^N \\alpha_n = 1, \\; \\alpha_n \\ge 0 \\right\\}. \\] The optimization problem can then be expressed as minimizing the distance between the convex hulls: \\[ \\min_{\\alpha} \\frac{1}{2} \\left\\| \\sum_{n: y_n = +1} \\alpha_n \\mathbf{x}_n - \\sum_{n: y_n = -1} \\alpha_n \\mathbf{x}_n \\right\\|^2, \\] subject to: \\[ \\sum_{n=1}^N y_n \\alpha_n = 0, \\quad \\alpha_n \\ge 0. \\] For the soft-margin case, a reduced convex hull is used by limiting \\(\\alpha_n \\le C\\), shrinking the hull and allowing for misclassified examples. Exercises Put some exercises here. "],["kernels.html", "8.4 Kernels", " 8.4 Kernels The dual formulation of the SVM depends only on inner products between training example \\(\\mathbf{x}_i\\) and \\(\\mathbf{x}_j\\). This modularity allows us to replace the inner product with a more general similarity function, giving rise to the concept of kernels. 8.4.1 Feature Representations and Kernels Let each input \\(\\mathbf{x}\\) be represented in some (possibly nonlinear) feature space by a mapping: \\[ \\phi(\\mathbf{x}) : \\mathcal{X} \\rightarrow \\mathcal{H}, \\] where \\(\\mathcal{H}\\) is a Hilbert space. In this case, the dual SVM objective involves terms of the form: \\[ \\langle \\phi(\\mathbf{x}_i), \\phi(\\mathbf{x}_j) \\rangle_{\\mathcal{H}}. \\] Rather than explicitly computing \\(\\phi(\\mathbf{x})\\), we define a kernel function: \\[ k(\\mathbf{x}_i, \\mathbf{x}_j) = \\langle \\phi(\\mathbf{x}_i), \\phi(\\mathbf{x}_j) \\rangle_{\\mathcal{H}}. \\] This allows the SVM to implicitly operate in a high-dimensional (or even infinite-dimensional) space, without directly computing the transformation. This substitution is known as the kernel trick  it hides the explicit feature mapping while preserving the geometric relationships defined by the inner product. 8.4.2 The Kernel Function and RKHS A kernel is a function \\(k : \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) that defines inner products in a reproducing kernel Hilbert space (RKHS). Every valid kernel has a unique RKHS (Aronszajn, 1950). The canonical feature map is \\(\\phi(\\mathbf{x}) = k(\\cdot, \\mathbf{x})\\). The kernel must satisfy: \\[ \\forall z \\in \\mathbb{R}^N : z^\\top K z \\ge 0, \\] where \\(K\\) is the Gram matrix (or kernel matrix) defined by \\(K_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\\).This ensures that \\(K\\) is symmetric and positive semidefinite. 8.4.3 Common Kernels Some widely used kernels for data \\(\\mathbf{x}_i \\in \\mathbb{R}^D\\) include: Polynomial kernel: \\[ k(\\mathbf{x}_i, \\mathbf{x}_j) = (\\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle + c)^d. \\] Gaussian radial basis function (RBF) kernel: \\[ k(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2}\\right). \\] Rational quadratic kernel: \\[ k(\\mathbf{x}_i, \\mathbf{x}_j) = 1 - \\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2 + c}. \\] These kernels allow nonlinear decision boundaries while the SVM still computes a linear separator in feature space. 8.4.4 Practical Aspects The kernel trick offers computational efficiency  the kernel can be computed directly without constructing \\(\\phi(\\mathbf{x})\\). For example, The polynomial kernel avoids explicit polynomial expansion. The Gaussian RBF kernel corresponds to an infinite-dimensional feature space. The choice of kernel and its parameters (e.g., degree \\(d\\), variance \\(\\sigma\\)) are typically selected via nested cross-validation. Kernels are not restricted to vector data. They can be defined over strings, graphs, sets, distributions, and other structured objects. 8.4.5 Terminology Note The term kernel can have different meanings: In this context: kernel functions defining an RKHS (machine learning). In linear algebra: the null space of a matrix. In statistics: smoothing kernels in kernel density estimation. Exercises Put some exercises here. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
