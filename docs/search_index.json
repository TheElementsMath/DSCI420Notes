[["matrix-decompositions.html", "Chapter 4 Matrix Decompositions", " Chapter 4 Matrix Decompositions Matrices provide a compact way to represent linear mappings and data, where rows often correspond to observations and columns correspond to features. This chapter focuses on three central questions about matrices: How to summarize a matrix with key numerical characteristics. How to decompose a matrix into simpler, interpretable components. How to use these decompositions for approximations and analysis. "],["determinant-and-trace.html", "4.1 Determinant and Trace", " 4.1 Determinant and Trace The determinant of a square matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\), denoted as \\(\\det(\\mathbf{A})\\) or \\(|\\mathbf{A}|\\), is a scalar that characterizes several key properties of \\(\\mathbf{A}\\). For small matrices: \\[ \\det \\begin{pmatrix} a_{11} \\end{pmatrix} = a_{11}, \\quad \\det \\begin{pmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{pmatrix} = a_{11}a_{22} - a_{12}a_{21}. \\] Example 4.1 Let \\[ \\mathbf{A} = \\begin{bmatrix} 3 &amp; -1 \\\\ 5 &amp; 2 \\end{bmatrix}. \\] The determinant of \\(\\mathbf{A}\\) is \\[ \\det(\\mathbf{A}) = (3)(2) - (-1)(5) = 6 + 5 = 11. \\] For larger matrices, we can compute determinants recursively using the Laplace expansion: \\[ \\det(\\mathbf{A}) = \\sum_{k=1}^n (-1)^{k+j} a_{kj} \\det(\\mathbf{A}_{k,j}), \\] where \\(\\mathbf{A}_{k,j}\\) is the submatrix obtained by removing row \\(k\\) and column \\(j\\). Example 4.2 Let \\[ \\mathbf{A} = \\begin{bmatrix} 2 &amp; -1 &amp; 3 \\\\ 1 &amp; 4 &amp; 0 \\\\ -2 &amp; 5 &amp; 1 \\end{bmatrix}. \\] We compute \\(\\det( \\mathbf{A})\\) using Laplace expansion along the first row. \\[ \\det( \\mathbf{A}) = 2 \\begin{vmatrix} 4 &amp; 0 \\\\ 5 &amp; 1 \\end{vmatrix} - (-1) \\begin{vmatrix} 1 &amp; 0 \\\\ -2 &amp; 1 \\end{vmatrix} + 3 \\begin{vmatrix} 1 &amp; 4 \\\\ -2 &amp; 5 \\end{vmatrix}. \\] Next, we compute each minor \\[\\begin{align*} \\begin{vmatrix} 4 &amp; 0 \\\\ 5 &amp; 1 \\end{vmatrix} &amp;= (4)(1) - (0)(5) = 4\\\\ \\begin{vmatrix} 1 &amp; 0 \\\\ -2 &amp; 1 \\end{vmatrix} &amp;= (1)(1) - (0)(-2) = 1\\\\ \\begin{vmatrix} 1 &amp; 4 \\\\ -2 &amp; 5 \\end{vmatrix} &amp;= (1)(5) - (4)(-2) = 5 + 8 = 13 \\end{align*}\\] Using these determinants in the Laplace formula gives us \\[ \\det( \\mathbf{A}) = 2(4) - (-1)(1) + 3(13) = 8 + 1 + 39 = 48. \\] A matrix \\(\\mathbf{A}\\) is invertible if and only if \\(\\det(\\mathbf{A}) \\neq 0\\). Example 4.3 Verify that a matrix is invertible if and only if its determinant is non-zero. For triangular matrices, the determinant equals the product of the diagonal elements. Example 4.4 Let \\[ \\mathbf{A} = \\begin{bmatrix} 3 &amp; 2 &amp; -1 \\\\ 0 &amp; 5 &amp; 4 \\\\ 0 &amp; 0 &amp; 7 \\end{bmatrix}. \\] Since \\(\\mathbf{A}\\) is upper triangular, we already know that \\[ \\det( \\mathbf{A}) = 3 \\cdot 5 \\cdot 7 = 105. \\] But we will verify this using Laplace expansion. Expand along column 1: \\[\\begin{align*} \\det(\\mathbf{A}) &amp;= 3 \\begin{vmatrix} 5 &amp; 4 \\\\ 0 &amp; 7 \\end{vmatrix} - 0 \\begin{vmatrix} 2 &amp; -1 \\\\ 0 &amp; 7 \\end{vmatrix} + 0 \\begin{vmatrix} 2 &amp; -1 \\\\ 5 &amp; 4 \\end{vmatrix}\\\\ &amp;= 3 \\begin{vmatrix} 5 &amp; 4 \\\\ 0 &amp; 7 \\end{vmatrix}\\\\ &amp;= 3\\left[(5)(7) - (4)(0)\\right]\\\\ &amp;= 3(35)\\\\ &amp;= 105. \\end{align*}\\] This matches the product of the diagonal entries, as expected for triangular matrices. The determinant changes sign when two rows (or columns) are swapped, and scales when a row is multiplied by a scalar. Example 4.5 Let \\[ \\mathbf{A}=\\begin{bmatrix}1 &amp; 2\\\\[4pt] 3 &amp; 4\\end{bmatrix}. \\] Compute \\(\\det( \\mathbf{A})\\): \\[ \\det( \\mathbf{A})=1\\cdot 4 - 2\\cdot 3 = 4 - 6 = -2. \\] Swap row 1 and row 2 to get \\[ \\mathbf{B}=\\begin{bmatrix}3 &amp; 4\\\\[4pt] 1 &amp; 2\\end{bmatrix}. \\] Compute \\(\\det( \\mathbf{B})\\): \\[ \\det( \\mathbf{B})=3\\cdot 2 - 4\\cdot 1 = 6 - 4 = 2. \\] Observation: \\(\\det( \\mathbf{B})=2 = -(-2)= -\\det( \\mathbf{A})\\). Swapping two rows changed the sign of the determinant. Example 4.6 Let \\[ \\mathbf{C}=\\begin{bmatrix}2 &amp; 1\\\\[4pt] 0 &amp; 3\\end{bmatrix}. \\] Compute \\(\\det( \\mathbf{C})\\): \\[ \\det( \\mathbf{C})=2\\cdot 3 - 1\\cdot 0 = 6 - 0 = 6. \\] Multiply the first row of \\(\\mathbf{C}\\) by \\(2\\) to get \\[ \\mathbf{D}=\\begin{bmatrix}4 &amp; 2\\\\[4pt] 0 &amp; 3\\end{bmatrix}. \\] Compute \\(\\det( \\mathbf{D})\\): \\[ \\det( \\mathbf{D})=4\\cdot 3 - 2\\cdot 0 = 12 - 0 = 12. \\] Observation: \\(\\det( \\mathbf{D})=12 = 2\\cdot 6 = 2\\det( \\mathbf{C})\\). Scaling a single row by a factor \\(2\\) scales the determinant by \\(2\\). 4.1.1 Geometric Interpretation The determinant measures the signed volume of the parallelepiped spanned by the columns of \\(\\mathbf{A}\\): In \\(\\mathbb{R}^2\\): \\(|\\det(\\mathbf{A})|\\) gives the area of a parallelogram. In \\(\\mathbb{R}^3\\): \\(|\\det(\\mathbf{A})|\\) gives the volume of a parallelepiped. If the determinant is zero, the columns are linearly dependent and the volume collapses to zero. 4.1.2 Properties of the Determinant Theorem 4.1 For square matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) and \\(\\lambda \\in \\mathbb{R}\\), the following properties hold: \\[ \\begin{aligned} \\det(\\mathbf{A}\\mathbf{B}) &amp;= \\det(\\mathbf{A})\\det(\\mathbf{B}), \\\\ \\det(\\mathbf{A}^\\top) &amp;= \\det(\\mathbf{A}), \\\\ \\det(\\mathbf{A}^{-1}) &amp;= \\frac{1}{\\det(\\mathbf{A})}, \\\\ \\det(\\lambda \\mathbf{A}) &amp;= \\lambda^n \\det(\\mathbf{A}). \\end{aligned} \\] Example 4.7 For \\(2 \\times 2\\) matrices, \\[ \\mathbf{A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} e &amp; f \\\\ g &amp; h \\end{bmatrix}, \\] we have \\[ \\mathbf{A}\\mathbf{B} = \\begin{bmatrix} ae + bg &amp; af + bh \\\\ ce + dg &amp; cf + dh \\end{bmatrix}. \\] This has determinant \\[\\begin{align*} \\det( \\mathbf{A}\\mathbf{B}) &amp;= \\begin{vmatrix} ae + bg &amp; af + bh \\\\ ce + dg &amp; cf + dh \\end{vmatrix} \\\\ &amp; = (ae + bg)(cf + dh) - (af + bh)(ce + dg)\\\\ &amp; = aecf + aedh + bgcf + bgdh - \\left( afce + afdg + bhce + bhdg \\right)\\\\ &amp; = ac(ef - fe) + ad(eh - fg) + bc(gf - he) + bd(gh - hg)\\\\ &amp; = ad(eh - fg) - bc(eh - fg)\\\\ &amp; = (ad - bc)(eh - fg) \\\\ &amp;= \\det( \\mathbf{A})\\det(\\mathbf{B}). \\end{align*}\\] Theorem 4.2 A matrix is invertible if and only if it is full rank, i.e. \\(\\text{rank}(\\mathbf{A}) = n\\). 4.1.3 Trace Definition 4.1 The trace of a square matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) is the sum of its diagonal elements: \\[ \\text{tr}(\\mathbf{A}) = \\sum_{i=1}^n a_{ii}. \\] Theorem 4.3 For square matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) and \\(\\alpha \\in \\mathbb{R}\\), the following properties hold: \\[ \\begin{aligned} \\text{tr}(\\mathbf{A} + \\mathbf{B}) &amp;= \\text{tr}(\\mathbf{A}) + \\text{tr}(\\mathbf{B}), \\\\ \\text{tr}(\\alpha \\mathbf{A}) &amp;= \\alpha \\, \\text{tr}(\\mathbf{A}), \\\\ \\text{tr}(\\mathbf{A}\\mathbf{B}) &amp;= \\text{tr}(\\mathbf{B}\\mathbf{A}), \\\\ \\text{tr}(\\mathbf{I}_n) &amp;= n. \\end{aligned} \\] The trace is invariant under cyclic permutations, meaning \\(\\text{tr}(\\mathbf{A}\\mathbf{K}\\mathbf{L}) = \\text{tr}(\\mathbf{K}\\mathbf{L}\\mathbf{A})\\). It is also independent of basis, so the trace of a linear map \\(\\Phi\\) is the same in all matrix representations. Example 4.8 Let \\[ \\mathbf{A} = \\begin{bmatrix} 3 &amp; -1 &amp; 4 \\\\ 0 &amp; 2 &amp; 5 \\\\ 7 &amp; 1 &amp; -6 \\end{bmatrix}. \\] The trace is \\[ \\text{tr}(\\mathbf{A}) = 3 + 2 + (-6) = -1. \\] 4.1.4 Characteristic Polynomial Definition 4.2 The characteristic polynomial of a square matrix \\(\\mathbf{A}\\) is defined as: \\[ p_ \\mathbf{A}(\\lambda) = \\det(\\mathbf{A} - \\lambda \\mathbf{I}) = c_0 + c_1 \\lambda + \\cdots + c_{n-1} \\lambda^{n-1} + (-1)^n \\lambda^n. \\] The characteristic polynomial for \\(\\mathbf{A}\\) encodes key properties of \\(\\mathbf{A}\\): \\[ c_0 = \\det(\\mathbf{A}), \\quad c_{n-1} = (-1)^{n-1} \\text{tr}(\\mathbf{A}). \\] The roots of this polynomial are the eigenvalues of \\(\\mathbf{A}\\), which will be explored in the next section. Example 4.9 Let \\[ \\mathbf{A} = \\begin{bmatrix} 3 &amp; -1 &amp; 4 \\\\ 0 &amp; 2 &amp; 5 \\\\ 7 &amp; 1 &amp; -6 \\end{bmatrix}. \\] The characteristic polynomial is \\[\\begin{align*} p(\\lambda) &amp;= \\det(A - \\lambda I) \\\\ &amp;= \\det\\left( \\begin{bmatrix} 3 - \\lambda &amp; -1 &amp; 4 \\\\ 0 &amp; 2 - \\lambda &amp; 5 \\\\ 7 &amp; 1 &amp; -6 - \\lambda \\end{bmatrix} \\right) \\\\ &amp;= (3 - \\lambda) \\begin{vmatrix} 2 - \\lambda &amp; 5 \\\\ 1 &amp; -6 - \\lambda \\end{vmatrix} + 7 \\begin{vmatrix} -1 &amp; 4 \\\\ 2 - \\lambda &amp; 5 \\end{vmatrix}\\\\ &amp;= (3 - \\lambda)(\\lambda^2 + 4\\lambda - 17) + 7(4\\lambda - 13) \\\\ &amp;= -\\lambda^3 - \\lambda^2 + 57\\lambda - 142. \\end{align*}\\] Exercises Exercise 4.1 Find \\(\\det\\left(\\begin{bmatrix} 2 &amp; 3 \\\\ 4 &amp; 5 \\end{bmatrix} \\right)\\) using the formula. Solution Exercise 4.2 Find \\(\\det\\left(\\begin{bmatrix} 2 &amp; 3 &amp;4\\\\ 5 &amp; 6 &amp;7\\\\ 8 &amp; 9 &amp; 1 \\end{bmatrix} \\right)\\) using the formula. Solution Exercise 4.3 Find \\(\\det\\left(\\begin{bmatrix} 2 &amp; 3 &amp;4\\\\ 5 &amp; 6 &amp;7\\\\ 8 &amp; 9 &amp; 1 \\end{bmatrix} \\right)\\) using the Laplace method. Solution Exercise 4.4 Prove that if \\(A\\) is a square matrix with a row or column of 0’s, then \\(\\det(A) = 0\\). Solution Exercise 4.5 Find \\(\\det\\left(\\begin{bmatrix} 0&amp; 2 &amp; 3 &amp;4\\\\ 5 &amp; 6 &amp;7&amp;0 \\\\1&amp; 8 &amp; 9 &amp; 1\\\\0&amp;2&amp;3&amp;0 \\end{bmatrix} \\right)\\) using the Laplace method. Solution Exercise 4.6 Prove that if \\(A\\) is a square matrix with 2 identical rows or columns, then \\(\\det(A) = 0\\). Solution Exercise 4.7 Find \\(\\det\\left(\\begin{bmatrix} 2 &amp; 0 &amp;0\\\\ 5 &amp; 6 &amp;0\\\\ 8 &amp; 9 &amp; 1 \\end{bmatrix} \\right)\\). Solution Exercise 4.8 Find \\(\\text{tr}\\left(\\begin{bmatrix} 2 &amp; 0 &amp;0\\\\ 5 &amp; 6 &amp;0\\\\ 8 &amp; 9 &amp; 1 \\end{bmatrix} \\right)\\). Solution Exercise 4.9 Find the characteristic polynomial for \\(\\begin{bmatrix} 2 &amp; 0 &amp;0\\\\ 5 &amp; 6 &amp;0\\\\ 8 &amp; 9 &amp; 1 \\end{bmatrix}\\). Solution Exercise 4.10 Prove the following properties of the trace: \\(\\text{tr}(A+B) = \\text{tr}(A) + \\text{tr}(B)\\) \\(\\text{tr}(\\alpha A) = \\alpha \\text{tr}(A)\\) \\(\\text{tr}(I_n) = n\\) Solution Exercise 4.11 Solution Exercise 4.12 Solution "],["eigenvalues-and-eigenvectors.html", "4.2 Eigenvalues and Eigenvectors", " 4.2 Eigenvalues and Eigenvectors Eigenvalues and eigenvectors provide a way to characterize a matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) and its associated linear mapping. Definition 4.3 Eigenvalue &amp; Eigenvector: A scalar \\(\\lambda \\in \\mathbb{R}\\) is an eigenvalue of \\(\\mathbf{A}\\) and a nonzero vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\) is a corresponding eigenvector if \\[ \\mathbf{A}\\mathbf{x} = \\lambda \\mathbf{x}. \\] This is called the eigenvalue equation. Definition 4.4 The span of the set of eigenvectors associated with \\(\\lambda\\) spans a subspace \\(E_\\lambda \\subset \\mathbb{R}^n\\) known as the eigenspace. Definition 4.5 The set of all eigenvalues of \\(\\mathbf{A}\\) is called the eigenspectrum. Definition 4.6 Number of times \\(\\lambda\\) appears as a root of the characteristic polynomial \\(p_ \\mathbf{A}(\\lambda) = \\det(\\mathbf{A} - \\lambda I)\\) is called the algebraic multiplicity of the eigenvalue. Definition 4.7 The dimension of the eigenspace associated with \\(\\lambda\\) is called the geometric multiplicity. Example 4.10 Find the eigenspace(s) of the matrix \\[ \\mathbf{A} = \\begin{pmatrix} 2 &amp; 1\\\\ 0 &amp; 2 \\end{pmatrix}. \\] Step 1: Find the eigenvalues Compute the characteristic polynomial: \\[ \\det(\\mathbf{A} - \\lambda \\mathbf{I}) = \\det \\begin{pmatrix} 2-\\lambda &amp; 1\\\\ 0 &amp; 2-\\lambda \\end{pmatrix} = (2-\\lambda)^2. \\] So the only eigenvalue is \\[ \\lambda = 2 \\quad \\text{(with algebraic multiplicity 2).} \\] Step 2: Find the eigenspace for \\(\\lambda = 2\\) Form the matrix: \\[ \\mathbf{A} - 2\\mathbf{I} = \\begin{pmatrix} 2-2 &amp; 1\\\\ 0 &amp; 2-2 \\end{pmatrix} = \\begin{pmatrix} 0 &amp; 1\\\\ 0 &amp; 0 \\end{pmatrix}. \\] Solve: \\[ (\\mathbf{A} - 2\\mathbf{I})\\mathbf{x} = 0. \\] This gives: \\[ x_2 = 0. \\] So \\(x_1\\) is free, and the solution vectors have the form: \\[ \\mathbf{x} = \\begin{pmatrix}x_1\\\\ 0\\end{pmatrix} = x_1 \\begin{pmatrix}1\\\\ 0\\end{pmatrix}. \\] Step 3: Write the eigenspace The eigenspace is: \\[ E_2 = \\text{span}\\left\\{\\begin{pmatrix}1\\\\ 0\\end{pmatrix}\\right\\}. \\] So the eigenspace is one-dimensional, even though the eigenvalue has algebraic multiplicity 2. 4.2.1 Key Properties Eigenvalues and eigenvectors have several key properties: \\(\\mathbf{A}\\) and \\(\\mathbf{A}^\\top\\) have the same eigenvalues, not necessarily the same eigenvectors. Similar matrices have identical eigenvalues. Matrices \\(\\mathbf{A}\\) and \\(\\mathbf{D}\\) are similar if \\(\\mathbf{A} = \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}\\) for some matrix \\(\\mathbf{P}\\). Symmetric, positive definite matrices have real, positive eigenvalues. A matrix with \\(n\\) distinct eigenvalues has linearly independent eigenvectors forming a basis of \\(\\mathbb{R}^n\\). Defective matrices: Have fewer than \\(n\\) linearly independent eigenvectors. Example 4.11 Consider the matrix \\[ \\mathbf{A} = \\begin{bmatrix} 2 &amp; 1 \\\\ 0 &amp; 3 \\end{bmatrix} \\] and the invertible matrix \\[ \\mathbf{P} = \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix}. \\] We can compute a matrix \\(\\mathbf{B}\\) that is similar to \\(\\mathbf{A}\\) using the formula: \\[ \\mathbf{B} = \\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P} \\] First, find \\(\\mathbf{P}^{-1}\\): \\[ \\mathbf{P}^{-1} = \\begin{bmatrix} 1 &amp; -1 \\\\ 0 &amp; 1 \\end{bmatrix}. \\] Then, \\[ \\mathbf{B} = \\begin{bmatrix} 1 &amp; -1 \\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 2 &amp; 1 \\\\ 0 &amp; 3 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 3 \\end{bmatrix}. \\] Thus, \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are similar matrices, since there exists an invertible matrix \\(\\mathbf{P}\\) such that \\(\\mathbf{B} = \\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P}\\). One can verify that matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) have the same eigenvalues (\\(\\lambda = 2\\) and 3). Theorem 4.4 Spectral Theorem: If \\(\\mathbf{A}\\) is symmetric, there exists an orthonormal basis of \\(\\mathbb{R}^n\\) consisting of eigenvectors of \\(\\mathbf{A}\\) (and all eigenvalues are real). Furthermore, \\(\\mathbf{A}\\) can be decomposed as \\[ \\mathbf{A} = \\mathbf{P}\\mathbf{D}\\mathbf{P}^\\top \\] where \\(\\mathbf{P}\\) contains eigenvectors and \\(\\mathbf{D}\\) is diagonal with eigenvalues. Example 4.12 We illustrate the decomposition of \\[ \\mathbf{A} = \\begin{pmatrix} 2 &amp; 1\\\\ 1 &amp; 2 \\end{pmatrix}. \\] First, we need to find the eigenvalues/ eigenvectors for \\(\\mathbf{A}\\). \\[ \\det(\\mathbf{A} - \\lambda \\mathbf{I}) = \\det\\begin{pmatrix} 2-\\lambda &amp; 1\\\\ 1 &amp; 2-\\lambda \\end{pmatrix} = (2-\\lambda)^2 - 1. \\] So the eigenvalues are: \\[ \\lambda_1 = 3, \\qquad \\lambda_2 = 1. \\] For \\(\\lambda_1 = 3\\), \\[ (\\mathbf{A} - 3\\mathbf{I})\\mathbf{v}_1 = \\begin{pmatrix} -1 &amp; 1\\\\ 1 &amp; -1 \\end{pmatrix} \\begin{pmatrix}x\\\\y\\end{pmatrix} = 0 \\Rightarrow x = y. \\] Choose: \\[ \\mathbf{v}_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix}1\\\\1\\end{pmatrix}. \\] For \\(\\lambda_2 = 1\\), \\[ (\\mathbf{A}-\\mathbf{I})\\mathbf{v}_2 = \\begin{pmatrix} 1 &amp; 1\\\\ 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix}x\\\\y\\end{pmatrix} = 0 \\Rightarrow x = -y. \\] Choose: \\[ \\mathbf{v}_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix}1\\\\-1\\end{pmatrix}. \\] To form \\(\\mathbf{P}\\) and \\(\\mathbf{D}\\), take the normalized eigenvectors to form the columns of \\(\\mathbf{P}\\) and the eigenvalues to form the main diagonal of \\(\\mathbf{D}\\), \\[ \\mathbf{P} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}}\\\\[4pt] \\frac{1}{\\sqrt{2}} &amp; -\\frac{1}{\\sqrt{2}} \\end{pmatrix}, \\qquad \\mathbf{D} = \\begin{pmatrix} 3 &amp; 0\\\\ 0 &amp; 1 \\end{pmatrix}. \\] To check, \\[ \\mathbf{PDP}^\\top = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}} &amp; -\\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} 3 &amp; 0\\\\ 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}} &amp; -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 2 &amp; 1\\\\ 1 &amp; 2 \\end{pmatrix} = \\mathbf{A}. \\] 4.2.2 Relations to Determinant and Trace Eigenvalues and eigenvectors are related to the determinant and trace of a matrix. For example, \\[\\det(\\mathbf{A}) = \\prod_{i=1}^n \\lambda_i.\\] Furthermore, \\[\\text{tr}(\\mathbf{A}) = \\sum_{i=1}^n \\lambda_i.\\] Geometrically, eigenvectors are directions stretched by \\(\\lambda_i\\); determinant gives volume scaling, trace gives scaling of perimeter. Example 4.13 Let \\(\\mathbf{A} = \\begin{pmatrix}4 &amp; 2 \\\\ 1 &amp; 3\\end{pmatrix}\\). Then Eigenvalues: \\(\\lambda_1 = 2, \\;\\;\\; \\lambda_2 = 5\\) Eigenspaces: \\(E_2 = \\text{span}\\{[1, -1]^\\top\\}, \\;\\;\\; E_5 = \\text{span}\\{[2,1]^\\top\\}\\) Trace is \\(\\text{sum of diagonals of the matrix} = 4 + 3 = 7 = 5 + 2 = \\text{sum of the eigenvalues}\\) Determinant is \\(\\text{by the formula} = 4(3) - 2(1) = 10 = 5 \\times 2 = \\text{product of the eigenvalues}\\). Google’s PageRank Algorithm uses the eigenvector of the maximal eigenvalue (\\(\\lambda = 1\\)) of the web connectivity matrix to rank web pages. Exercises Exercise 4.13 Find the characteristic equation, eigenvalues, eigenspace, determinant, and trace corresponding to \\[\\mathbf{A} = \\begin{bmatrix} 1 &amp; 4\\\\3 &amp; 2 \\end{bmatrix}.\\] Solution Exercise 4.14 Find the characteristic equation, eigenvalues, eigenspace, determinant, and trace corresponding to \\[\\mathbf{A} = \\begin{bmatrix} 2 &amp; 2\\\\1 &amp; 3 \\end{bmatrix}.\\] Solution Exercise 4.15 Find the characteristic equation, eigenvalues, eigenspace, determinant, and trace corresponding to \\[\\mathbf{A} = \\begin{bmatrix}1&amp;0&amp;0\\\\0&amp;1&amp;2\\\\0&amp;0&amp;0\\end{bmatrix}.\\] Solution Exercise 4.16 Find the characteristic equation, eigenvalues, eigenspace, determinant, and trace corresponding to \\[\\mathbf{A} = \\begin{bmatrix}2&amp;0&amp;4\\\\0&amp;3&amp;0\\\\0&amp;1&amp;2\\end{bmatrix}.\\] Solution Exercise 4.17 Given a matrix \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\), we can always obtain a symmetric positive semidefinite matrix \\(\\mathbf{S} \\in \\mathbb{R}^{n \\times n}\\) by defining \\(\\mathbf{S} = \\mathbf{A}^T\\mathbf{A}\\). Prove this statement for \\(2 \\times 2\\), \\(3 \\times 2\\) and \\(2 \\times 3\\) matrices. Solution Exercise 4.18 Explain why geometrically, an eigenvector is a vector whose direction is unchanged by multiplying by matrix \\(\\mathbf{A}\\). Solution Exercise 4.19 Solution "],["cholesky-decomposition.html", "4.3 Cholesky Decomposition", " 4.3 Cholesky Decomposition The Cholesky decomposition is a square-root-like factorization for symmetric, positive definite matrices. It generalizes the concept of a square root from numbers to matrices. Theorem 4.5 Cholesky Decomposition: A symmetric, positive definite matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) can be factorized as: \\[ \\mathbf{A} = L L^\\top, \\] where \\(L\\) is a lower-triangular matrix with positive diagonal entries. The matrix \\(L\\) is called the Cholesky factor of \\(\\mathbf{A}\\) and is unique. Example 4.14 For \\[ \\mathbf{A} = \\begin{bmatrix} a_{11} &amp; a_{21} &amp; a_{31} \\\\ a_{21} &amp; a_{22} &amp; a_{32} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{bmatrix}, \\quad L = \\begin{bmatrix} l_{11} &amp; 0 &amp; 0 \\\\ l_{21} &amp; l_{22} &amp; 0 \\\\ l_{31} &amp; l_{32} &amp; l_{33} \\end{bmatrix}, \\] the components of \\(L\\) are computed as: \\[ \\begin{aligned} l_{11} &amp;= \\sqrt{a_{11}}, &amp; l_{22} &amp;= \\sqrt{a_{22} - l_{21}^2}, &amp; l_{33} &amp;= \\sqrt{a_{33} - (l_{31}^2 + l_{32}^2)}, \\\\ l_{21} &amp;= \\frac{a_{21}}{l_{11}}, &amp; l_{31} &amp;= \\frac{a_{31}}{l_{11}}, &amp; l_{32} &amp;= \\frac{a_{32} - l_{31}l_{21}}{l_{22}}. \\end{aligned} \\] Example 4.15 Let \\[ \\mathbf{A} = \\begin{bmatrix} 4 &amp; 2 &amp; 2 \\\\ 2 &amp; 5 &amp; 1 \\\\ 2 &amp; 1 &amp; 3 \\end{bmatrix} \\] We want to find a lower triangular matrix \\(\\mathbf{L}\\) such that \\[ \\mathbf{A} = \\mathbf{L}\\mathbf{L}^T. \\] Assume \\[ \\mathbf{L} = \\begin{bmatrix} l_{11} &amp; 0 &amp; 0 \\\\ l_{21} &amp; l_{22} &amp; 0 \\\\ l_{31} &amp; l_{32} &amp; l_{33} \\end{bmatrix}. \\] By multiplying \\(\\mathbf{L}\\mathbf{L}^T\\) and comparing entries with \\(\\mathbf{A}\\), we obtain: \\[\\begin{align*} l_{11}^2 = 4 &amp;\\Rightarrow l_{11} = 2 &amp; 2l_{21} = 2 &amp;\\Rightarrow l_{21} = 1\\\\ 2l_{31} = 2 &amp;\\Rightarrow l_{31} = 1 &amp;1^2 + l_{22}^2 = 5 &amp;\\Rightarrow l_{22} = 2 \\\\ 1(1) + 2l_{32} = 1 &amp;\\Rightarrow l_{32} = 0 &amp; 1^2 + 0^2 + l_{33}^2 = 3 &amp;\\Rightarrow l_{33} = \\sqrt{2} \\end{align*}\\] Therefore, \\[ \\mathbf{L} = \\begin{bmatrix} 2 &amp; 0 &amp; 0 \\\\ 1 &amp; 2 &amp; 0 \\\\ 1 &amp; 0 &amp; \\sqrt{2} \\end{bmatrix} \\] and \\[ \\mathbf{L}^T = \\begin{bmatrix} 2 &amp; 1 &amp; 1 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sqrt{2} \\end{bmatrix}, \\] and \\[ \\mathbf{A} = \\mathbf{L}\\mathbf{L}^T. \\] Cholesky decompositions provide an efficient computation of determinants: \\(\\det(\\mathbf{A}) = \\prod_i l_{ii}^2\\). They are also important to provide numerical stability in machine learning algorithms Example 4.16 Find the determinant of \\[ \\mathbf{A} = \\begin{bmatrix} 4 &amp; 2 &amp; 2 \\\\ 2 &amp; 5 &amp; 1 \\\\ 2 &amp; 1 &amp; 3 \\end{bmatrix}. \\] Since \\[ \\mathbf{A} = \\mathbf{L}\\mathbf{L}^T, \\] we have \\[ \\det(\\mathbf{A}) = \\det(\\mathbf{L}\\mathbf{L}^T) = \\det(\\mathbf{L}) \\det(\\mathbf{L}^T) =(2 \\times 2 \\times \\sqrt{2}) \\times (2 \\times 2 \\times \\sqrt{2}) = 32. \\] Exercises Exercise 4.20 Let \\(\\mathbf{A} = \\begin{bmatrix}9 &amp; 6 \\\\ 6 &amp; a \\end{bmatrix}\\), \\(a &gt; 4\\). Verify that this matrix satisfies the criteria for the Cholesky decomposition and find it. Hint, you are going to have to derive the equations yourself. Solution Exercise 4.21 Compute the Cholesky decomposition for \\[\\mathbf{A} = \\begin{bmatrix} 1&amp;0&amp;0\\\\0&amp;1&amp;0\\\\0&amp;0&amp;1 \\end{bmatrix}.\\] Solution Exercise 4.22 Compute the Cholesky decomposition for \\[\\mathbf{A} = \\begin{bmatrix} 5&amp;0&amp;0\\\\0&amp;10&amp;0\\\\0&amp;0&amp;0.17 \\end{bmatrix}.\\] Solution Exercise 4.23 Compute the Cholesky decomposition for \\[\\mathbf{A} = \\begin{bmatrix} 2&amp;-1&amp;0\\\\-1&amp;2&amp;-1\\\\0&amp;-1&amp;2 \\end{bmatrix}.\\] Solution Exercise 4.24 Compute the Cholesky decomposition for \\[\\mathbf{A} = \\begin{bmatrix} 25&amp;15&amp;-5\\\\15&amp;18&amp;0\\\\-5&amp;0&amp;11 \\end{bmatrix}.\\] Solution Exercise 4.25 Show that if \\(\\mathbf{A}\\) is a symmetric, positive definite matrix, then \\[det(\\mathbf{A}) = det(\\mathbf{L})^2= \\left(\\prod_{i=1}^n l_{ii}\\right)^2,\\] where \\(\\mathbf{L}\\) is the Cholesky factor of \\(\\mathbf{A}\\). Verify each step. Solution Exercise 4.26 Solution "],["eigendecomposition-and-diagonalization.html", "4.4 Eigendecomposition and Diagonalization", " 4.4 Eigendecomposition and Diagonalization Definition 4.8 A diagonal matrix has zeros on all off-diagonal elements: \\[ \\mathbf{D} = \\begin{bmatrix} c_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; c_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; c_n \\end{bmatrix}. \\] Lemma 4.1 Let \\(\\mathbf{D}\\) be a diagonal matrix. Then \\(\\mathbf{D}\\) has the following properties: \\(\\det(\\mathbf{D}) = \\prod_i c_i\\) \\(\\mathbf{D}^k = \\text{diag}(c_1^k, \\dots, c_n^k)\\) \\(\\mathbf{D}^{-1} = \\text{diag}(1/c_1, \\dots, 1/c_n)\\) (if all \\(c_i \\neq 0\\)) Example 4.17 Consider the diagonal matrix \\[ \\mathbf{D} = \\begin{bmatrix} 2 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 4 \\end{bmatrix}. \\] The determinant is the product of the diagonal entries: \\[ \\det(\\mathbf{D}) = 2 \\cdot (-1) \\cdot 3 \\cdot 4 = -24. \\] The inverse is obtained by taking the reciprocal of each diagonal entry: \\[ \\mathbf{D}^{-1} = \\begin{bmatrix} \\frac{1}{2} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{1}{3} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\frac{1}{4} \\end{bmatrix}. \\] Powers of a diagonal matrix are computed by raising each diagonal entry to the given power: \\[ \\mathbf{D}^5 = \\begin{bmatrix} 2^5 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; (-1)^5 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 3^5 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 4^5 \\end{bmatrix} = \\begin{bmatrix} 32 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 243 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1024 \\end{bmatrix}. \\] 4.4.1 Diagonalizable Matrices Definition 4.9 A matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) is diagonalizable if there exists an invertible matrix \\(\\mathbf{P}\\) such that: \\[ \\mathbf{D} = \\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P}, \\] where \\(\\mathbf{D}\\) is diagonal. A matrix \\(\\mathbf{A}\\) is diagonalizable if and only if \\(\\mathbf{A}\\) has \\(n\\) linearly independent eigenvectors \\[ \\mathbf{A}\\mathbf{P} = \\mathbf{P}\\mathbf{D} \\quad \\Leftrightarrow \\quad \\mathbf{A} p_i = \\lambda_i p_i, \\ i=1,\\dots,n. \\] Here, the columns of \\(\\mathbf{P}\\) are eigenvectors of \\(\\mathbf{A}\\), and the diagonal entries of \\(\\mathbf{D}\\) are the eigenvalues of \\(\\mathbf{A}\\). 4.4.2 Eigendecomposition Theorems Theorem 4.6 Let \\(\\mathbf{A}\\) be a matrix. Then there exist a diagonal matrix \\(\\mathbf{D}\\) and matrix \\(\\mathbf{P}\\) consisting of eigenvectors of \\(\\mathbf{A}\\) such that \\[ \\mathbf{A} = \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}, \\] if and only if eigenvectors of \\(\\mathbf{A}\\) form a basis of \\(\\mathbb{R}^n\\). Only non-defective matrices (ones with \\(n\\) linear independent eigenvectors) are diagonalizable. Theorem 4.7 A symmetric matrix \\(S \\in \\mathbb{R}^{n \\times n}\\) is always diagonalizable. By the spectral theorem, the eigenvectors can form an orthonormal basis (ONB), giving: \\[ \\mathbf{D} = \\mathbf{P}^\\top \\mathbf{S} \\mathbf{P}, \\] with \\(\\mathbf{P}\\) orthogonal. Geometrically, eigendecomposition represents a basis change to the eigenbasis. \\(\\mathbf{D}\\) scales vectors along eigenvectors by eigenvalues \\(\\lambda_i\\) while \\(\\mathbf{P}\\) maps scaled vectors back to the standard basis. Example 4.18 Consider the matrix \\[ \\mathbf{A} = \\begin{bmatrix} 4 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix}. \\] The characteristic polynomial is \\[ \\det(\\mathbf{A} - \\lambda \\mathbf{I}) = \\begin{vmatrix} 4 - \\lambda &amp; 1 \\\\ 1 &amp; 2 - \\lambda \\end{vmatrix} = (4 - \\lambda)(2 - \\lambda) - 1 = \\lambda^2 - 6\\lambda + 7 = 0. \\] Solving, \\[ \\lambda_{1,2} = 3 \\pm \\sqrt{2}. \\] The eigenvector for \\(\\lambda_1 = 3 + \\sqrt{2}\\) is found when we solve \\((\\mathbf{A} - \\lambda_1 \\mathbf{I})\\mathbf{v} = \\mathbf{0}\\): \\[ \\begin{bmatrix} 1 - \\sqrt{2} &amp; 1 \\\\ 1 &amp; -1 - \\sqrt{2} \\end{bmatrix} \\mathbf{v} = \\mathbf{0}. \\] A corresponding eigenvector is \\[ \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ \\sqrt{2} - 1 \\end{bmatrix}. \\] The eigenvector for \\(\\lambda_2 = 3 - \\sqrt{2}\\) is found when we solve \\((\\mathbf{A} - \\lambda_2 \\mathbf{I})\\mathbf{v} = \\mathbf{0}\\): \\[ \\begin{bmatrix} 1 + \\sqrt{2} &amp; 1 \\\\ 1 &amp; -1 + \\sqrt{2} \\end{bmatrix} \\mathbf{v} = \\mathbf{0}. \\] A corresponding eigenvector is \\[ \\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ - (1 + \\sqrt{2}) \\end{bmatrix}. \\] Let \\[ \\mathbf{P} = \\begin{bmatrix} 1 &amp; 1 \\\\ \\sqrt{2} - 1 &amp; -(1 + \\sqrt{2}) \\end{bmatrix}, \\quad \\mathbf{D} = \\begin{bmatrix} 3 + \\sqrt{2} &amp; 0 \\\\ 0 &amp; 3 - \\sqrt{2} \\end{bmatrix}. \\] Then the eigendecompositio* of \\(\\mathbf{A}\\) is \\[ \\mathbf{A} = \\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}. \\] Eigendecomposition requires square matrices. For general matrices, the Singular Value Decomposition (SVD) is used. Exercises Exercise 4.27 Discuss 2 reasons why we might want to complete an eigendecomposition. Solution Exercise 4.28 Find an eigendecomposition for \\[\\mathbf{A} = \\begin{bmatrix}0&amp;1\\\\ 3&amp;2 \\end{bmatrix}.\\] Use this to compute \\(\\mathbf{A}^4\\). Solution Exercise 4.29 Find an eigendecomposition for \\[\\mathbf{A} = \\begin{bmatrix}0&amp;2\\\\ 2&amp;3 \\end{bmatrix}.\\] Use this to compute \\(\\mathbf{A}^5\\). Solution Exercise 4.30 Find an eigendecomposition for \\[\\mathbf{A} = \\begin{bmatrix}2&amp;3\\\\ 2&amp;1 \\end{bmatrix}.\\] Use this to compute \\(\\mathbf{A}^3\\). Solution Exercise 4.31 Show that, if \\(\\mathbf{A}\\) has an eigendecomposition, that \\(det(\\mathbf{A}) = \\prod_{i=1}^n \\lambda_{i}\\), where the product is over the eigenvalues. Solution "],["singular-value-decomposition-svd.html", "4.5 Singular Value Decomposition (SVD)", " 4.5 Singular Value Decomposition (SVD) The Singular Value Decomposition (SVD) is a fundamental matrix decomposition method in linear algebra, applicable to all matrices (square or rectangular). It expresses a matrix \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) as: \\[ \\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top \\] where: - \\(\\mathbf{U} \\in \\mathbb{R}^{m \\times m}\\) is an orthogonal matrix of left-singular vectors \\(u_i\\), - \\(\\mathbf{V} \\in \\mathbb{R}^{n \\times n}\\) is an orthogonal matrix of right-singular vectors \\(v_j\\), - \\(\\mathbf{\\Sigma} \\in \\mathbb{R}^{m \\times n}\\) is diagonal with non-negative singular values \\(\\sigma_i\\) (ordered \\(\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0\\)). 4.5.1 Geometric Intuition The SVD can be interpreted as three sequential linear transformations: Basis change in the domain via \\(\\mathbf{V}^\\top\\) Scaling by singular values via \\(\\mathbf{\\Sigma}\\) and possibly dimension change Basis change in the codomain via \\(\\mathbf{U}\\) Unlike eigendecomposition, the domain and codomain in SVD can have different dimensions, and \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthonormal but generally not inverses of each other. Example 4.19 Consider the matrix \\[ \\mathbf{A} = \\begin{bmatrix} 3 &amp; 1 \\\\ 0 &amp; 2 \\end{bmatrix}. \\] It has a Singular Value Decomposition given by \\[ \\mathbf{A} = \\underbrace{\\mathbf{U}}_{\\text{basis change}} \\;\\; \\underbrace{\\boldsymbol{\\Sigma}}_{\\text{scaling}} \\;\\; \\underbrace{\\mathbf{V}^\\top}_{\\text{basis change}} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 2 \\end{bmatrix} \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; -1 \\end{bmatrix}. \\] Geometric Interpretation: \\(\\mathbf{V}^\\top\\) Rotates the input vector into a new orthonormal basis. \\(\\boldsymbol{\\Sigma}\\) Scales the coordinates by factors of 3 and 2 along perpendicular axes. \\(\\mathbf{U}\\) Rotates the result into the output space. 4.5.2 Construction of the SVD The following are the steps required to complete a SVD: Compute \\(\\mathbf{A}^\\top \\mathbf{A}\\) (symmetric, positive semidefinite) Diagonalize \\(\\mathbf{A}^\\top \\mathbf{A} = \\mathbf{P} D \\mathbf{P}^\\top\\) to obtain right-singular vectors \\(\\mathbf{V} = \\mathbf{P}\\) Singular values \\(\\sigma_i = \\sqrt{\\lambda_i}\\), where \\(\\lambda_i\\) are eigenvalues of \\(\\mathbf{A}^\\top \\mathbf{A}\\) Compute left-singular vectors \\(u_i = \\frac{1}{\\sigma_i} \\mathbf{A} v_i\\) Assemble \\(\\mathbf{U}, \\mathbf{\\Sigma}, \\mathbf{V}\\) to form \\(\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top\\) Example 4.20 Consider the matrix \\[ \\mathbf{A} = \\begin{bmatrix} 3 &amp; 1 \\\\ 0 &amp; 2 \\end{bmatrix}. \\] We will compute its Singular Value Decomposition \\[ \\mathbf{A} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^\\top. \\] Step 1: Compute \\(\\mathbf{A}^\\top \\mathbf{A}\\) \\[ \\mathbf{A}^\\top \\mathbf{A} = \\begin{bmatrix} 3 &amp; 0 \\\\ 1 &amp; 2 \\end{bmatrix} \\begin{bmatrix} 3 &amp; 1 \\\\ 0 &amp; 2 \\end{bmatrix} = \\begin{bmatrix} 9 &amp; 3 \\\\ 3 &amp; 5 \\end{bmatrix}. \\] Step 2: Singular Values The eigenvalues of \\(\\mathbf{A}^\\top \\mathbf{A}\\) satisfy \\[ \\det(\\mathbf{A}^\\top \\mathbf{A} - \\lambda \\mathbf{I}) = \\begin{vmatrix} 9 - \\lambda &amp; 3 \\\\ 3 &amp; 5 - \\lambda \\end{vmatrix} = \\lambda^2 - 14\\lambda + 36 = 0. \\] So, \\[ \\lambda_1 = 9, \\quad \\lambda_2 = 4. \\] The singular values are \\[ \\sigma_1 = 3, \\quad \\sigma_2 = 2. \\] Thus, \\[ \\boldsymbol{\\Sigma} = \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 2 \\end{bmatrix}. \\] Step 3: Right Singular Vectors (\\(\\mathbf{V}\\)) Eigenvectors of \\(\\mathbf{A}^\\top \\mathbf{A}\\): For \\(\\lambda = 9\\): eigenvector \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) For \\(\\lambda = 4\\): eigenvector \\(\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\) After normalization, \\[ \\mathbf{V} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; -1 \\end{bmatrix}. \\] This represents the first basis change in the domain. Step 4: Left Singular Vectors (\\(\\mathbf{U}\\)) Using \\(\\mathbf{U} = \\mathbf{A}\\mathbf{V}\\boldsymbol{\\Sigma}^{-1}\\), we obtain \\[ \\mathbf{U} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}. \\] (This matrix happens to be the identity here, meaning no rotation in the codomain.) Step 5: Final SVD \\[ \\mathbf{A} = \\underbrace{\\mathbf{U}}_{\\text{basis change}} \\;\\; \\underbrace{\\boldsymbol{\\Sigma}}_{\\text{scaling}} \\;\\; \\underbrace{\\mathbf{V}^\\top}_{\\text{basis change}} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 2 \\end{bmatrix} \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; -1 \\end{bmatrix}. \\] The SVD always exists for any matrix. For symmetric positive definite matrices, it coincides with the eigendecomposition. Example 4.21 Consider the \\(3 \\times 2\\) matrix \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 2 \\\\ 0 &amp; 0 \\end{bmatrix}. \\] We compute the Singular Value Decomposition of \\(\\mathbf{A}\\). Step 1: Compute \\(\\mathbf{A}^\\top \\mathbf{A}\\) \\[ \\mathbf{A}^\\top \\mathbf{A} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 2 \\\\ 0 &amp; 0 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 4 \\end{bmatrix}. \\] Step 2: Singular Values The eigenvalues of \\(\\mathbf{A}^\\top \\mathbf{A}\\) are \\[ \\lambda_1 = 4, \\quad \\lambda_2 = 1. \\] Thus the singular values are \\[ \\sigma_1 = 2, \\quad \\sigma_2 = 1 \\;\\;\\; \\Longrightarrow \\;\\;\\; \\boldsymbol{\\Sigma} = \\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix}. \\] Step 3: Right Singular Vectors (\\(\\mathbf{V}\\)) Since \\(\\mathbf{A}^\\top \\mathbf{A}\\) is diagonal, \\[ \\mathbf{V} = \\begin{bmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{bmatrix} \\quad \\text{(any orthonormal ordering is valid)}. \\] This matrix represents a basis change in the domain \\(\\mathbb{R}^2\\). Step 4: Left Singular Vectors (\\(\\mathbf{U}\\)) Compute \\[ \\mathbf{U} = \\mathbf{A}\\mathbf{V}\\boldsymbol{\\Sigma}^{-1}. \\] The resulting orthonormal basis is \\[ \\mathbf{U} = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] This is a basis for \\(\\mathbb{R}^3\\), extending the image of \\(\\mathbf{A}\\) to a full orthonormal basis. Step 5: Final SVD \\[ \\mathbf{A} = \\underbrace{\\mathbf{U}}_{\\text{basis change in } \\mathbb{R}^3} \\underbrace{\\boldsymbol{\\Sigma}}_{\\text{scaling}} \\underbrace{\\mathbf{V}^\\top}_{\\text{basis change in } \\mathbb{R}^2}. \\] Explicitly, \\[ \\mathbf{A} = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} \\begin{bmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{bmatrix}. \\] 4.5.3 Comparison: Eigenvalue Decomposition vs SVD Feature Eigendecomposition SVD Matrix type Square only Any \\(m \\times n\\) Basis vectors Not necessarily orthonormal Orthonormal (U, V) Diagonal entries Eigenvalues (can be negative/complex) Non-negative singular values Basis change Same vector space Domain and codomain can differ Relation to eigenvectors Only eigenvectors of square matrices Left-singular vectors = eigenvectors of \\(\\mathbf{A}\\mathbf{A}^\\top\\), Right-singular = eigenvectors of \\(\\mathbf{A}^\\top \\mathbf{A}\\) Exercises Exercise 4.32 Find a SVD of \\[\\mathbf{A} = \\begin{bmatrix}3&amp;0\\\\4&amp;5 \\end{bmatrix}.\\] Solution Exercise 4.33 Find a SVD of \\[\\mathbf{A} = \\begin{bmatrix}3&amp;2&amp;2\\\\2&amp;3&amp;-2 \\end{bmatrix}.\\] Solution Exercise 4.34 Find a SVD of \\[\\mathbf{A} = \\begin{bmatrix}4&amp;0\\\\3&amp;-5 \\end{bmatrix}.\\] Solution Exercise 4.35 Find a SVD of \\[\\mathbf{A} = \\begin{bmatrix}1&amp;0&amp;1\\\\-1&amp;1&amp;0 \\end{bmatrix}.\\] Solution Exercise 4.36 Find a SVD of \\[\\mathbf{A} = \\begin{bmatrix}1&amp;-1\\\\0&amp;1\\\\1&amp;0 \\end{bmatrix}.\\] Solution Exercise 4.37 Find a SVD of \\[\\mathbf{A} = \\begin{bmatrix}1&amp;1&amp;1\\\\ -1&amp;0&amp;-2\\\\ 1&amp;2&amp;0 \\end{bmatrix}.\\] Solution "],["matrix-approximation-via-svd.html", "4.6 Matrix Approximation via SVD", " 4.6 Matrix Approximation via SVD The SVD of a matrix \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\): \\[ \\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top \\] allows us to represent \\(\\mathbf{A}\\) as a sum of rank-1 matrices: \\[ \\mathbf{A}_i := u_i v_i^\\top, \\quad \\mathbf{A} = \\sum_{i=1}^r \\sigma_i \\mathbf{A}_i, \\] where \\(r = \\text{rank}(\\mathbf{A})\\) and \\(\\sigma_i\\) are singular values. Definition 4.10 A rank-k approximation of \\(\\mathbf{A}\\) (with \\(k &lt; r\\)) is: \\[ \\mathbf{A}^{(k)} = \\sum_{i=1}^k \\sigma_i u_i v_i^\\top. \\] A rank-\\(k\\) approximation reduces storage and computation costs compared to the original. For example, a \\(1432 \\times 1910\\) image approximated with rank-5 requires 16,715 numbers instead of 2,735,120 (~0.6%). Example 4.22 Write \\(\\mathbf{A}\\) as the sum of rank 1 matrices where \\[ \\mathbf{A} = \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}. \\] Step 1: Singular Value Decomposition Since \\(\\mathbf{A}\\) is diagonal with positive entries, its SVD is especially simple: \\[ \\mathbf{A} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^\\top, \\] where \\[ \\mathbf{U} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}, \\quad \\boldsymbol{\\Sigma} = \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}, \\quad \\mathbf{V} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}. \\] The singular values are \\[ \\sigma_1 = 3, \\quad \\sigma_2 = 1. \\] Step 2: Rank-1 Decomposition The SVD gives the expansion \\[ \\mathbf{A} = \\sum_{i=1}^2 \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top, \\] where \\(\\mathbf{u}_i\\) and \\(\\mathbf{v}_i\\) are the columns of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\). Step 3: Individual Rank-1 Matrices First term: \\[ \\sigma_1 \\mathbf{u}_1 \\mathbf{v}_1^\\top = 3 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 \\end{bmatrix} = \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}. \\] Second term: \\[ \\sigma_2 \\mathbf{u}_2 \\mathbf{v}_2^\\top = 1 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\begin{bmatrix} 0 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}. \\] Each matrix has rank 1. Step 4: Sum of Rank-1 Matrices Adding the two rank-1 matrices: \\[ \\mathbf{A} = \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} + \\begin{bmatrix} 0 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}. \\] Each term \\(\\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top\\) is a rank-1 matrix. The SVD expresses \\(\\mathbf{A}\\) as a sum of rank-1 outer products. Truncating this sum gives the best low-rank approximation of \\(\\mathbf{A}\\) (in the least-squares sense). 4.6.1 Error Measurement The spectral norm of a matrix \\(\\mathbf{A}\\) is: \\[ \\|\\mathbf{A}\\|_2 := \\max_{x \\neq 0} \\frac{\\|\\mathbf{A}x\\|_2}{\\|x\\|_2}. \\] The spectral norm of \\(\\mathbf{A}\\) is its largest singular value \\(\\sigma_1\\). Theorem 4.8 Eckart-Young Theorem: For any rank-\\(k\\) approximation \\(\\mathbf{A}^{(k)}\\): \\[ \\mathbf{A}^{(k)} = \\arg \\min_{\\text{rank}(\\mathbf{B}) = k} \\|\\mathbf{A} - \\mathbf{B}\\|_2 \\] \\[ \\|\\mathbf{A} - \\mathbf{A}^{(k)}\\|_2 = \\sigma_{k+1} \\] The SVD provides the best low-rank approximation in the spectral norm sense. This is used for lossy compression, dimensionality reduction, noise filtering, and regularization. Example 4.23 Consider the matrix \\[ \\mathbf{A} = \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}. \\] Since \\(\\mathbf{A}\\) is diagonal, its singular values are simply the diagonal entries in descending order: \\[ \\sigma_1 = 3, \\quad \\sigma_2 = 1. \\] We can write the SVD as \\[ \\mathbf{A} = U \\Sigma V^\\top \\] with \\[ U = V = I_2, \\quad \\Sigma = \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}. \\] The rank-1 approximation of \\(\\mathbf{A}\\) is obtained by keeping only the largest singular value \\(\\sigma_1 = 3\\): \\[ \\mathbf{A}_1 = \\sigma_1 u_1 v_1^\\top = 3 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 \\end{bmatrix} = \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}. \\] The Eckart-Young Theorem states that this rank-1 approximation \\(\\mathbf{A}_1\\) is optimal in the sense that it minimizes the spectral norm of the difference: \\[ \\|\\mathbf{A} - \\mathbf{A}_1\\|_2 = \\sigma_2 = 1. \\] Indeed: \\[ \\mathbf{A} - \\mathbf{A}_1 = \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} - \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}. \\] The largest singular value of this difference is 1, which matches \\(\\sigma_2\\). Therefore, \\(\\mathbf{A}_1\\) captures the most significant component of \\(\\mathbf{A}\\). The error in the approximation, measured by the 2-norm, is exactly the next singular value \\(\\sigma_2 = 1\\). Exercises Exercise 4.38 Find a SVD of \\[\\mathbf{A} = \\begin{bmatrix}3&amp;0\\\\4&amp;5 \\end{bmatrix}.\\] Find the rank 1 approximation for \\(\\mathbf{A}\\). Solution Exercise 4.39 Find a SVD of \\[\\mathbf{A} = \\begin{bmatrix}3&amp;2&amp;2\\\\2&amp;3&amp;-2 \\end{bmatrix}.\\] Find the rank 1 approximation for \\(\\mathbf{A}\\). Solution Exercise 4.40 Find a SVD of \\[\\mathbf{A} = \\begin{bmatrix}4&amp;0\\\\3&amp;-5 \\end{bmatrix}.\\] Find the rank 1 approximation for \\(\\mathbf{A}\\). Solution Exercise 4.41 Find a SVD of \\[\\mathbf{A} = \\begin{bmatrix}1&amp;0&amp;1\\\\-1&amp;1&amp;0 \\end{bmatrix}.\\] Find the rank 1 approximation for \\(\\mathbf{A}\\). Solution Exercise 4.42 Find a SVD of \\[\\mathbf{A} = \\begin{bmatrix}1&amp;-1\\\\0&amp;1\\\\1&amp;0 \\end{bmatrix}.\\] Find the rank 1 approximation for \\(\\mathbf{A}\\). Solution Exercise 4.43 Find a SVD of \\[\\mathbf{A} = \\begin{bmatrix}1&amp;1&amp;1\\\\ -1&amp;0&amp;-2\\\\ 1&amp;2&amp;0 \\end{bmatrix}.\\] Find the rank 1 approximation for \\(\\mathbf{A}\\). Solution "],["matrix-phylogeny-overview.html", "4.7 Matrix Phylogeny (Overview)", " 4.7 Matrix Phylogeny (Overview) Matrices can be classified based on properties and decompositions: Matrix type Property Square, invertible Determinant \\(\\neq 0\\) Non-defective Diagonalizable, has \\(n\\) independent eigenvectors Normal \\(\\mathbf{A}^\\top \\mathbf{A} = \\mathbf{A}\\mathbf{A}^\\top\\) Orthogonal \\(\\mathbf{A}^\\top \\mathbf{A} = \\mathbf{A}\\mathbf{A}^\\top = I\\), subset of invertible matrices Symmetric \\(S = S^\\top\\), real eigenvalues Positive definite \\(x^\\top P \\mathbf{x} &gt; 0\\) for all \\(x \\neq 0\\), unique Cholesky decomposition Diagonal Closed under multiplication/addition, special case: identity matrix \\(I\\) SVD exists for all real matrices, square or rectangular. Eigenvalue decomposition exists only for non-defective square matrices. The phylogenetic relationships between matrix types help organize matrix operations and decompositions. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
