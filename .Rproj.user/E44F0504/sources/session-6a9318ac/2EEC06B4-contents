# Matrix Decompositions



Matrices provide a compact way to represent linear mappings and data, where rows often correspond to observations and columns correspond to features. This chapter focuses on three central questions about matrices:

1. How to summarize a matrix with key numerical characteristics.   
2. How to decompose a matrix into simpler, interpretable components.  
3. How to use these decompositions for approximations and analysis.



<p align="center">
<img src="Figure4.1MML.png" alt="A mind map of the concepts introduced in this chapter, along with where they are used in other parts of the book." width="400">
</p>




---

##  Determinant and Trace




The **determinant** of a square matrix \( A \in \mathbb{R}^{n \times n} \), denoted as \(\det(A)\) or \(|A|\), is a scalar that characterizes several key properties of \( A \).

For small matrices:

\[
\det \begin{pmatrix} a_{11} \end{pmatrix} = a_{11}, \quad
\det \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} = a_{11}a_{22} - a_{12}a_{21}.
\]


<div class="example">
Put example here
</div>

For larger matrices, we can compute determinants recursively using the **Laplace expansion**:

\[
\det(A) = \sum_{k=1}^n (-1)^{k+j} a_{kj} \det(A_{k,j}),
\]
where \( A_{k,j} \) is the submatrix obtained by removing row \( k \) and column \( j \).



<div class="example">
Put example here
</div>

A matrix \( A \) is **invertible** if and only if \( \det(A) \neq 0 \).  


<div class="example">
Put example here
</div>


For triangular matrices, the determinant equals the product of the diagonal elements. 


<div class="example">
Put example here
</div>

The determinant changes sign when two rows (or columns) are swapped, and scales when a row is multiplied by a scalar.



<div class="example">
Put example here
</div>


### Geometric Interpretation

The determinant measures the **signed volume** of the parallelepiped spanned by the columns of \( A \):

- In \( \mathbb{R}^2 \): \( |\det(A)| \) gives the **area** of a parallelogram.  
- In \( \mathbb{R}^3 \): \( |\det(A)| \) gives the **volume** of a parallelepiped.

If the determinant is zero, the columns are **linearly dependent** and the volume collapses to zero.



### Properties of the Determinant


<div class="theorem">
For square matrices $A$ and $B$ and $\lambda \in \mathbb{R}$, the following properties hold:
\[
\begin{aligned}
\det(AB) &= \det(A)\det(B), \\
\det(A^\top) &= \det(A), \\
\det(A^{-1}) &= \frac{1}{\det(A)}, \\
\det(\lambda A) &= \lambda^n \det(A).
\end{aligned}
\]
</div>



<div class="example">
Put example here
</div>

<div class="theorem">
A matrix is **invertible** if and only if it is **full rank**, i.e. \( \text{rank}(A) = n \).
</div>



<div class="example">
Put example here
</div>

---

### Trace



<div class="definition">
The **trace** of a square matrix \( A \in \mathbb{R}^{n \times n} \) is the sum of its diagonal elements:
\[
\text{tr}(A) = \sum_{i=1}^n a_{ii}.
\]
</div>



<div class="theorem">
For square matrices $A$ and $B$ and $\alpha \in \mathbb{R}$, the following properties hold:
\[
\begin{aligned}
\text{tr}(A + B) &= \text{tr}(A) + \text{tr}(B), \\
\text{tr}(\alpha A) &= \alpha \, \text{tr}(A), \\
\text{tr}(AB) &= \text{tr}(BA), \\
\text{tr}(I_n) &= n.
\end{aligned}
\]
</div>


The trace is **invariant under cyclic permutations**, meaning \(\text{tr}(AKL) = \text{tr}(KLA)\).  It is also **independent of basis**, so the trace of a linear map \( \Phi \) is the same in all matrix representations.



<div class="example">
Put example here
</div>

---

### Characteristic Polynomial



<div class="definition">
The **characteristic polynomial** of a square matrix \( A \) is defined as:
\[
p_A(\lambda) = \det(A - \lambda I) = c_0 + c_1 \lambda + \cdots + c_{n-1} \lambda^{n-1} + (-1)^n \lambda^n.
\]
</div>

The characteristic polynomial for $A$ encodes key properties of \( A \):
\[
c_0 = \det(A), \quad
c_{n-1} = (-1)^{n-1} \text{tr}(A).
\]
The roots of this polynomial are the **eigenvalues** of \( A \), which will be explored in the next section.

---



### Exercises {.unnumbered .unlisted}


Put some exercises here.






## Eigenvalues and Eigenvectors

Eigenvalues and eigenvectors provide a way to characterize a matrix \(A \in \mathbb{R}^{n \times n}\) and its associated linear mapping.



<div class="definition">
**Eigenvalue & Eigenvector**:  
  A scalar \(\lambda \in \mathbb{R}\) is an eigenvalue of \(A\) and a nonzero vector \(x \in \mathbb{R}^n\) is a corresponding eigenvector if  
  \[
  Ax = \lambda x.
  \]  
This is called the **eigenvalue equation**.
</div>




<div class="definition">
The span of the set of eigenvectors associated with \(\lambda\) spans a subspace \(E_\lambda \subset \mathbb{R}^n\) known as the **eigenspace**.
</div>



<div class="definition">
The set of all eigenvalues of \(A\) is called the **eigenspectrum**.
</div>




<div class="definition">
Number of times \(\lambda\) appears as a root of the characteristic polynomial \(p_A(\lambda) = \det(A - \lambda I)\) is called the **algebraic multiplicity** of the eigenvalue.
</div>




<div class="definition">
The dimension of the eigenspace associated with \(\lambda\) is called the **geometric multiplicity**.
</div>


### Key Properties

Eigenvalues and eigenvectors have several key properties:

- \(A\) and \(A^\top\) have the same eigenvalues, not necessarily the same eigenvectors.  
- Similar matrices have identical eigenvalues. Matrices $A$ and $D$ are similar if $A = P D P^\top$ for some matrix $P$.   
- Symmetric, positive definite matrices have real, positive eigenvalues.  
- A matrix with \(n\) distinct eigenvalues has linearly independent eigenvectors forming a basis of \(\mathbb{R}^n\).  
- **Defective matrices**: Have fewer than \(n\) linearly independent eigenvectors.  


<div class="example">
Consider the matrix 

\[
A = 
\begin{bmatrix} 
2 & 1 \\ 
0 & 3 
\end{bmatrix}
\]

and the invertible matrix 

\[
P = 
\begin{bmatrix} 
1 & 1 \\ 
0 & 1 
\end{bmatrix}.
\]

We can compute a matrix \(B\) that is **similar** to \(A\) using the formula:

\[
B = P^{-1} A P
\]

First, find \(P^{-1}\):

\[
P^{-1} = 
\begin{bmatrix} 
1 & -1 \\ 
0 & 1 
\end{bmatrix}.
\]

Then,

\[
B = 
\begin{bmatrix} 1 & -1 \\ 0 & 1 \end{bmatrix}
\begin{bmatrix} 2 & 1 \\ 0 & 3 \end{bmatrix}
\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}
=
\begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}.
\]

Thus, \(A\) and \(B\) are **similar matrices**, since there exists an invertible matrix \(P\) such that \(B = P^{-1} A P\).
</div>

<div class="theorem">
**Spectral Theorem:** If \(A\) is symmetric, there exists an orthonormal basis of $\mathbb{R}^n$ consisting of eigenvectors of $A$ (and all eigenvalues are real).  Furthermore, $A$ can be decomposed as  
  \[
  A = P D P^\top
  \]  
  where \(P\) contains eigenvectors and \(D\) is diagonal with eigenvalues.
</div>




<div class="example">
Put example here
</div>


### Relations to Determinant and Trace

Eigenvalues and eigenvectors are related to the determinant and trace of a matrix.  For example, \[\det(A) = \prod_{i=1}^n \lambda_i.\]  Furthermore, \[\text{tr}(A) = \sum_{i=1}^n \lambda_i.\]  Geometrically, eigenvectors are directions stretched by \(\lambda_i\); determinant gives volume scaling, trace gives scaling of perimeter/trace.



<div class="example">
Let $A = \begin{pmatrix}4 & 2 \\ 1 & 3\end{pmatrix}$.  Then  

-  Eigenvalues: $\lambda_1 = 2, \;\;\; \lambda_2 = 5$  
-  Eigenspaces: $E_2 = \text{span}\{[1, -1]^\top\}, \;\;\;  E_5 = \text{span}\{[2,1]^\top\}$
</div>

**Google's PageRank Algorithm** uses the eigenvector of the maximal eigenvalue (\(\lambda = 1\)) of the web connectivity matrix to rank web pages. 



---



### Exercises {.unnumbered .unlisted}


Put some exercises here.






## Cholesky Decomposition

The **Cholesky decomposition** is a square-root-like factorization for symmetric, positive definite matrices. It generalizes the concept of a square root from numbers to matrices.




<div class="theorem">
**Cholesky Decomposition:**  A symmetric, positive definite matrix \(A \in \mathbb{R}^{n \times n}\) can be factorized as:
\[
A = L L^\top,
\]
where \(L\) is a **lower-triangular matrix** with positive diagonal entries. The matrix \(L\) is called the **Cholesky factor** of \(A\) and is unique.
</div>



<div class="example">
For
\[
A = \begin{bmatrix}
a_{11} & a_{21} & a_{31} \\
a_{21} & a_{22} & a_{32} \\
a_{31} & a_{32} & a_{33}
\end{bmatrix},
\quad
L = \begin{bmatrix}
l_{11} & 0 & 0 \\
l_{21} & l_{22} & 0 \\
l_{31} & l_{32} & l_{33}
\end{bmatrix},
\]
the components of \(L\) are computed as:
\[
\begin{aligned}
l_{11} &= \sqrt{a_{11}}, & l_{22} &= \sqrt{a_{22} - l_{21}^2}, & l_{33} &= \sqrt{a_{33} - (l_{31}^2 + l_{32}^2)}, \\
l_{21} &= \frac{a_{21}}{l_{11}}, & l_{31} &= \frac{a_{31}}{l_{11}}, & l_{32} &= \frac{a_{32} - l_{31}l_{21}}{l_{22}}.
\end{aligned}
\]
</div>



<div class="example">
Put one here
</div>


Cholesky decompositions provide an efficient computation of determinants: \(\det(A) = \prod_i l_{ii}^2\).  They are also important to provide numerical stability in machine learning algorithms





<div class="example">
Put one here
</div>



---



### Exercises {.unnumbered .unlisted}


Put some exercises here.







## Eigendecomposition and Diagonalization



<div class="definition">
A **diagonal matrix** has zeros on all off-diagonal elements:
\[
D = \begin{bmatrix}
c_1 & 0 & \cdots & 0 \\
0 & c_2 & \cdots & 0 \\
\vdots & & \ddots & \vdots \\
0 & 0 & \cdots & c_n
\end{bmatrix}.
\]
</div>




<div class="lemma">
Let $D$ be a diagonal matrix.  Then $D$ has the following properties:

- \(\det(D) = \prod_i c_i\)  
- \(D^k = \text{diag}(c_1^k, \dots, c_n^k)\)  
- \(D^{-1} = \text{diag}(1/c_1, \dots, 1/c_n)\) (if all \(c_i \neq 0\))  
</div>

### Diagonalizable Matrices



<div class="definition">
A matrix \(A \in \mathbb{R}^{n \times n}\) is **diagonalizable** if there exists an invertible matrix \(P\) such that:
\[
D = P^{-1} A P,
\]
where \(D\) is diagonal.
</div>

A matrix $A$ is diagonalizable **if and only if** \(A\) has \(n\) linearly independent eigenvectors
\[
AP = PD \quad \Leftrightarrow \quad A p_i = \lambda_i p_i, \ i=1,\dots,n.
\]
Here, the columns of \(P\) are eigenvectors of \(A\), and the diagonal entries of \(D\) are the eigenvalues of \(A\).

### Eigendecomposition Theorems


<div class="theorem">
Let $A$ be a matrix.  Then there exist a diagonal matrix $D$ and matrix $P$ consisting of eigenvectors of $A$ such that
\[
A = P D P^{-1},
\]
if and only if eigenvectors of \(A\) form a basis of \(\mathbb{R}^n\). Only **non-defective matrices** (ones with $n$ linear independent eigenvectors) are diagonalizable.
</div>



<div class="theorem">
A symmetric matrix \(S \in \mathbb{R}^{n \times n}\) is always diagonalizable. By the spectral theorem, the eigenvectors can form an orthonormal basis (ONB, giving:
\[
D = P^\top S P,
\]
with \(P\) orthogonal.
</div>

Geometrically, eigendecomposition represents a basis change to the eigenbasis.  \(D\) scales vectors along eigenvectors by eigenvalues \(\lambda_i\) while \(P\) maps scaled vectors back to the standard basis.


<div class="example">
For
\[
A = \frac{1}{2} \begin{bmatrix} 5 & -2 \\ -2 & 5 \end{bmatrix},
\]

- Eigenvalues: \(\lambda_1 = \frac{7}{2}, \lambda_2 = \frac{3}{2}\)
- Eigenvectors (orthonormal):
\[
p_1 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -1 \end{bmatrix}, \quad
p_2 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix}.
\]
Then
\[
P^{-1} A P = D = \begin{bmatrix} 7/2 & 0 \\ 0 & 3/2 \end{bmatrix}, \quad A^k = P D^k P^{-1}
\]

- Determinant: \(\det(A) = \prod_i d_{ii}\)
</div>



<div class="note">
Eigendecomposition requires square matrices. For general matrices, the **Singular Value Decomposition (SVD)** is used.
</div>






---



### Exercises {.unnumbered .unlisted}


Put some exercises here.




## Singular Value Decomposition (SVD)

The Singular Value Decomposition (SVD) is a fundamental matrix decomposition method in linear algebra, applicable to all matrices (square or rectangular). It expresses a matrix \(A \in \mathbb{R}^{m \times n}\) as:

\[
A = U \Sigma V^\top
\]

where:  
- \(U \in \mathbb{R}^{m \times m}\) is an orthogonal matrix of **left-singular vectors** \(u_i\),  
- \(V \in \mathbb{R}^{n \times n}\) is an orthogonal matrix of **right-singular vectors** \(v_j\),  
- \(\Sigma \in \mathbb{R}^{m \times n}\) is diagonal with **non-negative singular values** \(\sigma_i\) (ordered \(\sigma_1 \ge \sigma_2 \ge \dots \ge 0\)).

### Geometric Intuition

The SVD can be interpreted as three sequential linear transformations:  

1. **Basis change** in the domain via \(V^\top\)  
2. **Scaling** by singular values via \(\Sigma\) and possibly dimension change  
3. **Basis change** in the codomain via \(U\)  

Unlike eigendecomposition, the domain and codomain in SVD can have different dimensions, and \(U\) and \(V\) are orthonormal but generally not inverses of each other.

<div class="example">
Put example here
</div>

### Construction of the SVD

The following are the steps required to complete a SVD:

1. Compute \(A^\top A\) (symmetric, positive semidefinite)  
2. Diagonalize \(A^\top A = P D P^\top\) to obtain right-singular vectors \(V = P\)  
3. Singular values \(\sigma_i = \sqrt{\lambda_i}\), where \(\lambda_i\) are eigenvalues of \(A^\top A\)  
4. Compute left-singular vectors \(u_i = \frac{1}{\sigma_i} A v_i\)  
5. Assemble \(U, \Sigma, V\) to form \(A = U \Sigma V^\top\)  

<div class="example">
Put example here
</div>


<div class="note">
The SVD always exists for any matrix. For symmetric positive definite matrices, it coincides with the eigendecomposition.
</div>

<div class="example">
Put example here
</div>

### Comparison: Eigenvalue Decomposition vs SVD

| Feature | Eigendecomposition | SVD |
|---------|------------------|-----|
| Matrix type | Square only | Any \(m \times n\) |
| Basis vectors | Not necessarily orthonormal | Orthonormal (U, V) |
| Diagonal entries | Eigenvalues (can be negative/complex) | Non-negative singular values |
| Basis change | Same vector space | Domain and codomain can differ |
| Relation to eigenvectors | Only eigenvectors of square matrices | Left-singular vectors = eigenvectors of \(AA^\top\), Right-singular = eigenvectors of \(A^\top A\) |



<div class="example">
Put example here
</div>


---



### Exercises {.unnumbered .unlisted}


Put some exercises here.








##  Matrix Approximation via SVD

The SVD of a matrix \(A \in \mathbb{R}^{m \times n}\):
\[
A = U \Sigma V^\top
\]
allows us to represent \(A\) as a sum of **rank-1 matrices**:
\[
A_i := u_i v_i^\top, \quad 
A = \sum_{i=1}^r \sigma_i A_i,
\]
where \(r = \text{rank}(A)\) and \(\sigma_i\) are singular values.  




<div class="definition">
A **rank-k approximation** of \(A\) (with \(k < r\)) is:
\[
A^{(k)} = \sum_{i=1}^k \sigma_i u_i v_i^\top.
\]
</div>

A rank-$k$ approximation reduces storage and computation costs compared to the original.  For example, a \(1432 \times 1910\) image approximated with rank-5 requires 16,715 numbers instead of 2,735,120 (~0.6%).  

### Error Measurement

The **spectral norm** of a matrix \(A\) is:
\[
\|A\|_2 := \max_{x \neq 0} \frac{\|Ax\|_2}{\|x\|_2}.
\]
The spectral norm of \(A\) is its largest singular value \(\sigma_1\).  



<div class="theorem">
**Eckart-Young Theorem:** For any rank-\(k\) approximation \(A^{(k)}\):
\[
A^{(k)} = \arg \min_{\text{rank}(B) = k} \|A - B\|_2
\]
\[
\|A - A^{(k)}\|_2 = \sigma_{k+1}
\]
</div>

The SVD provides the best low-rank approximation in the spectral norm sense.  This is used for **lossy compression**, dimensionality reduction, noise filtering, and regularization.




---


### Exercises {.unnumbered .unlisted}


Put some exercises here.




## 4.7 Matrix Phylogeny (Overview)

Matrices can be classified based on properties and decompositions:

| Matrix type | Property |
|------------|----------|
| Square, invertible | Determinant \(\neq 0\) |
| Non-defective | Diagonalizable, has \(n\) independent eigenvectors |
| Normal | \(A^\top A = AA^\top\) |
| Orthogonal | \(A^\top A = AA^\top = I\), subset of invertible matrices |
| Symmetric | \(S = S^\top\), real eigenvalues |
| Positive definite | \(x^\top P x > 0\) for all \(x \neq 0\), unique Cholesky decomposition |
| Diagonal | Closed under multiplication/addition, special case: identity matrix \(I\) |

SVD exists for all real matrices, square or rectangular.  Eigenvalue decomposition exists only for non-defective square matrices.  The phylogenetic relationships between matrix types help organize matrix operations and decompositions.





---



### Exercises {.unnumbered .unlisted}


Put some exercises here.


