# Analytic Geometry

In Chapter 2, we explored **vectors**, **vector spaces**, and **linear mappings** at an abstract level.  This chapter introduces **geometric interpretation** and **intuition** to these ideas, focusing on:

- **Lengths** and **distances** between vectors  
- **Angles** between vectors  
- **Inner products** and their induced geometry  
- **Orthogonal projections** (important for PCA and regression)  

The **inner product** introduces geometry into vector spaces by defining similarity, length, and distance.




<p align="center">
<img src="Figure3.1MML.png" alt="A mind map of the concepts introduced in this chapter, along with when they are used in other parts of the book." width="400">
</p>




---

##  Norms

A **norm** measures the length of a vector.


<div class="definition"> 
A norm on a vector space \(V\) is a function:
\[
\| \cdot \| : V \to \mathbb{R}, \quad x \mapsto \|x\|
\]
satisfying:

1. **Absolute homogeneity:** \(\|\lambda x\| = |\lambda| \|x\|\)  
2. **Triangle inequality:** \(\|x + y\| \le \|x\| + \|y\|\)  
3. **Positive definiteness:** \(\|x\| \ge 0\) and \(\|x\| = 0 \iff x = 0\)  
</div>

Geometrically, the triangle inequality means that in any triangle, the sum of any two sides is at least the length of the third.



<div class="example">
An example of a norm is the Manhattan Norm (or $l_1$ norm):
\[
\|x\|_1 = \sum_{i=1}^n |x_i|.
\]
One can show that it satisfies all of norm properties given in the definition.
</div>




<div class="example">
Another example of a norm is the Euclidean Norm (or $l_2$ norm):
\[
\|x\|_2 = \sqrt{\sum_{i=1}^n x_i^2} = \sqrt{x^\top x}.
\]
Again, this function satisfies all of the properties of a norm.
</div>


<div class="note"> 
The **Euclidean norm** is the default norm used in the book and these notes.
</div>





---



### Exercises {.unnumbered .unlisted}


Put some exercises here.



##  Inner Products

Inner products provide the foundation for geometric concepts such as **length**, **angle**, and **orthogonality**.



<div class="definition">
An **inner product** is a function that takes two vectors and gives a real number.  It must satisfy three key properties:

1. **Symmetry:**  
   The order of the vectors doesn’t matter.
   \[
   \langle x, y \rangle = \langle y, x \rangle
   \]  

2. **Linearity:**  
   It behaves nicely with addition and scaling:  
   \[
   \langle a x + b y, z \rangle = a \langle x, z \rangle + b \langle y, z \rangle
   \]

3. **Positive Definiteness:**  
   The inner product of a vector with itself is always positive unless the vector is zero:  
   \[
   \langle x, x \rangle > 0 \quad \text{if } x \ne 0
   \]
</div>


<div class="example">
The **dot product** in \(\mathbb{R}^n\) is:
\[
x^\top y = \sum_{i=1}^n x_i y_i.
\]
This is a specific example of an **inner product**.
</div>


The pair \((V, \langle \cdot, \cdot \rangle)\) is an **inner product space**.  If the dot product is used, it is a **Euclidean vector space**.



<div class="example">
For \(V = \mathbb{R}^2\),
\[
\langle x, y \rangle = x_1 y_1 - (x_1 y_2 + x_2 y_1) + 2x_2 y_2.
\]
This satisfies the properties of an inner product but differs from the dot product.
</div>


---

###  Symmetric, Positive Definite Matrices

Symmetric, positive definite (SPD) matrices are closely related to inner products and are central in machine learning and matrix decompositions.


Let \(V\) be an \(n\)-dimensional inner product space with basis \(B = (b_1, ..., b_n)\).  For \(x, y \in V\).  Since $x = \sum_{i=1}^n \psi_i b_i$, and $y = \sum_{j=1}^n \lambda_j b_j$ for some $\psi_i$ and $\lambda_j$ values, linearity of the inner product gives us
\[
\langle x, y \rangle = \sum_{i=1}^n \sum_{j=1}^n \psi_i \langle b_i, b_j \rangle \lambda_j = \hat{x}^\top A \hat{y},
\]
where \(A_{ij} = \langle b_i, b_j \rangle\).  Thus, the inner product is fully determined by a **symmetric matrix** \(A\), and if it is positive definite, then:
\[
x^\top A x > 0 \quad \forall x \ne 0.
\]



<div class="example">
1.  Show that if the $b_i$ terms are orthogonal, then $A$ is the identity matrix and the inner product is the dot product.
2.  Show an example where the $b_i$ are not orthogonal and what does that make $A$?
</div>



<div class="definition">
A symmetric matrix \(A \in \mathbb{R}^{n \times n}\) is **positive definite** if:
\[
x^\top A x > 0 \quad \forall x \ne 0
\]
If \(x^\top A x \ge 0\), \(A\) is **positive semidefinite**.
</div>




<div class="example">
\[
A_1 = \begin{bmatrix} 9 & 6 \\ 6 & 5 \end{bmatrix}, \quad
A_2 = \begin{bmatrix} 9 & 6 \\ 6 & 3 \end{bmatrix}
\]

- \(A_1\) is **positive definite**, since:
  \[
  x^\top A_1 x = (3x_1 + 2x_2)^2 + x_2^2 > 0
  \]
- \(A_2\) is **not positive definite**, since for \(x = [2, -3]^\top\),
  \[
  x^\top A_2 x < 0
  \]
</div>
  



<div class="theorem">
For a real, finite-dimensional vector space \(V\) with basis \(B\):
\[
\langle x, y \rangle = \hat{x}^\top A \hat{y}
\]
is an inner product *iff* \(A\) is symmetric and positive definite.
</div>



<div class="example">
Show

- The **kernel (null space)** of \(A\) is \(\{0\}\).
- All **diagonal entries** \(a_{ii} > 0\).
</div>

---



### Exercises {.unnumbered .unlisted}


Put some exercises here.




## Lengths and Distances

An inner product naturally induces a norm that measures the length of a vector:
\[
\|x\| = \sqrt{\langle x, x \rangle}.
\]
This means we can compute vector lengths directly from the inner product.  However, not all norms come from inner products (for example, the Manhattan norm does not).



<div class="theorem">
For any vectors \(x, y\) in an inner product space:
\[
|\langle x, y \rangle| \leq \|x\| \|y\|.
\]
</div>

This fundamental inequality relates the inner product to the lengths of vectors.

---

### Distance and Metrics



<div class="definition">
In an inner product space \((V, \langle \cdot, \cdot \rangle)\), the **distance** between two vectors is defined as:
\[
d(x, y) = \|x - y\| = \sqrt{\langle x - y, x - y \rangle}.
\]
</div>

<div class="example">
Put example here
</div>

If the inner product is the standard dot product, \(d(x, y)\) is the **Euclidean distance**.



<div class="example">
Put example here
</div>


<div class="definition">
A **metric** \(d: V \times V \to \mathbb{R}\) satisfies:

1. **Positive definiteness:** \(d(x, y) \ge 0\) and \(d(x, y) = 0 \iff x = y\)  
2. **Symmetry:** \(d(x, y) = d(y, x)\)  
3. **Triangle inequality:** \(d(x, z) \le d(x, y) + d(y, z)\)  
</div>


<div class="example">
Put example here
</div>

Note that while inner products measure similarity, distances measure difference —  similar vectors have a large inner product but a small distance.

---




### Exercises {.unnumbered .unlisted}


Put some exercises here.







## Angles and Orthogonality

Inner products also define **angles** between vectors.  

<div class="lemma">
For nonzero \(x, y\):
\[
\cos \omega = \frac{\langle x, y \rangle}{\|x\| \|y\|},
\]
where \(\omega \in [0, \pi]\) is the angle between \(x\) and \(y\).
</div>

When \(\omega = 0\), the vectors point in the same direction.  When \(\omega = \pi/2\), the vectors are **orthogonal** (perpendicular).


<div class="example">
Put example here
</div>


<div class="definition">
Two vectors \(x\) and \(y\) are **orthogonal** if:
\[
\langle x, y \rangle = 0.
\]
If both vectors also have unit length (\(\|x\| = \|y\| = 1\)), they are **orthonormal**.
</div>


<div class="example">
Put example here
</div>

Orthogonality generalizes the geometric idea of perpendicularity to arbitrary inner products.  
Vectors orthogonal under one inner product may not be orthogonal under another.


<div class="example">
Put example here
</div>


---

### Orthogonal Matrices

<div class="definition">
A square matrix \(A \in \mathbb{R}^{n \times n}\) is **orthogonal** if:
\[
A^\top A = AA^\top = I.
\]
This implies:
\[
A^{-1} = A^\top.
\]
</div>


<div class="example">
Put example here
</div>

Orthogonal matrices preserve both **lengths** and **angles**:
\[
\|Ax\| = \|x\|, \quad \text{and} \quad \langle Ax, Ay \rangle = \langle x, y \rangle.
\]
Hence, they represent **rotations and reflections** in space.


<div class="example">
Put example here
</div>



<div class="example">
Put example here
</div>




---




### Exercises {.unnumbered .unlisted}


Put some exercises here.





##  Orthonormal Basis


<div class="definition">
An **orthonormal basis (ONB)** in an \( n \)-dimensional vector space \( V \) consists of basis vectors \( \{b_1, ..., b_n\} \) that satisfy:
\[
\langle b_i, b_j \rangle = 0 \text{ for } i \neq j, \quad \langle b_i, b_i \rangle = 1.
\]
</div>


If only the first condition holds, the basis is *orthogonal*.  The **Gram-Schmidt process** constructs an orthonormal basis from any set of linearly independent vectors.  



<div class="example">
In \( \mathbb{R}^2 \),
\[
b_1 = \frac{1}{\sqrt{2}}\begin{pmatrix}1 \\ 1\end{pmatrix}, \quad 
b_2 = \frac{1}{\sqrt{2}}\begin{pmatrix}1 \\ -1\end{pmatrix}
\]
form an ONB because \( b_1^Tb_2 = 0 \) and \( \|b_1\| = \|b_2\| = 1 \).
</div>




---




### Exercises {.unnumbered .unlisted}


Put some exercises here.



## Orthogonal Complement



<div class="definition">
Given a subspace \( U \subseteq V \), the **orthogonal complement** \( U^\perp \) contains all vectors in \( V \) orthogonal to every vector in \( U \).  
</div>


It is easily shown that \( U \cap U^\perp = \{0\} \), and \( \dim(U) + \dim(U^\perp) = \dim(V) \).  Furthermore, any vector \( x \in V \) can be decomposed as:
\[
x = \sum_{m=1}^{M} \lambda_m b_m + \sum_{j=1}^{D-M} \psi_j b_j^\perp.
\]
In \( \mathbb{R}^3 \), a plane and its normal vector illustrate the relationship between a subspace and its orthogonal complement.




---




### Exercises {.unnumbered .unlisted}


Put some exercises here.



## Inner Product of Functions

The concept of inner product extends from finite-dimensional vectors to functions.



<div class="definition">
For functions $u$ and $v$, we define the inner product of $u$ and $v$ as:
\[
\langle u, v \rangle = \int_a^b u(x)v(x) \, dx.
\]
</div>


<div class="example">
Put example here
</div>


If \( \langle u, v \rangle = 0 \), the functions are orthogonal. 


<div class="example">
\( \sin(x) \) and \( \cos(x) \) are orthogonal on \([-\pi, \pi]\).  

The set \( \{1, \cos(x), \cos(2x), \ldots\} \) forms an orthogonal system on this interval, foundational to **Fourier series**.
</div>


---




### Exercises {.unnumbered .unlisted}


Put some exercises here.




##  Orthogonal Projections


<div class="definition">
A **projection** \( \pi: V \to U \) satisfies \( \pi^2 = \pi \).  
The corresponding **projection matrix** \( P_\pi \) also satisfies \( P_\pi^2 = P_\pi \).
</div>

Projections are linear transformations that map vectors in a vector space \( V \) onto a subspace \( U \subseteq V \).  Applying the projection twice does not change the result — once a vector has been projected, it already lies in the subspace.

Formally, for any \( x \in V \):
\[
\pi(\pi(x)) = \pi(x)
\quad \text{or equivalently} \quad
P_\pi^2 = P_\pi.
\]


<div class="example">
Put example here
</div>


<div class="example">
**Example:**  
Let \( B = \begin{bmatrix} 1 \\ 0 \end{bmatrix} \).  
Then:
\[
P = B(B^\top B)^{-1} B^\top = 
\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}.
\]
This matrix projects any vector in \( \mathbb{R}^2 \) onto the x-axis.
</div>


<div class="example">
Let
\[
U = \text{span}\!\left(\begin{bmatrix} 1 \\ 1 \end{bmatrix}\right),
\quad
B = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}.
\]
Then
\[
P = B B^\top = \frac{1}{2}\begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}.
\]
For \( x = \begin{bmatrix} 2 \\ 1 \end{bmatrix} \), we have
\[
P x = \frac{1}{2}\begin{bmatrix} 3 \\ 3 \end{bmatrix}
= \begin{bmatrix} 1.5 \\ 1.5 \end{bmatrix}.
\]
Hence, \( P x \) is the orthogonal projection of \( x \) onto the line \( y = x \).
</div>



### Projection onto a Line


<div class="definition">
For a line \( U = \text{span}(b) \), the projection of \( x \in \mathbb{R}^n \) is:
\[
\pi_U(x) = \frac{b^T x}{b^T b} b
\]
\[
P_\pi = \frac{b b^T}{b^T b}
\]
</div>


<div class="example">
Put example here
</div>


If \( \|b\| = 1 \), then \( \pi_U(x) = (b^T x) b \).


<div class="example">
Put example here
</div>


---

###  Projection onto General Subspaces

For subspace \( U = \text{span}(b_1, ..., b_m) \) with \( B = [b_1, ..., b_m] \):
\[
\pi_U(x) = B(B^T B)^{-1} B^T x
\]
\[
P_\pi = B(B^T B)^{-1} B^T.
\]
The **normal equation** \( B^T B \lambda = B^T x \) gives the coordinates \( \lambda \). If \( \{b_i\} \) form an **ONB**, the formula simplifies to:
\[
\pi_U(x) = B B^T x.
\]
This is computationally efficient since \( B^T B = I \).

<div class="example">
Put example here
</div>

---

### Gram-Schmidt Orthogonalization

The Gram-Schmidt algorithm is Used to construct an orthogonal (or orthonormal) basis from any basis \( \{b_1, ..., b_n\} \):
\[
u_1 = b_1, \quad
u_k = b_k - \pi_{\text{span}(u_1, ..., u_{k-1})}(b_k).
\]
After orthogonalization, normalize each \( u_k \) to form an ONB.



<div class="example">
In \( \mathbb{R}^2 \):
\[
b_1 = \begin{pmatrix}2 \\ 0\end{pmatrix}, \quad 
b_2 = \begin{pmatrix}1 \\ 1\end{pmatrix} \Rightarrow
u_1 = b_1, \quad
u_2 = \begin{pmatrix}0 \\ 1\end{pmatrix}
\]
</div>

<div class="example">
Put example here
</div>


---

### Projection onto Affine Subspaces

For an **affine subspace** \( L = x_0 + U \):
\[
\pi_L(x) = x_0 + \pi_U(x - x_0).
\]
The distance from \( x \) to \( L \) equals the distance from \( x - x_0 \) to \( U \):
\[
d(x, L) = \|x - \pi_L(x)\| = \|x - x_0 - \pi_U(x - x_0)\|.
\]
This concept is fundamental to **support vector machines** and **hyperplane separation** in later chapters.




<div class="example">
Put example here
</div>


---




### Exercises {.unnumbered .unlisted}


Put some exercises here.





## Rotations

Rotations are linear mappings that preserve both lengths and angles (as discussed in Section 3.4).  They are represented by orthogonal matrices, where the transformation rotates vectors through a given angle \( \theta \) around the origin.  Rotations are important in fields such as **computer graphics** and **robotics**, where objects or coordinate systems must be rotated precisely in space.

---

### Rotations in \( \mathbb{R}^2 \)

In \( \mathbb{R}^2 \), a rotation by angle \( \theta \) about the origin is defined as:
\[
R(\theta) =
\begin{pmatrix}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{pmatrix}.
\]
This matrix transforms the standard basis vectors as follows:
\[
\Phi(e_1) =
\begin{pmatrix}
\cos \theta \\ \sin \theta
\end{pmatrix},
\quad
\Phi(e_2) =
\begin{pmatrix}
-\sin \theta \\ \cos \theta
\end{pmatrix}.
\]
The transformation is **counterclockwise** for \( \theta > 0 \).  Rotations in \( \mathbb{R}^2 \) are **commutative**, meaning \( R(\phi)R(\theta) = R(\theta)R(\phi) \).  The rotation matrix represents a **basis change** in the plane.



<div class="example">
Put example here
</div>

---

### Rotations in \( \mathbb{R}^3 \)

In \( \mathbb{R}^3 \), a rotation occurs in a **two-dimensional plane** about a one-dimensional axis.  We define three fundamental rotation matrices corresponding to the standard coordinate axes.

**1. Rotation about the \( e_1 \)-axis:**
\[
R_1(\theta) =
\begin{pmatrix}
1 & 0 & 0 \\
0 & \cos \theta & -\sin \theta \\
0 & \sin \theta & \cos \theta
\end{pmatrix}.
\]

**2. Rotation about the \( e_2 \)-axis:**
\[
R_2(\theta) =
\begin{pmatrix}
\cos \theta & 0 & \sin \theta \\
0 & 1 & 0 \\
-\sin \theta & 0 & \cos \theta
\end{pmatrix}.
\]

**3. Rotation about the \( e_3 \)-axis:**
\[
R_3(\theta) =
\begin{pmatrix}
\cos \theta & -\sin \theta & 0 \\
\sin \theta & \cos \theta & 0 \\
0 & 0 & 1
\end{pmatrix}.
\]



<div class="example">
Put example here
</div>

A rotation is considered counterclockwise when viewed along the axis toward the origin. In \( \mathbb{R}^3 \), rotations do not commute; the order of rotations matters.

---

### Rotations in \( n \) Dimensions

In \( n \)-dimensional Euclidean space, rotations generalize to two-dimensional planes within \( \mathbb{R}^n \).  All other \( n-2 \) dimensions remain fixed.



<div class="definition">
A **Givens rotation** \( R_{ij}(\theta) \in \mathbb{R}^{n \times n} \) is defined as:
\[
R_{ij}(\theta) =
\begin{pmatrix}
I_{i-1} & & & & \\
& \cos \theta & & -\sin \theta & \\
& & I_{j-i-1} & & \\
& \sin \theta & & \cos \theta & \\
& & & & I_{n-j}
\end{pmatrix},
\]
for \( 1 \leq i < j \leq n \) and \( \theta \in \mathbb{R} \).  
</div>

This means:
\[
r_{ii} = \cos \theta, \quad
r_{ij} = -\sin \theta, \quad
r_{ji} = \sin \theta, \quad
r_{jj} = \cos \theta.
\]
In 2D, this reduces to the familiar matrix \( R(\theta) \).  Givens rotations are especially useful in numerical linear algebra for zeroing specific elements (e.g., QR decomposition).




<div class="example">
Put example here
</div>

---

### Properties of Rotations

Rotations have several key properties derived from their orthogonality:

1. **Distance Preservation:**  
   Rotations do not alter the distance between any two points.
   \[
   \|x - y\| = \|R_\theta(x) - R_\theta(y)\|
   \]

2. **Angle Preservation:**  
   The angle between \( x \) and \( y \) equals the angle between \( R_\theta x \) and \( R_\theta y \).

3. **Non-Commutativity (in 3D or higher):**  
   The composition of rotations depends on their order:  
   \[
   R_i(\phi)R_j(\theta) \neq R_j(\theta)R_i(\phi)
   \]
   Only in 2D do rotations commute and form an **Abelian group**.






<div class="example">
Put example here
</div>


---



### Exercises {.unnumbered .unlisted}


Put some exercises here.


