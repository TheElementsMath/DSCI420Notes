# Data, Models, and Learning


The second part of the book introduces the **four pillars of machine learning**:

1. **Regression** (Chapter 9)  
2. **Dimensionality Reduction** (Chapter 10)  
3. **Density Estimation** (Chapter 11)  
4. **Classification** (Chapter 12)

This part connects the mathematical foundations from earlier chapters to practical algorithms that solve real-world tasks. The goal is to show how concepts from linear algebra, probability, and optimization enable the design of learning systems — not to explore every advanced method, but to establish a practical and mathematical grounding.

---

## The Three Components of Machine Learning

Machine learning systems involve three fundamental components:

- **Data** — numerical or structured information used for training and testing.  
- **Models** — mathematical structures or functions that represent relationships in the data.  
- **Learning** — the process of adjusting model parameters to improve performance.

A “good” model is one that performs *well on unseen data*, judged by *performance metrics* such as accuracy or distance from a ground truth.

This chapter outlines common frameworks for training and evaluating models, including:

- **Empirical risk minimization (ERM)**  
- **Maximum likelihood estimation (MLE)**  
- **Probabilistic modeling**  
- **Graphical models**  
- **Model selection techniques**

---

### Data as Vectors

Machine learning assumes data can be represented numerically, typically in tabular form where:

- Rows correspond to **examples** (or data points).  
- Columns correspond to **features** (also called attributes or covariates).  

Each example is a **vector**:
\[
\mathbf{x}_n \in \mathbb{R}^D, \quad n = 1, \ldots, N
\]
where \( D \) is the number of features and \( N \) the number of samples.

It is also important to remember that:

- **Categorical data** must be encoded numerically (e.g., gender as 0/1).  
- **Scaling**: Data should typically be standardized so each feature has mean 0 and variance 1.  
- **Identifiers** (e.g., names) are often dropped for privacy and because they provide no predictive power.  

Therefore, the dataset is represented as:  
  \[
  \{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_N, y_N)\}, \quad \mathcal{X} \in \mathbb{R}^{N \times D}
  \]


<div class="example">
Predicting annual salary \( y \) from age \( x \) is a **supervised learning** problem, where each data point has an associated **label**.
</div>



---

### Models as Functions



<div class="definition">
A **model** defines a mapping from inputs to outputs — a **predictor**:
\[
f: \mathbb{R}^D \to \mathbb{R}
\]
</div>


For simplicity, many algorithms use **linear predictors**:
\[
f(x) = \mathbf{\mathbf{\theta}}^{\top}\mathbf{x} + \mathbf{\theta}_0
\]
where \( \mathbf{\mathbf{\theta}} \) and \( \mathbf{\theta}_0 \) are parameters to be learned.

Linear models balance mathematical simplicity and expressive power, forming the foundation for regression and classification methods.

---

### Models as Probability Distributions

Real-world data is noisy, so models must handle uncertainty.  



<div class="definition">
A **probabilistic model** represents predictions not as fixed outputs but as **distributions** over possible outcomes.
</div>


Instead of a single predictor \( f(\mathbf{x}) \), we consider a **distribution over functions** parameterized by finite-dimensional variables.  These models express:

- **Uncertainty in predictions** (e.g., confidence intervals)  
- **Uncertainty in parameters**   

Probability theory (Chapter 6) provides the foundation for these ideas.  Probabilistic modeling is used to describe machine learning systems in Section 8.4, and represent them compactly with graphical models in Section 8.5.

---


### Learning as Finding Parameters



<div class="definition">
**Learning** is the process of finding model parameters that perform well on unseen data.  
</div>


This involves three algorithmic phases:

1. **Prediction (Inference)** — Using a trained model on new data.  
2. **Training (Parameter Estimation)** — Adjusting parameters based on training data.  
3. **Model Selection (Hyperparameter Tuning)** — Choosing among competing models or configurations.

There are many different training strategies that may be utilized.  Some of those include:

- **Empirical Risk Minimization (ERM)** — Optimize parameters by minimizing prediction error on training data (Section 8.2).  
- **Maximum Likelihood Estimation (MLE)** — Choose parameters that make observed data most probable (Section 8.3).  
- **Bayesian Inference** — Model uncertainty over parameters using probability distributions (Section 8.4).  

Training typically involves numerical optimization (Chapter 7), often framed as minimizing a cost function.  Cross-validation (Section 8.2.4) is used to simulate performance on unseen data.

---

### Regularization and Model Complexity

To achieve good generalization, we balance:

- **Fit to training data**, and  
- **Model simplicity**.  

This is achieved through:

- **Regularization** — adding penalty terms to discourage complexity (Section 8.2.3)  
- **Bayesian priors** — probabilistic constraints on parameters (Section 8.3.2)  

This process reflects **abduction** — *inference to the best explanation* — rather than strict induction or deduction.

---

### Model Selection and Hyperparameters

Model selection chooses the best model or hyperparameters (e.g., number of components, type of distribution).  This can be done using:

- **Cross-validation**  
- **Nested cross-validation** for hyperparameter tuning.  




---



### Exercises {.unnumbered .unlisted}


<div class="exercise">
Suppose we have a model \[\hat{y}_i = \beta_0 + \beta_1 \mathbf{x}_i.\]  The least squares problem aims to minimize the loss function \[l(\mathbf{x}) =  (y_i - (\beta_0 + \beta_1\mathbf{x}_i))^2.\]  Suppose we add a regularization term $\lambda ||\mathbf{ \beta }||^2$ to the function we want to minimize.  Construct the function we want to now minimize and find the equations that minimize the sum of squared residuals. This is known as L2 regularization.

<div style="text-align: right;">
[Solution]( )
</div> 
</div>


<div class="exercise">
Suppose we have a model \[\hat{y}_i = \beta_0 + \beta_1 \mathbf{x}_i.\]  The least squares problem aims to minimize the loss function \[l(\mathbf{x}) =  (y_i - (\beta_0 + \beta_1\mathbf{x}_i))^2.\]  Suppose we add a regularization term $\lambda \sum_{i=0}^1 |\beta_i |$ to the function we want to minimize.  Construct the function we want to now minimize and find the equations that minimize the sum of squared residuals. This is known as L1 regularization.

<div style="text-align: right;">
[Solution]( )
</div> 
</div>
 




## Empirical Risk Minimization

Empirical Risk Minimization (ERM) is the foundation of learning in machine learning. It focuses on estimating model parameters from training data to minimize prediction error. The section introduces four main design components:

1. **Hypothesis Class:** What kind of functions can the predictor take?  
2. **Loss Function:** How do we measure how well the predictor fits the training data?  
3. **Regularization:** How do we prevent overfitting and ensure good generalization?  
4. **Model Search:** How do we assess and choose the best model?

---

###  Hypothesis Class of Functions

In supervised learning, we are given a dataset of input-output pairs:
\[
\mathcal{D} = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_N, y_N)\},
\]
where each \(\mathbf{x}_n \in \mathbb{R}^D\) represents an input feature vector, and \(y_n \in \mathbb{R}\) (or a discrete set) represents the target label.  The goal of learning is to find a **predictor function** \( f(\mathbf{x}, \mathbf{\mathbf{\theta}}) \) that maps input features to outputs, such that the predictions are as close as possible to the true values \( y_n \).  Here, \( \mathbf{\mathbf{\theta}} \) denotes the parameters (or weights) that define the specific function within a family of possible functions.




<div class="definition">
The **hypothesis class** is the set of all functions that the learning algorithm can choose from, based on the model type and its parameters.

Formally:
\[
\mathcal{H} = \{ f(\mathbf{x}, \mathbf{\mathbf{\theta}}) : \mathbf{\mathbf{\theta}} \in \Theta \},
\]
where:

- \( \mathcal{H} \) is the hypothesis class (the space of possible predictors),  
- \( f(\mathbf{x}, \mathbf{\mathbf{\theta}}) \) is the model function,  
- \( \Theta \) is the parameter space that determines which functions are possible.  
</div>

The hypothesis class defines the **capacity** of a model — how flexible or expressive it can be in fitting data.



<div class="example">
Linear (Affine) Models:

A common and simple choice for \( f(\mathbf{x}, \mathbf{\mathbf{\theta}}) \) is a **linear model**:
\[
f(\mathbf{x}, \mathbf{\mathbf{\theta}}) = \mathbf{\mathbf{\theta}}^\top \mathbf{x} + b,
\]
where: 

- \( \mathbf{\mathbf{\theta}} \in \mathbb{R}^D \) is a weight vector,  
- \( b \in \mathbb{R} \) is a bias term.  

The hypothesis class for linear regression is:
\[
\mathcal{H}_{\text{linear}} = \{ f(\mathbf{x}) = \mathbf{\theta}^\top \mathbf{x} + b : \mathbf{\theta} \in \mathbb{R}^D, b \in \mathbb{R} \}.
\]
This means all **hyperplanes** in \( \mathbb{R}^D \) are possible predictors.
</div>

Linear models are interpretable and efficient but limited in representing nonlinear relationships.  To capture more complex patterns, we can expand the hypothesis class by transforming the inputs or using nonlinear functions.

<div class="example">
Nonlinear Models:

**Polynomial models:**
\[
f(\mathbf{x}, \mathbf{\theta}) = \mathbf{\theta}_0 + \mathbf{\theta}_1 \mathbf{x} + \mathbf{\theta}_2 \mathbf{x}^2 + \cdots + \mathbf{\theta}_k \mathbf{x}^k
\]

**Neural networks:**
\[
f(\mathbf{x}, \mathbf{\theta}) = \sigma(W_2 \, \sigma(W_1 \mathbf{x} + b_1) + b_2)
\]
where \( \sigma(\cdot) \) is a nonlinear activation function.
</div>

These types of models can approximate complex relationships but risk **overfitting** if the hypothesis class is too flexible relative to the data size.

- A **small hypothesis class** (e.g., linear functions) may lead to **underfitting**, failing to capture important patterns.  
- A **large hypothesis class** (e.g., deep neural networks) can represent almost any function but may **overfit** training data.  

The key is to choose a hypothesis class that is **rich enough** to capture underlying relationships but **constrained enough** to generalize well.


<div class="note">
Geometrically, each function \( f(\mathbf{x}, \mathbf{\theta}) \) in \( \mathcal{H} \) corresponds to a **surface** (in regression) or **decision boundary** (in classification) in feature space. Learning means selecting one of these functions (via parameter estimation) that best fits the observed data under a chosen loss function.
</div>



---

### Loss Function for Training



<div class="definition">
A **loss function** \(\ell(y_n, \hat{y}_n)\) measures the discrepancy between the true label \(y_n\) and prediction \(\hat{y}_n = f(\mathbf{x}_n, \mathbf{\theta})\).  
</div>



<div class="definition">
The **empirical risk** is the average loss across all training examples:
\[
\mathbf{R}_{\text{emp}}(f, \mathbf{\mathbf{X}}, \mathbf{y}) = \frac{1}{N} \sum_{n=1}^N \ell(y_n, f(\mathbf{x}_n, \mathbf{\theta}))
\]
</div>


ERM (Empirical Risk Minimization) seeks to minimize this risk:
\[
\min_{\mathbf{\theta}} \mathbf{R}_{\text{emp}}(f, \mathbf{\mathbf{X}}, \mathbf{y}).
\]



<div class="example">
In least squares regression, the average empirical risk is given by
\[
\frac{1}{N} \sum_{n=1}^N (y_n - \mathbf{\theta}^\top \mathbf{x}_n)^2.
\]
ERM seeks to find
\[
\min_{\mathbf{\theta}} \frac{1}{N} \sum_{n=1}^N (y_n - \mathbf{\theta}^\top \mathbf{x}_n)^2
\]
or equivalently, in matrix form:
\[
\min_{\mathbf{\theta}} \frac{1}{N} \|\mathbf{y} - \mathbf{\mathbf{X}}\mathbf{\theta}\|^2
\]
</div>



<div class="definition">
The **true (expected) risk** measures performance on unseen data:
\[
\mathbf{R}_{\text{true}}(f) = \mathbb{E}_{\mathbf{x},y}[\ell(y, f(\mathbf{x}))]
\]
</div>

ERM approximates true risk with finite training data.

---

### Regularization to Reduce Overfitting


Regularization reduces overfitting and improves generalization.

Once a hypothesis class \( \mathcal{H} \) is chosen, the next challenge is ensuring that the model not only fits the **training data** well but also **generalizes** effectively to unseen data.  This problem is known as the **bias-variance trade-off**, and one of the most powerful tools to manage it is **regularization**.



<div class="definition">
A model **overfits** when it learns patterns that exist only in the training data — including noise or random fluctuations — rather than the underlying structure of the problem.  
</div>

For example:

- A high-degree polynomial may perfectly interpolate training points but produce wild oscillations on new inputs.  
- A deep neural network with too many parameters may memorize the data rather than learn general features.  

Overfitting leads to **low training error** but **high test error**.




<div class="definition">
**Regularization** introduces a **penalty** for model complexity directly into the optimization objective.  It biases the learning algorithm toward simpler models that are less likely to overfit.
</div>

The general form of a **regularized optimization problem** is:
\[
\min_{\mathbf{\theta}} \; \mathcal{L}_{\text{train}}(\mathbf{\theta}) + \lambda \, \Omega(\mathbf{\theta})
\]
where:

- \( \mathcal{L}_{\text{train}}(\mathbf{\theta}) \): training loss (e.g., mean squared error, cross-entropy)  
- \( \Omega(\mathbf{\theta}) \): regularization term that penalizes large or complex parameter values  
- \( \lambda \ge 0 \): regularization coefficient controlling the strength of the penalty  

A large \( \lambda \) enforces more simplicity (strong regularization), while a small \( \lambda \) allows more flexibility.

---

### Cross-Validation to Assess Generalization

Even after applying regularization, a key question remains: **How well does our model generalize to unseen data?**

In machine learning, generalization refers to a model’s ability to perform well not just on the training data (used to learn parameters), but also on new, unseen data drawn from the same underlying distribution.  Because we only have access to a finite dataset, we must estimate generalization performance empirically.  Cross-validation provides a systematic way to do this.

---

#### The Challenge of Estimating Generalization

If we train and test on the same dataset, the model will almost always appear to perform well — often unrealistically so.  This is because the model has already "seen" the data during training, and may have overfit to it.

To better approximate the **true risk** \[ R_{\text{true}}(f) = \mathbb{E}_{\mathbf{x}, y}[\ell(y, f(\mathbf{x}))],\]  we need a separate dataset on which the model has not been trained.  A simple approach is to split the data into:

- a **training set** for fitting parameters, and  
- a **test (or validation) set** for evaluating generalization.  

However, when data is limited, such a simple split can be unreliable — the test performance may depend heavily on which points happened to be held out.  This motivates **cross-validation**.

---

#### The Idea of Cross-Validation


**Cross-validation (CV)** is a statistical resampling method that allows us to:

1. Use **all** the data for both training and validation (just not at the same time), and  
2. Obtain a more **robust estimate** of model performance by averaging across multiple train/test splits.

The most widely used form is **K-fold cross-validation**.

---

#### K-Fold Cross-Validation

In **K-fold cross-validation**, the dataset is randomly partitioned into \( K \) equally (or nearly equally) sized subsets, or *folds*.  Then, the procedure is:

1. For each \( k = 1, \ldots, K \):  
   - Hold out fold \( k \) as the **validation set** \( V^{(k)} \).  
   - Use the remaining \( K - 1 \) folds as the **training set** \( R^{(k)} \).  
   - Train the model on \( R^{(k)} \), producing a predictor \( f^{(k)} \).  
   - Compute the empirical risk (validation error) on \( V^{(k)} \):  
     \[
     R_{\text{emp}}(f^{(k)}, V^{(k)}) = \frac{1}{|V^{(k)}|} \sum_{(\mathbf{x}_i, y_i) \in V^{(k)}} \ell(y_i, f^{(k)}(\mathbf{x}_i))
     \]  
2. Average the validation errors across all folds:
   \[
   E_V[R(f, V)] \approx \frac{1}{K} \sum_{k=1}^{K} R_{\text{emp}}(f^{(k)}, V^{(k)})
   \]
   
The resulting average gives an estimate of the **expected generalization error**.

---

#### Choosing \( K \)

The choice of \( K \) controls the bias–variance tradeoff of the cross-validation estimate:

| K | Training Proportion | Bias | Variance | Computational Cost |
|---|----------------------|------|-----------|--------------------|
| 2 | 50% | High | Low | Low |
| 5 | 80% | Moderate | Moderate | Moderate |
| 10 | 90% | Low | Moderate | Higher |
| N (Leave-One-Out CV) | ~100% | Very Low | High | Very High |


Generally speaking, a smaller \( K \) leads to less computation, but a higher bias (since less data per training run).  A larger \( K \) has better performance estimates, but more computationally expensive.  In practice, \( K = 5 \) or \( K = 10 \) are common choices.








---



### Exercises {.unnumbered .unlisted}


<div class="exercise">
Verify that \[\mathcal{L}(\mathbf{\theta})=-\log(p(\mathcal{Y}|\mathcal{X},\mathbf{\theta}) = -\sum_{n=1}^N \log(p(y_n|\mathbf{x}_n,\mathbf{\theta}).\]    Make sure to justify all of your steps.

<div style="text-align: right;">
[Solution]( )
</div> 
</div>




<div class="exercise">
Verify that \[\mathcal{L}(\mathbf{\theta})=\dfrac{1}{2\sigma^2}\sum_{n=1}^N (y_n - \mathbf{x}_n^T\mathbf{\theta})^2 - \sum_{n=1}^N \log \dfrac{1}{\sqrt{2\pi\sigma^2}}.\] Make sure to justify all of your steps.

<div style="text-align: right;">
[Solution]( )
</div> 
</div>



<div class="exercise">
If we have prior knowledge about a distribution of the parameters, $\theta$, we can multiply an additional term to our likelihood, $p(\mathbf{x} | \mathbf{\theta})$.  We know \[p(\mathbf{\theta}|\mathbf{x})=\dfrac{p(\mathbf{x} | \mathbf{\theta})p(\mathbf{\theta})}{p(\mathbf{x})}.\]  However, we can ignore $p(\mathbf{x})$ and we write \[p(\mathbf{\theta}|\mathbf{x}) \propto p(\mathbf{x} | \mathbf{\theta})p(\mathbf{\theta}).\]  Why?

<div style="text-align: right;">
[Solution]( )
</div> 
</div>





## Parameter Estimation

In this section, we introduce how probability distributions can model uncertainty in data and parameters. This extends the concepts from empirical risk minimization to a probabilistic framework, allowing us to reason about both the data-generating process and the model parameters.

### 8.3.1 Maximum Likelihood Estimation (MLE)


<div class="definition">
**Maximum Likelihood Estimation (MLE)** is a statistical method used to estimate the parameters of a model by finding the values that make the observed data most probable.  
</div>

Suppose we have data \( \mathbf{x} = (\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N) \) drawn from a probability distribution \( p(\mathbf{x} | \mathbf{\theta}) \) that depends on some unknown parameter(s) \( \mathbf{\theta} \).  
The **likelihood function** represents how likely the observed data is for different parameter values:
\[
L(\mathbf{\theta}) = p(\mathbf{x} | \mathbf{\theta}) = \prod_{n=1}^{N} p(\mathbf{x}_n | \mathbf{\theta})
\]
The goal of MLE is to find the parameter value \( \hat{\mathbf{\theta}} \) that maximizes this likelihood:
\[
\hat{\mathbf{\theta}}_{\text{MLE}} = \arg\max_{\mathbf{\theta}} L(\mathbf{\theta})
\]
Because products of probabilities can become very small, it is common to work with the **log-likelihood**, which turns products into sums and simplifies computation:
\[
\ell(\mathbf{\theta}) = \log L(\mathbf{\theta}) = \sum_{n=1}^{N} \log p(\mathbf{x}_n | \mathbf{\theta})
\]
Maximizing the log-likelihood is equivalent to maximizing the likelihood itself, since the logarithm is a monotonic transformation.  


<div class="note">
In practice, optimization algorithms often minimize rather than maximize functions. Therefore, we typically minimize the **negative log-likelihood (NLL)**:
\[
\mathcal{L}(\mathbf{\theta}) = -\ell(\mathbf{\theta}) = - \sum_{n=1}^{N} \log p(\mathbf{x}_n | \mathbf{\theta})
\]
</div>

When we maximize the log-likelihood (or minimize the negative log-likelihood), \( \mathbf{\theta} \) varies while the data \( \mathbf{x} \) is fixed. Minimizing \( L(\mathbf{\theta}) \) corresponds to maximizing the likelihood — that is, finding the parameters most likely to have produced the observed data.




<div class="example"> 
If \( p(y_n | \mathbf{x}_n, \mathbf{\theta}) \) is Gaussian, MLE corresponds to minimizing the sum of squared residuals — the classic linear regression case.  
</div>

Although MLE can yield closed-form solutions in simple cases, it may suffer from **overfitting** and may require numerical optimization when closed forms are unavailable.

---

### Maximum A Posteriori (MAP) Estimation

When **prior knowledge** about parameters is available, we can combine it with the likelihood using **Bayes’ theorem**:
\[
p(\mathbf{\theta} | \mathbf{x}) = \frac{p(\mathbf{x} | \mathbf{\theta}) p(\mathbf{\theta})}{p(\mathbf{x})}.
\]
Since \( p(\mathbf{x}) \) does not depend on \( \mathbf{\theta} \), maximizing the posterior is equivalent to maximizing \( p(\mathbf{x} | \mathbf{\theta})p(\mathbf{\theta}) \).  This leads to **Maximum A Posteriori (MAP) Estimation**, where we minimize the **negative log-posterior**:
\[
L_{\text{MAP}}(\mathbf{\theta}) = -\log p(\mathbf{x} | \mathbf{\theta}) - \log p(\mathbf{\theta})
\]

MAP estimation adds a regularizing effect, since the prior \( p(\mathbf{\theta}) \) discourages implausible parameter values.



<div class="example">
With a Gaussian likelihood and a Gaussian prior on parameters (e.g., zero-mean prior), the MAP estimate resembles ridge regression — balancing data fit and parameter simplicity.
</div>

---

###  Model Fitting

**Model fitting** involves optimizing parameters \( \mathbf{\theta} \) to minimize a loss (e.g., the negative log-likelihood).  The model class \( \mathcal{M}_\mathbf{\theta} \) defines the family of possible predictors, and fitting finds the instance within this class that best approximates the true data-generating process \( \mathcal{M}^* \).

There are three main fitting outcomes:

1. **Overfitting:**  
   - The model class is too flexible.  
   - Captures noise as if it were signal.  
   - Low training error but high test error.

2. **Underfitting:**  
   - The model class is too simple.  
   - Fails to capture the true data structure.  
   - High error on both training and test data.

3. **Good Fit:**  
   - The model class is appropriately complex.  
   - Balances bias and variance.  
   - Exhibits good generalization.

To mitigate overfitting, we can apply:

- **Regularization** (Section 8.2.3), or  
- **Priors** (Section 8.3.2).

In practice, large model classes such as deep neural networks rely on these techniques to control generalization and improve performance.

---



### Exercises {.unnumbered .unlisted}


<div class="exercise">
Suppose that you would like to estimate the portion of voters in your town that plan to vote for Party A in an upcoming election. To do so, you take a random sample of size $n$ from the likely voters in the town. Since you have a limited amount of time and resources, your sample is relatively small. Specifically, suppose that $n=20$. After doing your sampling, you find out that 6 people in your sample say they will vote for Party A.  In the previous election, 40\% of voters voted for party A.  Provide both a frequentist and Bayesian approach to dealing with this problem.

<div style="text-align: right;">
[Solution]( )
</div> 
</div>





##  Probabilistic Modeling and Inference


In machine learning, we use probabilistic models to represent uncertainty in data and parameters.  These models describe how observed data are generated from underlying parameters, allowing us to reason about prediction, inference, and decision-making under uncertainty.

---

### Probabilistic Models



<div class="definition">
A **probabilistic model** is defined by the **joint distribution** of all random variables:
\[
p(\mathbf{x}, \mathbf{\theta})
\]
where  

- \(\mathbf{x}\) represents the **observed data**, and  
- \(\mathbf{\theta}\) represents the **model parameters**.
</div>

This joint distribution encapsulates:

- The **likelihood** \(p(\mathbf{x} | \mathbf{\theta})\)  
- The **prior** \(p(\mathbf{\theta})\)  
- The **posterior** \(p(\mathbf{\theta} | \mathbf{x})\)  
- The **marginal likelihood** \(p(\mathbf{x}) = \int p(\mathbf{x} | \mathbf{\theta})p(\mathbf{\theta}) d\mathbf{\theta}\)

The probabilistic framework provides a consistent way to model, infer, and predict using probability theory.

---

###  Bayesian Inference

**Bayesian inference** is concerned with computing the posterior distribution of parameters given data:
\[
p(\mathbf{\theta} | \mathcal{X}) = \frac{p(\mathcal{X} | \mathbf{\theta}) p(\mathbf{\theta})}{p(\mathcal{X})}
\]
where
\[
p(\mathcal{X}) = \int p(\mathcal{X} | \mathbf{\theta}) p(\mathbf{\theta}) d\mathbf{\theta}
\]
acts as a normalization constant (marginal likelihood).  Bayesian inference **inverts** the relationship between parameters and data.  Instead of finding a single “best” parameter estimate (as in MLE or MAP), it computes a **distribution over parameters**, capturing full uncertainty.

Predictions are then made by **marginalizing over parameters**:
\[
p(\mathbf{x}) = \int p(\mathbf{x} | \mathbf{\theta})p(\mathbf{\theta})d\mathbf{\theta} = \mathbb{E}_{\mathbf{\theta}}[p(\mathbf{x} | \mathbf{\theta})]
\]

#### Comparison with Parameter Estimation:
| Approach | Output | Main Computation | Example Methods |
|-----------|---------|------------------|----------------|
| **MLE / MAP** | Point estimate \( \mathbf{\theta}^* \) | Optimization | Gradient Descent, Least Squares |
| **Bayesian Inference** | Distribution \( p(\mathbf{\theta} | \mathcal{X}) \) | Integration | MCMC, Laplace, Variational Inference |

Bayesian methods allow:

- Incorporation of prior knowledge  
- Uncertainty propagation in predictions  
- Better handling of small datasets or noisy data  

However, Bayesian inference often requires approximations since integrals for our posterior distribution and our parameters are rarely analytic.  Common approximation methods include:

- **Stochastic methods:** Markov Chain Monte Carlo (MCMC)  
- **Deterministic methods:** Laplace approximation, Variational Inference, Expectation Propagation  

---

### Latent-Variable Models



<div class="definition">
A **latent-variable model** introduces hidden variables \( z \) that help explain the data.  
</div>
These variables are not directly observed but simplify or enrich the model’s structure. The generative process is:
\[
p(\mathbf{x} | \mathbf{z}, \mathbf{\theta})
\]
with a prior on latent variables:
\[
p(\mathbf{z}).
\]
To obtain the **likelihood** of observed data, we integrate out the latent variables:
\[
p(\mathbf{x} | \mathbf{\theta}) = \int p(\mathbf{x} | \mathbf{z}, \mathbf{\theta})p(\mathbf{z})d\mathbf{z}.
\]
Once the likelihood is known, we can:

- Perform **maximum likelihood** or **MAP estimation** for parameters  
- Conduct **Bayesian inference** to obtain posterior distributions  

The posterior over parameters is:
\[
p(\mathbf{\theta} | \mathcal{X}) = \frac{p(\mathcal{X} | \mathbf{\theta})p(\mathbf{\theta})}{p(\mathcal{X})}
\]
and the posterior over latent variables is:
\[
p(\mathbf{z} | \mathcal{X}, \mathbf{\theta}) = \frac{p(\mathcal{X} | \mathbf{z}, \mathbf{\theta})p(\mathbf{z})}{p(\mathcal{X} | \mathbf{\theta})}.
\]

---

### Examples of Latent-Variable Models

- **Principal Component Analysis (PCA)** – dimensionality reduction  
- **Gaussian Mixture Models (GMMs)** – density estimation  
- **Hidden Markov Models (HMMs)** – time-series analysis  
- **Dynamical Systems** – modeling temporal dependencies  
- **Meta-Learning / Task Generalization** – learning across tasks

Although introducing latent variables can make models more interpretable and flexible, inference becomes more challenging because marginalization is often intractable and requires approximation methods.


---



### Exercises {.unnumbered .unlisted}


Put some exercises here.






##  Directed Graphical Models

Directed graphical models, also known as **Bayesian networks**, provide a graphical language for specifying probabilistic models. They offer a compact and intuitive way to visualize dependencies between random variables and to represent how a joint distribution can be decomposed into simpler conditional distributions.

The key idea is that **nodes** represent random variables, and **edges (arrows)** represent probabilistic or conditional relationships.  The joint distribution is expressed as a product of conditional probabilities.  Graphical models help identify **independence and conditional independence** relationships among variables.

Some of the benefits of graphical models are that they:

- Provide a simple way to **visualize model structure**.  
- Facilitate the **design and understanding** of statistical models.  
- Make **independence properties** easy to identify.  
- Enable **simplified inference and learning** computations through graphical manipulations.  

---

###  Graph Semantics

A **directed graphical model** (or Bayesian network) represents conditional dependencies among random variables.



<div class="example">
A joint distribution:
\[
p(a, b, c) = p(c | a, b) \, p(b | a) \, p(a)
\]
implies the following:

- \( c \) depends on \( a \) and \( b \)  
- \( b \) depends on \( a \)  
- \( a \) is independent of \( b \) and \( c \)  

This factorization can be represented as a directed graph with arrows from \( a \to b \) and \( a, b \to c \).
</div>


#### General Rule for Constructing Graphs

1. Create a **node** for each random variable.  
2. For each conditional probability, draw arrows from the **conditioning variables** to the **conditioned variable**.  

#### General Form of the Joint Distribution

For random variables \( \mathbf{x}_1, \ldots, \mathbf{x}_K \), the joint distribution factorizes as:
\[
p(\mathbf{x}) = p(\mathbf{x}_1, \ldots, \mathbf{x}_K) = \prod_{k=1}^K p(\mathbf{x}_k | \text{Pa}_k)
\]
where \( \text{Pa}_k \) denotes the **parent nodes** of \( \mathbf{x}_k \) (nodes with arrows pointing to \( \mathbf{x}_k \)).



<div class="example">
Coin Flip Experiment

For a Bernoulli trial with parameter \( \mu \):
\[
p(\mathbf{x} | \mu) = \text{Ber}(\mu).
\]
Repeating the experiment \( N \) times:
\[
p(\mathbf{x}_1, \ldots, \mathbf{x}_N | \mu) = \prod_{n=1}^N p(\mathbf{x}_n | \mu).
\]

This can be represented graphically:

- Each outcome \( \mathbf{x}_n \) depends on the same latent variable \( \mu \).  
- **Plate notation** is used to represent the repetition \( N \) times.

We can also assign a **hyperprior** to \( \mu \), for instance:
\[
\mu \sim \text{Beta}(\alpha, \beta)
\]
where \( \alpha \) and \( \beta \) may be treated as deterministic parameters.
</div>


---

###  Conditional Independence and d-Separation

Directed graphical models can reveal **conditional independence relationships** directly from the graph using the concept of **d-separation** (Pearl, 1988).



<div class="definition">
Given disjoint node sets \( A, B, C \) in a directed acyclic graph, \( A \) is **d-separated** from \( B \) given \( C \), written as:
  \[
  A \perp\!\!\!\perp B \,|\, C
  \]
if all paths between nodes in \( A \) and \( B \) are **blocked**.

A path is **blocked** if:

1. The arrows meet **head-to-tail** or **tail-to-tail** at a node in \( C \) (a path of the form $a \rightarrow c \rightarrow b$ or $a \leftarrow c \rightarrow b$), or  
2. The arrows meet **head-to-head** at a node not in \( C \), and none of its descendants are in \( C \) (a path of the form $a \rightarrow c \leftarrow b$).
</div>

If all paths are blocked, \( A \) and \( B \) are conditionally independent given \( C \).


<div class="example">
In the graphical model:



<p align="center">
<img src="Example89.png" alt="A E-separation example" width="200">
</p>


We can infer:

- \( b \perp d \,|\, a, c \)  
- \( a \perp c \,|\, b \)  
- \( b \not\!\perp d \,|\, c \) since $d$ is a descendant of $c$  
- \( a \not\!\perp c \,|\, b, e \) since $d$ is a descendant of $c$  
</div>





<div class="example">
d-Separation in a Bayesian Network

Consider the following directed acyclic graph (DAG):

\[
A \longrightarrow C \longrightarrow D \longleftarrow B
\]
We will analyze conditional independence relationships using **d-separation**.



We examine whether variables \(A\) and \(B\) are independent under different conditioning sets.  There is exactly one undirected path between \(A\) and \(B\):
\[
A \rightarrow C \rightarrow D \leftarrow B.
\]
This path contains a **collider** at node \(D\).


If we condition on nothing:

- The path contains a collider \(D\)  
- Colliders block paths unless conditioned on (or their descendants are)

The path is blocked
\[
A \;\perp\!\!\!\perp\; B
\]



Now condition on \(C\):

- \(C\) is a chain node  
- Conditioning on a chain node blocks the path  
- The collider at \(D\) remains unconditioned  

The path is still **blocked**
\[
A \;\perp\!\!\!\perp\; B \mid C
\]


Now condition on \(D\):

- Conditioning on a collider opens the path  

The path is now **unblocked**
\[
A \;\not\!\perp\!\!\!\perp\; B \mid D
\]
Knowing \(A\) gives information about \(B\) once \(D\) is known.


Suppose \(E\) is a descendant of \(D\):
\[
A \rightarrow C \rightarrow D \leftarrow B, \quad D \rightarrow E
\]
Condition on \(E\):

- Conditioning on a descendant of a collider also opens the path  

The path is unblocked
\[
A \;\not\!\perp\!\!\!\perp\; B \mid E
\]

Summary of Results

| Conditioning Set | Are \(A\) and \(B\) d-separated? |
|------------------|----------------------------------|
| None             | Yes |
| \(C\)            | Yes |
| \(D\)            | No |
| \(E\)            | No |


</div>


---



### Exercises {.unnumbered .unlisted}



<div class="exercise">

Chain (No Conditioning)

\[
X \rightarrow Y \rightarrow Z
\]

Question: Are \(X\) and \(Z\) independent?

- The path is a **chain**
- No conditioning blocks the path

\[
X \;\not\!\perp\!\!\!\perp\; Z
\]
<div style="text-align: right;">
[Solution]( )
</div> 
</div>




<div class="exercise">
Chain (Condition on the Middle)

\[
X \rightarrow Y \rightarrow Z
\]

Condition on \(Y\):

- Conditioning on a **chain node blocks the path**

\[
X \;\perp\!\!\!\perp\; Z \mid Y
\]
<div style="text-align: right;">
[Solution]( )
</div> 
</div>




<div class="exercise">
Fork (Common Cause)

\[
X \leftarrow Y \rightarrow Z
\]

No conditioning:

- \(Y\) is a **common cause**
- Path is open

\[
X \;\not\!\perp\!\!\!\perp\; Z
\]
<div style="text-align: right;">
[Solution]( )
</div> 
</div>




<div class="exercise">
Fork (Condition on the Common Cause)

\[
X \leftarrow Y \rightarrow Z
\]

Condition on \(Y\):

- Conditioning on a **fork node blocks the path**

\[
X \;\perp\!\!\!\perp\; Z \mid Y
\]
<div style="text-align: right;">
[Solution]( )
</div> 
</div>




<div class="exercise">
Collider (No Conditioning)

\[
X \rightarrow Y \leftarrow Z
\]

No conditioning:

- \(Y\) is a **collider**
- Colliders block paths by default

\[
X \;\perp\!\!\!\perp\; Z
\]
<div style="text-align: right;">
[Solution]( )
</div> 
</div>



<div class="exercise">
Collider (Condition on the Collider)

\[
X \rightarrow Y \leftarrow Z
\]

Condition on \(Y\):

- Conditioning on a **collider opens the path**

\[
X \;\not\!\perp\!\!\!\perp\; Z \mid Y
\]
<div style="text-align: right;">
[Solution]( )
</div> 
</div>





## Model Selection

Model selection in machine learning involves choosing among different models or configurations that can best generalize to unseen data. The goal is to balance model complexity and data fit — avoiding both underfitting and overfitting.

Complex models (e.g., higher-degree polynomials) are more expressive and can describe a wider variety of datasets. However, greater flexibility often leads to overfitting on the training set, which reduces performance on unseen data.  Therefore, model selection aims to identify the simplest model that explains the data sufficiently well — a concept known as **Occam’s Razor**.

---

### Nested Cross-Validation



<div class="definition">
**Cross-validation** estimates a model’s generalization error by repeatedly splitting data into training and validation sets.  
</div>

<div class="definition">
**Nested cross-validation** extends the idea of cross validation by using two levels of cross-validation:

- **Inner loop**: Chooses the best model or hyperparameters based on validation performance.   
- **Outer loop**: Estimates the generalization performance of the chosen model on unseen test data.
</div>



<div class="lemma">
The expected validation error estimate is given by:
\[
E_V[\mathbf{R}(\mathcal{V} | M)] \approx \frac{1}{K} \sum_{k=1}^{K} \mathbf{R}(\mathcal{V}^{(k)} | M),
\]
where \( \mathbf{R}(\mathcal{V} | M) \) is the empirical risk (e.g., RMSE) of model \( M \) on validation set \( \mathcal{V} \).  
</div>

This process yields both a **mean generalization estimate** and a **standard error** for uncertainty quantification.

---

### Bayesian Model Selection

Bayesian model selection provides a probabilistic framework for comparing models. It incorporates both data fit and model complexity via Bayes’ theorem:
\[
p(M_k | \mathcal{D}) \propto p(M_k) \, p(\mathcal{D} | M_k).
\]
Here:

- \( p(M_k) \): prior probability of model \( M_k \)  
- \( p(\mathcal{D} | M_k) \): **model evidence** or **marginal likelihood**
\[
p(\mathcal{D} | M_k) = \int p(\mathcal{D} | \mathbf{\theta}_k) \, p(\mathbf{\theta}_k | M_k) \, d\mathbf{\theta}_k
\]

This integral marginalizes over model parameters \( \mathbf{\theta}_k \), automatically penalizing overly complex models.



<div class="definition">
**Model evidence** quantifies how well a model predicts observed data after accounting for parameter uncertainty.  
</div>

Thus, the **MAP estimate** of the best model is:
\[
M^* = \arg\max_{M_k} p(M_k | \mathcal{D})
\]


---

### Bayes Factors for Model Comparison

To compare two models \( M_1 \) and \( M_2 \), we consider their **posterior odds**:
\[
\frac{p(M_1 | \mathcal{D})}{p(M_2 | \mathcal{D})} =
\underbrace{\frac{p(M_1)}{p(M_2)}}_{\text{prior odds}} \times
\underbrace{\frac{p(\mathcal{D} | M_1)}{p(\mathcal{D} | M_2)}}_{\text{Bayes factor}}
\]

The **Bayes factor** \( \frac{p(\mathcal{D} | M_1)}{p(\mathcal{D} | M_2)} \) measures relative model support from the data. With **uniform priors**, model selection depends only on the Bayes factor.  If the Bayes factor > 1, \( M_1 \) is preferred; otherwise, \( M_2 \) is chosen.


<div class="note">
The **Jeffreys-Lindley paradox** is a phenomenon in Bayesian statistics that highlights a surprising difference between Bayesian model comparison and classical (frequentist) hypothesis testing.

Suppose we want to test:

\[
H_0: \mathbf{\theta} = \mathbf{\theta}_0 \quad \text{vs.} \quad H_1: \mathbf{\theta} \neq \mathbf{\theta}_0
\]

- In **frequentist hypothesis testing**, we might reject \(H_0\) if the p-value is small.  
- In **Bayesian model selection**, we compute the **Bayes factor**:
\[
\text{BF} = \frac{p(\text{data} | H_0)}{p(\text{data} | H_1)}
\]

The paradox arrises when, even if the data strongly rejects \(H_0\) according to a classical test (small p-value), the Bayes factor may favor the null hypothesis \(H_0\).  This happens especially when the prior for the alternative hypothesis \(H_1\) is **diffuse** (spread over a wide range of possible parameter values).  As a result, \(H_0\) can appear more probable in the Bayesian sense, even if the observed data seems extreme.

The result is that Bayesian and frequentist conclusions can disagree.  Prior choices in Bayesian analysis have a strong influence on model comparison.  The paradox emphasizes the importance of carefully selecting priors for alternative hypotheses.
</div>
---

### Computing the Marginal Likelihood

The marginal likelihood integral:
\[
p(\mathcal{D} | M_k) = \int p(\mathcal{D} | \mathbf{\theta}_k) p(\mathbf{\theta}_k | M_k) \, d\mathbf{\theta}_k
\]
is often **analytically intractable**.  Common approximation techniques include:

- **Numerical integration**  
- **Monte Carlo sampling**  
- **Bayesian Monte Carlo methods**

However, when using **conjugate priors**, this term can sometimes be computed in **closed form**.






---



### Exercises {.unnumbered .unlisted}


Put some exercises here.



